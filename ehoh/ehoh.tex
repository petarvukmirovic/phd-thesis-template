\chapter{Extending a Brainiac Prover to Lambda-Free Higher-Order Logic}
\setheader{Extending a Brainiac Prover to Lambda-Free Higher-Order Logic}
\label{ch:ehoh}

\DeclareMathAlphabet{\mathcalx}{OT1}{pzc}{m}{it}
\renewcommand{\confrep}[2]{#2}
\includeversion{rep}
\excludeversion{conf}

\blfootnote{In this work I desgined, implemented and evaluated all changes to term representation, algorithms and indexing data structures.}

\authors{Joint work with\\
Jasmin~Blanchette, Simon~Cruanes and Stephan~Schulz}


\begin{abstract}
Decades of work have gone into developing efficient proof calculi, data
structures, algorithms, and heuristics for first-order automatic theorem
proving. Higher-order provers lag behind in terms of efficiency. Instead of
developing a new higher-order prover from the ground up, we propose to start
with the state-of-the-art superposition prover E and gradually enrich it with
higher-order features. We explain how to extend the prover's data structures,
algorithms, and heuristics to $\lambda$-free higher-order logic, a formalism
that supports partial application and applied variables. Our extension
outperforms the traditional encoding and appears promising as a stepping stone
toward full higher-order logic.    
\end{abstract}

\newpage

\section{Introduction}
\label{sec:ehoh:introduction}

\begin{sloppypar}
Superposition provers such as E~\cite{scv-19-e23}, SPASS \cite{wdfksw-09-spass},
and Vampire \cite{lkav-13-vampire} are among the most successful first-order
reasoning systems. They serve as backends in various frameworks, including
software verifiers (e.g., Why3 \cite{fp-13-why3}),
%
% \pagebreak[2]%
automatic higher-order theorem provers (e.g., \hbox{Leo-III} \cite{sb-21-leo3},
Satallax \cite{cb-12-satallax}), and \begin{rep}one-click \end{rep}``hammers'' in proof assistants
(e.g., HOLyHammer in HOL Light \cite{ku-15-holyhammer}, Sledgehammer in
Isabelle \cite{pb-12-sh}).


Dec\-ades of research have gone
into refining calculi, devising efficient data structures and algorithms,
and developing heuristics to guide proof search\begin{rep}
  \cite{ss-17-arcade}\end{rep}.
This work has mostly focused on first-order logic with equality.
\end{sloppypar}

Research on higher-order automatic provers has resulted
in systems such as LEO \cite{cbmk-98-leo}, \textsc{Leo}-II
\cite{bspt-15-leo2}, and Leo-III \cite{sb-21-leo3},
based on resolution and paramodulation, and Satallax \cite{cb-12-satallax},
based on \confrep{tableaux}{tableaux and SAT solving}. They
feature a ``cooperative'' architecture, pioneered by LEO: They
are full-fledged higher-order provers that regularly invoke an
external first-order prover \confrep{}{with a low time limit as a terminal
procedure, }in an attempt to finish the proof quickly using only
first-order reasoning. However, the first-order backend will succeed
only
if all the necessary higher-order reasoning has been performed,
meaning that much of the first-order reasoning is carried out by the
slower higher-order prover. As a result, this architecture leads to
suboptimal performance on largely first-order problems,
such as those that often arise
in interactive verification \cite{ns-13-leo2sh}. For example,
at the 2017 installment of the CADE ATP System Competition (CASC)
\cite{gs-17-casc}, Leo-III, which uses E as a backend, proved
652 out of 2000 first-order problems in the Sledgehammer division, compared
with 1185 for E on its own and 1433 for Vampire.

\looseness=-1
To obtain better performance, we propose to start with a competitive
first-order prover and extend it to full higher-order logic one
feature at a time.  Our goal is a \emph{graceful} extension, so that
the system behaves as before on first-order problems, performs mostly
like a first-order prover on typical, mildly higher-order problems,
and scales up to arbitrary higher-order problems, in keeping with the
zero-overhead principle: \emph{What you don't use, you
  don't pay for.}

As a stepping stone toward full higher-order logic, we initially restrict our
focus to a higher-order logic without \hbox{$\lambda$-expressions}
(Sect.~\ref{sec:ehoh:logic}). Compared with first-order logic, its distinguishing
features are partial application and applied variables. It
is rich enough to express the recursive equations of higher-order
combinators, such as $\cst{map}$ on lists:
%
\begin{align*}
\cst{map} \> f \> \cst{nil} & \eq \cst{nil}
&
\cst{map} \> f \> (\cst{cons} \; x \; \mathit{xs}) & \eq
\cst{cons} \; (f \> x) \; (\cst{map} \> f \> \mathit{xs})
\end{align*}

Our vehicle is E\begin{rep} \cite{ss-02-brainiac,scv-19-e23}\end{rep},
a prover developed primarily by Schulz. It is written in C and offers
good performance, with more emphasis on ``brainiac''
heuristics than on raw speed. E regularly scores among the top
systems at CASC and is usually the strongest open-source prover in
the relevant divisions. It also serves as a backend for
competitive higher-order provers. We refer to our extended version of
E as Ehoh. It corresponds to a prerelease version of E~2.5 configured with the
option \verb|--enable-ho|.%
\footnote{\url{https://github.com/eprover/eprover/commit/80946ac}}
%We plan to include the new higher-order inferences in the E 2.5 release.

The main challenges we faced concerned the
representation of types and terms
(Sect.~\ref{sec:ehoh:types-and-terms}), the unification
\confrep{algorithm}{and matching algorithms}
(Sect.~\ref{sec:ehoh:unif-match}), and the indexing data structures
(Sect.~\ref{sec:ehoh:indexing}). We also adapted the
inference rules (Sect.~\ref{sec:ehoh:inferences})\confrep{ and}{,} the
heuristics (Sect.~\ref{sec:ehoh:heuristics})\confrep{}{, and the
  preprocessor (Sect.~\ref{sec:ehoh:preprocessing})}.

\looseness=-1
A central aspect of our work is a set of techniques we call
\emph{prefix optimization}. Taking a traditional look at higher-order terms, they contain twice as many proper
subterms as first-order terms; for example,
$\cst{f}\;(\cst{g}\;\cst{a})\;\cst{b}$ contains not only the ``argument'' subterms
$\cst{g}\;\cst{a}$, $\cst{a}$, $\cst{b}$ but also the ``prefix'' subterms
$\cst{f}$, $\cst{f}\;(\cst{g}\;\cst{a})$, $\cst{g}$.
\begin{rep}Many operations, including superposition and rewriting, require
traversing all subterms of a term.\end{rep}
Using the optimization, the prover traverses subterms recursively in a
first-order fashion, considering all the prefixes of a given subterm
together. %, at little additional cost.
%
Our experiments (Sect.~\ref{sec:ehoh:evaluation}) show that Ehoh is
almost as fast as E on first-order problems and can also prove
higher-order problems that do not require synthesizing
$\lambda$-terms. As next steps, we plan to add support for
$\lambda$-terms and higher-order unification.

\section{Logic}
\label{sec:ehoh:logic}

\looseness=-1
Our logic is a variant of the intensional $\lambda$-free Boolean-free higher-order
logic (\lfhol{}) described by Bentkamp et al.\ %, Blanchette, Cruanes, and Waldmann
\cite[Sect.~2]{bbcw-21-lfho}, which could also be called ``applicative
first-order logic.'' In the spirit of FOOL \cite{kotelnikov-16-fool}, we
%% kotelnikov-15-fool
extend the syntax of this logic by erasing the distinction between terms and
formulas, and its semantics by interpreting the Boolean type $o$ as a domain
of cardinality~2. Functional extensionality can be obtained by adding
suitable axioms \cite[Sect.~3.1]{bbcw-21-lfho}.

\looseness=-1

This logic differs from the higher-order logic described in
Sect.~\ref{sec:pre:hol} three ways. First, $\lambda$-abstraction is disallowed.
Second, logical connectives are not part of the set of symbols. Instead, there
is a special inductive case in the definition of terms that defines formulas.
Third, subterms are defined in a more traditional way. 

\looseness=-1
For completeness, we provide the definition of terms. Terms, ranged over by $s,t,u,v$, are either
\emph{variables} $x, y, z, \dots$, (\emph{function}) \emph{symbols} $\cst{a},
\cst{b}, \cst{c}, \cst{d},\allowbreak \cst{f},\allowbreak \cst{g}, \ldots{}$
(often called ``constants'' in the higher-order literature), binary applications
$s \; t$, or Boolean terms $\itrue$, $\ifalse$, $\inot s$, $s \mathbin{\iand}
t$, $s \mathbin{\ior} t$, $s \mathbin{\iimplies} t$, $s \mathbin{\iequiv} t$,
$\iforall x.\> s$, $\iexists x.\> s$, $s \mathrel{\ieq} t$. E and Ehoh clausify
the input as a preprocessing step, producing a clause set in which the only
proper Boolean subterms are variables, $\itrue$, and $\ifalse$. Note that we use
lowercase letters for free variables as bound variables do not appear in
clauses. A term's \emph{arity} is the number of extra arguments it can take. If
$\iota$ is a base type, $\cst{f}$ has type $\iota \to \iota \to \iota$, and
$\cst{a}$ has type $\iota$, then $\cst{f}$ is binary, $\cst{f}\;\cst{a}$ is
unary, and $\cst{f}\;\cst{a}\;\cst{a}$ is nullary. Subterms are defined in the
traditional higher-order way; for example, $s \; t$ has all subterms of $s$ and
$t$ as subterms, in addition to $s \; t$ itself; as a consequence
$\cst{f} \; \cst{a}$ is a subterm of $\cst{f} \; \cst{a} \;\cst{a}$. With $\Var(x)$
we denote the set of free variables of $x$ where $x$ ranges over terms, clauses,
set of clauses or any other objects that contain terms.


In this chapter, substitutions $\sigma$ are partial
functions of finite domain from variables to terms, written $\{ x_1 \mapsto s_1,
\ldots, x_m \mapsto s_m \}$, where each $s_i$ has the same type as $x_i$. The
substitution $\sigma[x \mapsto s]$ maps $x$ to $s$ and otherwise coincides with
$\sigma$. Applying $\sigma$ to a variable beyond $\sigma$'s domain is the
identity. We deviated from the view of substitutions in Section \ref{sec:pre:hol}
as it made proofs in Sections \ref{sec:ehoh:unif-match} and \ref{sec:ehoh:indexing}
easier. It is easy to check that both views are equivalent. We also consider unification 
constraints $s \unif t$ as \emph{ordered} pairs.
%Applying a substitution to a term~$t$ applies it homomorphically to $t$'s
%variables. 

A well-known technique to support \lfhol{} %in first-order reasoning systems
is to use the \emph{applicative encoding}:
Every $n$-ary symbol is mapped to a nullary symbol, and
application is represented by a distinguished binary symbol $\cst{@}.$ Thus,
the \lfhol{} term $\cst{f} \; (x\; \cst{a}) \; \cst{b}$ is
encoded as the first-order term $\cst{@}(\cst{@}(\cst{f}, \cst{@}(x,
\cst{a})), \cst{b}).$ However, this representation is not graceful, since it
also introduces $\cst{@}$'s for terms within \lfhol's first-order fragment. By
doubling the size and depth of terms, the encoding clutters data
structures and slows down term traversals.
%By changing the root symbol of terms,
%it impacts proof search heuristics in subtle ways.
In our empirical evaluation, we find that the applicative encoding can
decrease the success rate by up to \NumberOK{15\%} (Sect.~\ref{sec:ehoh:evaluation}).
For these and further reasons, it
is not ideal (Sect.~\ref{sec:ehoh:discussion-and-related-work}).



\section{Types and Terms}
\label{sec:ehoh:types-and-terms}

\looseness=-1
The term representation is a central concern when building a theorem
prover. Delicate changes to E's representation were needed to support
partial application and especially applied variables. In contrast, the
introduction of a higher-order type system had a less dramatic impact on the
prover's code.
%In this section we
%describe changes we made to generalize E's term representation and how
%we completely replaced FOL many-sorted type system with simple types
%needed for \lfhol{}.

\ourpara{Types} For most of its history, E supported only untyped first-order logic. Cruanes
implemented support for atomic types for E 2.0
\cite[p.\,117]{sc-15-simon-phd}. Symbols are declared
with a type signature:
$\cst{f} : (\tau_1, \ldots, \tau_{n}) \rightarrow \tau.$
Atomic types are represented by integers, leading to efficient type comparisons.

\looseness=-1
In \lfhol{}, a type signature is simply a type~$\tau$, in which the
type constructor $\to$ can be nested---e.g., $(\iota \to \iota)\allowbreak
\to \iota.$
%
A natural way to represent such types is to mimic their recursive
structure using a tagged union. However, this leads to memory fragmentation;
a simple operation such as querying the type of a function's $i$th
argument would require dereferencing $i$ pointers. We prefer a
flattened representation, in
which a type $\tau_1 \rightarrow \cdots \rightarrow \tau_n \rightarrow \iota$
is represented by a single node labeled with ${\rightarrow}$ and pointing to
the array $(\tau_1,\dots,\tau_n,\iota).$

Ehoh stores all types in a shared bank and
implements perfect sharing, ensuring that types that are structurally the same
are represented by the same object in memory. Type equality can then be
implemented as a pointer comparison.

\ourpara{Terms}

In E, terms are stored as perfectly shared directed acyclic graphs\begin{rep}
\cite{ls-01-shared}\end{rep}.
%
%\begin{rep}
%-- or kill completely
%Even though we could implement applicative encoding as a preprocessor outside of E
%and feature full support for \lfhol{} in that way, we decided to generalize existing
%data structures and algorithms and relax existing invariants that are valid
%only for FOL terms. These generalizations and relaxations are largely dependent
%on the way E deals with term representation.
%\end{rep}
%
Each node, or \emph{cell}, contains 11~fields, including
\verb|f_code|, an integer that identifies the term's head symbol (if ${\ge}\;0$)
or variable (if ${<}\;0$); \verb|arity|, an integer corresponding to the number
of arguments passed to the head; \verb|args|, an array of size
\verb|arity| consisting of pointers to arguments; and \verb|binding|,
which may store a substitution for a variable\begin{rep} (if
\verb|f_code|$\;{<}\;0$)\end{rep}, used for unification and matching.

In first-order logic, the arity of a variable is always 0, and the arity of a
symbol is given by its type signature.
In higher-order logic, variables may have function type and be applied, and
symbols can be applied to fewer arguments than specified by their type
signatures. A natural representation of \lfhol{} terms as tagged unions
would distinguish between variables~$x$, symbols~$\cst{f}$, and binary
applications $s \; t.$ However, this scheme suffers from memory
fragmentation and linear-time access, as with the representation of types,
affecting performance on purely or mostly first-order problems. Instead, we
propose a flattened representation, as a generalization of E's existing data
structures: Allow arguments to variables, for symbols let \verb|arity| be
the number of \begin{conf}\emph{actual} arguments\end{conf}\begin{rep}actual
arguments, %as opposed to the declared arity,
and rename the field \verb|num_args|\end{rep}. This representation, often called
``spine notation,'' % \cite{cervesato-pfenning-2003},
is isomorphic to the standard definition of higher-order terms with binary
application. It is employed in various higher-order reasoning systems,
including Leo-III \cite{sb-21-leo3} and
Zipperposition \cite{bbtvw-21-sup-lam}.

%\begin{rep} This approach parallels our representation of types.\end{rep}

A side effect of the flattened representation is that prefix subterms are not
shared. For example, the terms $\cst{f}\;\cst{a}$ and $\cst{f}\;\cst{a}\;\cst{b}$
correspond to the flattened cells $\cst{f}(\cst{a})$ and
$\cst{f}(\cst{a}, \cst{b}).$ The argument subterm $\cst{a}$ is shared, but not
the prefix $\cst{f}\;\cst{a}.$ Similarly,
$x$ and $x\;\cst{b}$ are represented by two distinct cells, $x()$ and
$x(\cst{b})$, and there is no connection between the two occurrences of~$x$.
%
In particular,
despite perfect sharing, their \verb|binding| fields are unconnected, leading
to inconsistencies.

A potential solution would be to systematically traverse a
clause and set the \verb|binding| fields of all cells of the form $x(\overline{s})$
whenever a variable~$x$ is bound, but this would be inefficient and inelegant.
Instead, we implemented a hybrid approach: Variables are applied by an
explicit application operator
\appvar{}, to ensure that they are always perfectly shared. Thus, $x\;\cst{b}\;\cst{c}$
is represented by the cell $\appvar(x, \cst{b}, \cst{c})$, where $x$ is a shared
subcell. This is graceful, since variables never occur applied in first-order
terms.
%
The main drawback is that some normalization is necessary
after substitution: Whenever a variable is instantiated by a symbol-headed term,
the $\appvar$ symbol must be eliminated.
Applying the substitution $\{x \mapsto \cst{f} \; \cst{a}\}$ to the cell
$\appvar(x, \cst{b}, \cst{c})$ must produce $\cst{f}(\cst{a}, \cst{b}, \cst{c})$ and
not $\appvar(\cst{f}(\cst{a}), \cst{b}, \cst{c})$, for consistency with other
occurrences of $\cst{f} \; \cst{a} \; \cst{b} \; \cst{c}.$

There is one more complication related to the \verb|binding| field. In E, it
is easy and useful to traverse a term as if a substitution has been applied,
by following all set \verb|binding| fields. In Ehoh, this is not enough,
because cells must also be normalized. To avoid repeatedly creating the
same normalized cells, we introduced a \verb|binding_cache| field
that connects a $\appvar(x, \overline{s})$ cell with its substitution.
%
However, this cache can easily become stale when $x$'s \verb|binding| pointer is
updated. To detect this situation, we store $x$'s \verb|binding|
value in the $\appvar(x, \overline{s})$ cell's \verb|binding|
field (which is otherwise unused).
To find out whether the cache is valid, it suffices to check that the
\verb|binding| fields of $x$ and $\appvar(x, \overline{s})$ are equal.

\ourpara{Term Orders} Superposition provers rely on term orders to prune the search
space. The order must be a simplification order that
%can be extended to a simplification order that
% That's true but pointless. It's equivalent to saying that you need a
% simplification order and the prover may underapproximate it. -JB
is total on variable-free
terms. E implements both the Knuth--Bendix order (KBO) and the lexicographic path order
(LPO). KBO is widely regarded as the more
robust option for superposition.
%
In earlier work, Blanchette and colleagues have shown that only KBO can be
generalized gracefully while preserving the necessary properties for
superposition \cite{bbww-17-kbo,bww-17-rpo}. For this
reason, we focus on KBO.

E implements L\"ochner's linear-time algorithm for KBO
\cite{bl-06-kbo}, which relies on the \relax{tupling} method to store
intermediate results. % , avoiding repeated computations.
It is straightforward to generalize the algorithm to compute the
graceful \lfhol{} version of KBO \cite{bbww-17-kbo}.
The main difference is that when comparing two terms $\cst{f} \; \tuple{s}{m}$
and $\cst{f} \; \tuple{t}{n}$, because of partial application we may now
have $m \not= n$; this required changing the implementation to
perform a length-lexicographic comparison of the tuples $\tuple{s}{m}$ and
$\tuple{t}{n}.$

\ourpara{Input and Output Syntax} 
E implements the TPTP \cite{gs-17-tptp} formats FOF and TF0, corresponding to
untyped and mono\-morphic first-order logic, for both input and output. In
Ehoh, we added support for the \lfhol{} fragment of TPTP TH0, which provides
mono\-morphic higher-order logic. Thanks to the use of a standard format,
Ehoh's proofs can immediately be parsed by Sledgehammer
\cite{pb-12-sh}, which reconstructs them using a variety of
techniques.
There is ongoing work on increasing the level of detail of E's proofs, to
facilitate proof interchange and independent proof checking
\cite{rs-17-checkable};
this will also benefit Ehoh.

\section{Unification and Matching}
\label{sec:ehoh:unif-match}

Syntactic unification of \lfhol{} terms has a first-order flavor.
It is decidable, and most general unifiers (MGUs) are unique up to variable
renaming. For example, the unification constraint $\cst{f} \; (x \;
\cst{a}) \unif x \; (\cst{f} \; \cst{a})$, used to illustrate an infinite set of 
independent unifiers in full higher-order logic (Sect.~\ref{sec:pre:unif}), has the MGU 
$\{x \mapsto \cst{f}\}$ in \lfhol{}.
% in \lfhol{} the unification constraint $\cst{f} \; (y \;
% \cst{a}) \unif y \; (\cst{f} \; \cst{a})$ has the MGU $\{y \mapsto \cst{f}\}$,
% while in full higher-order logic infinitely many independent solutions exist as
% described in Sect.~\ref{sec:pre:unif}. 
Matching is a special case of unification
where only the variables on the left-hand side can be instantiated.

An easy but inefficient way to implement unification and matching for \lfhol{}
is to apply the applicative encoding (Sect.~\ref{sec:ehoh:logic}), perform
first-order unification or matching, and decode the resulting
substitution. To avoid the overhead, we generalize the first-order unification
and matching procedures to operate directly on \lfhol{} terms.

\subsection{Unification}
We present our unification procedure as a \begin{rep}nondeterministic \end{rep}transition system
that generalizes Baader and Nipkow \cite{bn-98-tr-and-all-that}.
A unification problem consists of a finite set~$\mathit{S}$ of unification constraints $s_i
\unif t_i$, where $s_i$ and $t_i$ are of the same type.
A problem is in \emph{solved form} if it has the form $\{ x_1 \unif t_1,\allowbreak
\ldots, x_n \unif t_n \}$, where the $x_i$'s are distinct and do not occur in
the $t_j$'s. The corresponding unifier is
$\{ x_1 \mapsto t_1,\allowbreak \ldots, x_n \mapsto t_n \}.$
%
The transition rules attempt to bring the input constraints into solved form.
\begin{rep}They can be applied in any order and eventually reach a normal form, which is
either an idempotent MGU expressed in solved form or the special
value $\bot$, denoting unsatisfiability of the constraints.\end{rep}

The first group of rules\confrep{ }{---the \emph{positive} rules---}consists
of operations that focus on a single constraint and replace it with a new
(possibly empty) set of constraints:

\noindent
\begin{tabular}{ll}
  \textsf{Delete} & $\{ t \unif t \} \uplus \mathit{S} \Unifarrow \mathit{S}$ \\[\jot]
  \textsf{Decompose} & $\{ \cst{f} \; \tuple{s}{m} \unif \cst{f} \; \tuple{t}{m} \} \uplus \mathit{S} \Unifarrow {} 
                        \mathit{S} \cup \{ s_1 \unif t_1, \ldots, s_m \unif t_m \}$ \\[\jot]
  \textsf{DecomposeX} & $\{ x \; \tuple{s}{{m}} \unif u \; \tuple{t}{{m}} \} \uplus \mathit{S} \Unifarrow {} 
                         \mathit{S} \cup \{ x \unif u{,}\; s_1 \unif t_1, \ldots, s_{m} \unif t_{m} \}$  \\
                      & if $x$ and $u$ have the same type and $m > 0$ \\[\jot]
  \textsf{Orient}     & $\{ \cst{f} \; \overline{s} \unif x \; \overline{t} \} \allowbreak \uplus \mathit{S}
                         \Unifarrow \mathit{S} \cup \{ x \; \overline{t} \unif \cst{f} \; \overline{s} \}$ \\[\jot]
  \textsf{OrientXY}   &  $\{ x \; \tuple{s}{m} \unif y \; \tuple{t}{n} \} \allowbreak \uplus \mathit{S}
                         \Unifarrow \mathit{S} \cup \{ y \; \tuple{t}{n} \unif x \; \tuple{s}{m} \}$  \\
                      &   if $m > n$ \\[\jot]
  \textsf{Eliminate}  & $\{ x \unif t \} \uplus \mathit{S} \Unifarrow \{ x \unif t \} \cup \{ x \mapsto t \}(\mathit{S})$ \\
                      &   if $x \in \Var(\mathit{S}) \setminus \Var(t)$
\end{tabular}


The \textsf{Delete}, \textsf{Decompose}, and \textsf{Eliminate} rules are
essentially as for first-order terms. The \textsf{Orient} rule is generalized
to allow applied variables and complemented by a new \textsf{Orient\-XY} rule.
\textsf{DecomposeX}, also a new rule, can be seen as a variant of
\textsf{Decompose} that analyzes applied variables; the term $u$ may be an
application.

The rules of the second group\confrep{ }{---the \emph{negative}
rules---}detect unsolvable constraints:
%
% \begin{description}[labelwidth=\widthof{\rm\textsf{OccursCheck}}]
% \item[\rm\textsf{Clash}]
%   $\{ \cst{f} \; \overline{s} \unif \cst{g} \; \overline{t} \} \uplus \mathit{S} \Unifarrow \bot$%
%   \quad if $\cst{f} \not= \cst{g}$

% \smallskip
% \item[\rm\textsf{ClashTypeX}]
%   \begin{tabular}[t]{@{}l@{}}
%     $\{ x \; \tuple{s}{m} \unif u \; \tuple{t}{m} \} \allowbreak \uplus \mathit{S} \Unifarrow \bot$ \\
%     if $x$ and $u$ have different types
%   \end{tabular}

% \smallskip
% \item[\rm\textsf{ClashLenXF}]
%   $\{ x \; \tuple{s}{m} \unif \cst{f} \; \tuple{t}{n} \} \allowbreak \uplus \mathit{S} \Unifarrow \bot$%
%   \quad if $m > n$

% \smallskip
% \item[\rm\textsf{OccursCheck}]
%   $\{ x \unif t \} \uplus \mathit{S} \Unifarrow \bot$%
%   \quad if $x \in \Var(t)$ and $x \not= t$
% \end{description}
%

\noindent
\begin{tabular}{ll}
  \textsf{Clash} &   $\{ \cst{f} \; \overline{s} \unif \cst{g} \; \overline{t} \} \uplus \mathit{S} \Unifarrow \bot$; if $\cst{f} \not= \cst{g}$ \\[\jot]
  \textsf{ClashTypeX} & $\{ x \; \tuple{s}{m} \unif u \; \tuple{t}{m} \} \allowbreak \uplus \mathit{S} \Unifarrow \bot$ ; if $x$ and $u$ have different types \\[\jot]
  \textsf{ClashLenXF} & $\{ x \; \tuple{s}{m} \unif \cst{f} \; \tuple{t}{n} \} \allowbreak \uplus \mathit{S} \Unifarrow \bot$; if $m > n$ \\[\jot]
  \textsf{OccursCheck}     & $\{ x \unif t \} \uplus \mathit{S} \Unifarrow \bot$;  if $x \in \Var(t)$ and $x \not= t$
\end{tabular}

\begin{rep}\textsf{Clash} and \textsf{OccursCheck} are essentially as in
Baader and Nipkow. \textsf{ClashTypeX} and \textsf{ClashLenXF}
are variants of \textsf{Clash} for applied variables.\end{rep}

\newcommand\unifARROW[1]{\rlap{\ensuremath{\Unifarrow_{#1}\;}}\phantom{\Unifarrow_\textsf{DecomposeX}\;}}

The derivation below demonstrates the computation of MGUs for the
unification problem
$\{x \> (z \> \cst{b} \> \cst{c}) \unif \cst{g} \> \cst{a} \> (y \> \cst{c})\}$:
%
\begin{align*}
& \{ \relax{x \> (z \> \cst{b} \> \cst{c})  \unif  \cst{g} \>  \cst{a} \>  (y \> \cst{c})}\}
\\[-1\jot]
\unifARROW{\textsf{DecomposeX}}
& \{ \relax{x \unif \cst{g} \> \cst{a}}{,}\; z \> \cst{b} \> \cst{c} \unif y \> \cst{c} \}
\\[-1\jot]
\unifARROW{\textsf{OrientXY}}
& \{ x \unif \cst{g} \> \cst{a}{,}\; \relax{y \> \cst{c} \unif z \> \cst{b} \> \cst{c}} \}
\\[-1\jot]
\unifARROW{\textsf{DecomposeX}}
& \{ x \unif \cst{g} \> \cst{a}{,}\; \relax{y  \unif z \> \cst{b}}{,}\;  \cst{c} \unif  \cst{c} \}
\\[-1\jot]
\unifARROW{\textsf{Delete}}
& \{ x \unif \cst{g} \> \cst{a}{,}\; y  \unif z \> \cst{b} \}
\end{align*}

E stores open constraints in a double-ended
queue. Constraints are processed from the front. New constraints are
added at the front if they involve complex
terms that can be dealt with swiftly by \textsf{Decompose} or
\textsf{Clash}, or to the back if one side is a variable. \begin{rep}This
delays instantiation of variables %(with a possible increase in term size)
and allows E to detect structural clashes early. \end{rep}%

During proof search, E repeatedly needs to test a term~$s$ for unifiability
not only with some other term $t$ but also with $t$'s subterms.
Prefix optimization speeds up this test: The subterms of $t$ are
traversed in a first-order fashion; for each such subterm $\zeta \;
\tuple{t}{n}$, at most one prefix $\zeta \;
\tuple{t}{k}$, with $k \le n$, is possibly unifiable with $s$, by virtue
of their having the same arity. \begin{rep}For first-order problems, we can only have $k =
n$, since all functions are fully applied. \end{rep}Using this technique, Ehoh
is virtually as efficient as E on first-order terms.

\looseness=-1
  The transition system introduced above always terminates with a correct
  answer. Our proofs follow the lines of Baader and Nipkow.
  The metavariable $\mathcalx{R}$ is used to range over constraint sets $\mathit{S}$
  and the special value $\bot.$
  The set of all unifiers of $\mathit{S}$ is denoted by $\Unifier(\mathit{S}).$
  Note that $\Unifier(\mathit{S} \cup \mathit{S}') = \Unifier(\mathit{S}) \cap \Unifier(\mathit{S}').$
  We let $\Unifier(\bot) = \emptyset.$
  The notation $\mathit{S} \Unifarrow^{!} \mathit{S}'$ indicates that $\mathit{S}
  \Unifarrow^{*} \mathit{S}'$ and $\mathit{S}'$ is a normal form (i.e., there exists no
  $\mathit{S}''$ such that $\mathit{S}' \Unifarrow \mathit{S}''$).
  A variable $x$ is \emph{solved} in $\mathit{S}$
  if it occurs exactly once in $\mathit{S}$, in a constraint of the form $x \unif t.$

  \begin{lemma}\label{lemma:ehoh:unif-preserve}
    If $\mathit{S} \Unifarrow \mathcalx{R}$, then $\Unifier(\mathit{S}) = \Unifier(\mathcalx{R}).$
  \end{lemma}
  \begin{proof}
    The rules \textsf{Delete}, \textsf{Decompose}, \textsf{Orient}, and
    \textsf{Eliminate} are proved as in Baader and Nipkow.
    \textsf{OrientXY} trivially preserves unifiers.
    For \textsf{DecomposeX}, the core of the argument is as follows:
    %
    \begin{align*}
    & \sigma \in \Unifier(\{ x \; \tuple{s}{{m}} \unif u \; \tuple{t}{{m}} \}) \\[-.5\jot]
      \text{iff}\quad
    & \sigma(x \; \tuple{s}{{m}}) = \sigma(u \; \tuple{t}{{m}}) \\[-.5\jot]
      \text{iff}\quad
    & \sigma(x) \; \sigma(s_1) \, \ldots \, \sigma(s_m)
        = \sigma(u) \; \sigma(t_1) \, \ldots \, \sigma(t_m) \\[-.5\jot]
      \text{iff}\quad
    & \sigma(x) = \sigma(u){,}~ \sigma(s_1) = \sigma(t_1){,}\; \ldots{,}~ \text{and}~ \sigma(s_m) = \sigma(t_m) \\[-.5\jot]
      \text{iff}\quad
    & \sigma \in \Unifier(\{ x \unif u{,}\; s_1 \unif t_1, \ldots, s_{m} \unif t_{m}\})
    \end{align*}

    The proof of the problem's unsolvability if rule \textsf{Clash} or
    \textsf{OccursCheck} is applicable carries over from Baader and Nipkow.
    For \textsf{ClashTypeX}, the justification is that $\sigma(x \;
    \tuple{s}{m}) = \sigma(u \; \tuple{t}{m})$ is possible only if
    $\sigma(x) = \sigma(u)$, which requires $x$ and $u$ to have the same
    type. Similarly, for \textsf{ClashLenXF}, if
    $\sigma(x \; \tuple{s}{m}) = \sigma(\cst{f} \; \tuple{t}{n})$ with
    $m > n$, we must have $\sigma(x \; \tuple{s}{{m-n}}) =
    \sigma(x) \; \sigma(s_1) \, \dots \, \sigma(s_{m-n}) = \cst{f}$,
    which is impossible.
  \end{proof}
\newpage
  \begin{lemma}\label{lemma:ehoh:unif-normal-imp-solved}
    If $\mathit{S}$ is a normal form, then $\mathit{S}$ is in solved form.
  \end{lemma}
  \begin{proof}
    Consider an arbitrary unification constraint $s \unif t \in \mathit{S}.$ We show
    that in all but one cases, a rule is applicable, contradicting the
    hypothesis that $\mathit{S}$ is a normal form. In the remaining case, $s$ is a
    solved variable in $\mathit{S}$.

    \medskip
    \noindent
    \textsc{Case} $s = x$:
    \begin{itemize}
    \item \textsc{Subcase} $t = x$:
      \textsf{Delete} is applicable.

%      \smallskip
    \item \textsc{Subcase} $t \neq x$ and $x \in \Var(t)$:
      \textsf{OccursCheck} is applicable.

%      \smallskip
    \item \textsc{Subcase} $t \neq x$, $x \not \in \Var(t)$, and $x \in
      \Var(\mathit{S} \setminus \{ s \unif t \})$:
      \textsf{Eliminate} is applicable.

%      \smallskip
    \item \textsc{Subcase} $t \neq x$, $x \not \in \Var(t)$,
          and $x \notin \Var(\mathit{S} \setminus \{ s \unif t \})$:
      The variable $x$ is solved in $\mathit{S}$.
    \end{itemize}

    \noindent
    \textsc{Case} $s = x \; \tuple{s}{m}$ for $m > 0$:
    \begin{itemize}
    \item \textsc{Subcase} $t = \eta \; \tuple{t}{n}$ for $n \ge m$:
      \textsf{DecomposeX} or \textsf{Clash\-TypeX} is applicable, depending on
      whether $x$ and $\eta \; \tuple{t}{{n-m}}$ have the same type.

%      \smallskip
    \item \textsc{Subcase} $t = y \; \tuple{t}{n}$ for $n < m$:
      \textsf{OrientXY} is applicable.

%      \smallskip
    \item \textsc{Subcase} $t = \cst{f} \; \tuple{t}{n}$ for $n < m$:
      \textsf{ClashLenXF} is applicable.
    \end{itemize}

    \noindent
    \textsc{Case} $s = \cst{f} \; \tuple{s}{m}$:
    \begin{itemize}
    \item \textsc{Subcase} $t = x \; \tuple{t}{n}$:
      \textsf{Orient} is applicable.

%      \smallskip
    \item \textsc{Subcase} $t = \cst{f} \; \tuple{t}{n}$:
      Due to well-typedness, $m = n$. \textsf{Decompose} is
      applicable.

%      \smallskip
    \item \textsc{Subcase} $t = \cst{g} \; \tuple{t}{n}$:
      \textsf{Clash} is applicable.
    \end{itemize}

    \noindent
    Since each constraint is of the form $x \unif t$ where $x$ is solved in
    $\mathit{S}$, the problem $\mathit{S}$ is in solved form.
  \end{proof}

  \begin{lemma}\label{lemma:ehoh:unif-idempotent-mgu}
    If the constraint set $\mathit{S}$ is in solved form, then the associated
    substitution is an idempotent MGU of $\mathit{S}.$
  \end{lemma}
  \begin{proof}
    This lemma corresponds to Lemma~4.6.3 of Baader and Nipkow. Their proof
    carries over to \lfhol{}.
  \end{proof}

  \begin{theorem}[Partial Correctness]\label{thm:ehoh:unif-partial-correctness}\,%
    If $\mathit{S} \Unifarrow^{!} \bot$, then $\mathit{S}$ has no solutions.
    If $\mathit{S} \Unifarrow^{!} \mathit{S}'\!$, then $\mathit{S}'$ is in solved form and the
    associated substitution is an idempotent MGU of $\mathit{S}$.
    \end{theorem}

    \begin{proof}
    The first part follows from Lemma~\ref{lemma:ehoh:unif-preserve}.
    %applied transitively
    The second part follows from Lemma \ref{lemma:ehoh:unif-preserve}
    %applied transitively
    and Lemmas
    \ref{lemma:ehoh:unif-normal-imp-solved}~and~\ref{lemma:ehoh:unif-idempotent-mgu}.
    \end{proof}

      \newpage
      \begin{theorem}[Termination]\label{thm:ehoh:unif-termination}\,%
      The relation $\Unifarrow$ is well founded.
      \end{theorem}
    
      \begin{proof}
      We define an auxiliary notion of weight:
      $\mathcalx{W}(\zeta \; \tuple{s}{m}) =
        \allowbreak m + 1 + \sum_{i=1}^{m} \mathcalx{W}(s_i)$.
      Well-foundedness is proved by exhibiting a measure function from
      constraint sets to quadruples of natural numbers $(n_1, n_2, n_3, n_4)$,
      where
      $n_1$ is the number of unsolved variables in $\mathit{S}$;
      $n_2$ is the sum of all term weights,
      $\sum_{s \unif t \in \mathit{S}} \mathcalx{W}(s) + \mathcalx{W}(t)$;
      $n_3$ is the number of right-hand sides with variable heads,
      $|\{ s \unif x \; \overline{t} \in \mathit{S} \} |$; and
      $n_4$ is the number of arguments to left-hand side variable heads,
        $\sum_{x \> \tuple{s}{m} \unif t  \in \mathit{S}} m$.
  
      The following table shows that the application of each positive rule
      lexicographically decreases the quadruple:
  
        \begin{center}
          \begin{tabular}{l@{\qquad}l@{\quad}l@{\quad}l@{\quad}l@{\quad}}
                              & $n_1$  & $n_2$ & $n_3$ & $n_4$ \\[\jot]
          \textsf{Delete}     & $\geq$ & $>$   &       &       \\
          \textsf{Decompose}  & $\geq$ & $>$   &       &       \\
          \textsf{DecomposeX} & $\geq$ & $>$   &       &       \\
          \textsf{Orient}     & $\geq$ & $=$   & $>$   &       \\
          \textsf{OrientXY}   & $\geq$ & $=$   & $=$   & $>$   \\
          \textsf{Eliminate}  & $>$    &       &       &
          \end{tabular}
        \end{center}
  %
  %    \noindent
  %    \textsf{Eliminate}, by solving one variable, decreases $n_1.$
  %    \textsf{Delete} removes a constraint from~$\mathit{S}$, decreasing the
  %    weight.
  %    \textsf{Decompose} reduces the weight by at least~2.
  %    \textsf{Prefix} reduces the set weight by at least $m>0$, thanks to the
  %    presence of $m$ in the definition of $\mathcalx{W}$.
      The negative rules, which produce the special value $\bot$, cannot
      contribute to an infinite $\Unifarrow$ chain.
      %For example, the quadruples corresponding to the derivation
      %starting from $\{x \> (z \> \cst{b} \> \cst{c}) \unif \cst{g} \> \cst{a} \> (y \> \cst{c})\}$
      %given above are $(3,14,0,1) > (2,12,1,2) > (2, 12, 1, 1) > (1, 10, 1, 0) > (1, 8, 1, 0)$.
  %    where $>$ denotes the lexicographic extension of $>$ on natural numbers.
      \end{proof}
  
  A unification algorithm for \lfhol{} can be derived from the above transition
  system, by committing to a strategy for applying the rules.
  %
  This algorithm closely follows the Ehoh implementation, abstracting away from
  complications such as prefix optimization. We assume a flattened
  representation of terms; as in Ehoh, each variable stores the term it is bound
  to in its \textit{binding} field (Sect.~\ref{sec:ehoh:types-and-terms}).
  We also rely on a \textsc{ApplySubst} function, which applies the
  binding to the top-level variable. The algorithm assumes that the terms
  to be unified have the same type. The pseudocode is as follows:

  \begin{quotex}
    \MyFunction{SwapNeeded}{Term $\mathit{s}$, Term $\mathit{t}$}
    \q \MyReturn $\begin{aligned}[t]
      & t.\mathit{head}.\mathit{isVar}() \\[-\jot]
      & {\mathrel\land}\; (\lnot\, s.\mathit{head}.\mathit{isVar}()
      \mathrel\lor s.\mathit{num\_args} > t.\mathit{num\_args})
      \end{aligned}$
    
    \vskip 1.5\jot
    
    \MyFunction{Deref}{Term $\mathit{s}$}
    \q \MyWhile{$s.\mathit{head}.\mathit{isVar}() \mathrel\land s.\mathit{head}.\mathit{binding} \neq \mathit{Null}$}
    \qq $s \gets \textsc{ApplySubst}(s{,}\; s.\mathit{head}.\mathit{binding})$ \\
    \q \MyReturn $s$
    
    \vskip 1.5\jot
    
    \MyFunction{GobblePrefix}{Term $x$, Term $t$}
    \q $\mathit{res} \gets \mathit{Null}$ \\
    \q \MyIf{$x.\mathit{type}.\mathit{args}$ is suffix of
          $\mathit{t}.\mathit{head}.\mathit{type}.\mathit{args}$}
    \qq $\mathit{pref\_len} \gets \mathit{t}.\mathit{head}.\mathit{type}.\mathit{arity} - x.\mathit{type}.\mathit{arity}$ \\
    \qq \MyIf{$\mathit{pref\_len} \leq t.\mathit{num\_args}$}
    \qqq $\mathit{res} \gets $
                \textsc{Term}$(t.\mathit{head}$, $t.\mathit{args}[1 \,.\,.\, \mathit{pref\_len}])$ \\
    \q \MyReturn $\mathit{res}$
    
    \vskip 1.5\jot
    
    \MyFunction{Unify}{Term $\mathit{s}$, Term $\mathit{t}$}
    \q $\mathit{constraints} \gets \textsc{DoubleEndedQueue}()$ \\
    \q $\mathit{constraints}.\mathit{prepend(s)}$ \\
    \q $\mathit{constraints}.\mathit{prepend(t)}$
    \\[\jot]
    \q \MyWhile{$\lnot\, \mathit{constraints}.\mathit{isEmpty}()$}
    \qq $t \gets \textsc{Deref}(\mathit{constraints}.\mathit{dequeue}())$ \\
    \qq $s \gets \textsc{Deref}(\mathit{constraints}.\mathit{dequeue}())$
    \\[\jot]
    \qq \MyIf{$s \neq t$}
    \qqq \MyIf{\textsc{SwapNeeded}$(s,t)$}
    \qqqq $(t, s) \gets (s, t)$
    \\[\jot]
    \qqq \MyIf{$s.\mathit{head}.\mathit{isVar}()$}
    \qqqq $x \gets s.\mathit{head}$ \\
    \qqqq $\mathit{prefix} \gets \textsc{GobblePrefix}(x,t)$ \\
    \qqqq \MyIf{$\mathit{prefix} \neq \mathit{Null}$}
    \qqqqq $\mathit{start\_idx} \gets \mathit{prefix}.\mathit{num\_args} + 1$ \\
    \qqqqq \MyIf{$x$ occurs in $\mathit{prefix}$}
    \qqqqqq \MyReturn $\textit{False}$ \\
    \qqqqq \MyElse
    \qqqqqq $x.\mathit{binding} \gets \mathit{prefix}$ \\
    \qqqq \MyElse
    \qqqqq \MyReturn $\textit{False}$ \\
    \qqq \MyElsIf{$s.\mathit{head} = t.\mathit{head}$}
    \qqqq  $\mathit{start\_idx} \gets 1$ \\
    \qqq \MyElse
    \qqqq \MyReturn $\textit{False}$
    \\[\jot]
    \qqq \MyForTo{$i \gets \mathit{start\_idx}$}{$t.\mathit{num\_args}$}
    \qqqq $\mathit{s\_arg} \gets s.\mathit{args}[i-\mathit{start\_idx}+1]$ \\
    \qqqq $\mathit{t\_arg} \gets t.\mathit{args}[i]$ \\
    \qqqq \MyIf{$(\mathit{s\_arg}.\mathit{head}.\mathit{isVar}() \lor \mathit{t\_arg}.\mathit{head}.\mathit{isVar}())$}
    \qqqqq $\mathit{constraints}.\mathit{append}(t\_\mathit{arg})$ \\
    \qqqqq $\mathit{constraints}.\mathit{append}(s\_\mathit{arg})$ \\
    \qqqq \MyElse
    \qqqqq $\mathit{constraints}.\mathit{prepend}(s\_\mathit{arg})$ \\
    \qqqqq $\mathit{constraints}.\mathit{prepend}(t\_\mathit{arg})$
    \\[\jot]
    \q \MyReturn $\textit{True}$
    \end{quotex}






\section{Indexing Data Structures}
\label{sec:ehoh:indexing}

\section{Inference Rules}
\label{sec:ehoh:inferences}

\section{Heuristics}
\label{sec:ehoh:heuristics}

\section{Preprocessing}
\label{sec:ehoh:preprocessing}

\section{Evaluation}
\label{sec:ehoh:evaluation}

\section{Discussion and Related Work}
\label{sec:ehoh:discussion-and-related-work}

\section{Conclusion}
\label{sec:ehoh:conclusion}
