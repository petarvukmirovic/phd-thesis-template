\chapter{Boolean Reasoning in a Higher-Order Superposition Prover}
\setheader{Boolean Reasoning in a Higher-Order Superposition Prover}
\label{ch:bools}


\blfootnote{In this work I implemented and evalauted most of the described approaches.
Visa Nummelin implemented the FOOL preprocessing module.}

\authors{Joint work with Visa Nummelin}

\begin{abstract}
    We present a pragmatic approach to extending a Boolean-free higher-order
    superposition calculus to support Boolean reasoning. Our approach extends
    inference rules that have been used only in a first-order setting and uses some
    rules previously implemented in higher-order provers, as well as new rules.
    We have implemented the approach in the Zipperposition
    theorem prover. The evaluation shows highly competitive performance of our approach
    and a clear improvement over previous techniques.
\end{abstract}

\newpage

\section{Introduction} 
\label{sect:bool:introduction}

In Chapter~\ref{ch:intro} we motivated our work by the need to bridge the gap
between higher-order frontends and first-order backends. This gap is
traditionally bridged using translations from higher-order to
first-order logic \cite{ar-70-hol, mp-08-trans}. However, as shown in Chapter
\ref{ch:ehoh}, translations are usually less efficient than native support. The
distinguishing features of higher-order logic used by proof assistants that the
translation must eliminate include $\lambda$ binders, function
extensionality---the property that functions are equal if they agree on every
argument, described by the axiom $\iforall xy. \allowbreak\, (\iforall z. \, x
\, z \ieq y \,z) \iimplies x \ieq y$---and formulas occurring as arguments of
function symbols \cite{mp-08-trans}.

\looseness=-1

% A group of authors including Vukmirovi\'c\ \cite{bbtvw-21-sup-lam} designed an
% extension of superposition for extensional Boolean-free higher-order logic. The
% extension removes the need to translate $\lambda$-binders and functional
% extensionality. 
As mentioned in Sect.~\ref{sec:pre:ho-sup-calculi}, Bentkamp et al. developed
\lsup{}, a complete higher-order calculus that alleviates the need to translate
$\lambda$-terms and function extensionality. Kotelnikov et al.\
\cite{kotelnikov-15-fool,kotelnikov-16-fool} extended the language of
first-order logic to support the third feature of higher-order logic that
requires translation. They described two approaches: One based on a calculus-level
treatment of Booleans and other, which requires no changes to the calculus,
based on preprocessing.

\looseness=-1 To fully bridge the gap between higher-order and first-order tools,
we combine the two approaches: We take \lsup{} as our basis and  extend it with inference rules that reason with
Boolean terms. In early work, Kotelnikov et al.\ 
\cite{kotelnikov-15-fool} have described a \emph{FOOL paramodulation}
rule that, under some order requirements, removes the need for the axiom
describing the Boolean domain---$\iforall p. \; p \ieq \itrue \llor p
\ieq \ifalse$. In this approach, it is assumed that a problem with formulas occurring as
arguments of symbols is translated to first-order logic. 

\looseness=-1
The backbone of our approach is based on an extension of this rule to higher-order
logic: We do not translate away any Boolean structure that is nested inside
non-Boolean terms and allow our rule to hoist the nested Booleans to the
literal level. Then, we clausify the resulting formula (i.e., a clause containing formulas in literals) using a new rule.


\looseness=-1 An important feature that we inherit by building on top of \lsup{}
is support for function extensionality. Moving to higher-order logic with
Booleans also means that we need to consider \emph{Boolean extensionality}:
$\iforall pq. \, (p \iequiv q) \iimplies p \ieq q$. We extend the rules of
\lsup{} that treat function extensionality to also treat Boolean
extensionality.

\looseness=-1
Rules that extend the two orthogonal approaches form the basis of our support
for Boolean reasoning (Sect.~\ref{sect:bool:native}). In addition, we have
implemented rules that are inspired by the ones used in the higher-order provers
Leo-III \cite{sb-21-leo3} and Satallax \cite{cb-12-satallax}, such as
elimination of Leibniz equality, primitive instantiation, and treatment of the choice
operator \cite{pa-01-classical-ty-thy}. We have also designed new rules including those that use
higher-order unification to resolve Boolean formulas that are hoisted to the literal
level, delay clausification of nonatomic literals, and reason about formulas under
$\lambda$-binders. Even though the rules we use are inspired by
the ones of refutationally complete higher-order provers, we do not guarantee
completeness of our extension of \lsup{}. Using the insights that we gained with
this incomplete extension of \lsup{} to full higher-order logic, Bentkamp et al.
designed a complete calculus for full higher-order logic---\osup{}
\cite{bbtv-21-full-ho-sup}.


\looseness=-1
We compare our native approach with two alternatives which are based on
preprocessing (Sect.~\ref{sect:bool:alternative}). First, we compare it to  an
axiomatization of the theory of Booleans. Second, inspired by work of
Kotelnikov et al.\ \cite{kotelnikov-16-fool}, we implement the preprocessing approach that does not
require introduction of Boolean axioms.
We discuss some examples, coming from TPTP \cite{gs-17-tptp}, that
illustrate advantages and disadvantages of our approach (Sect.~\ref{sect:bool:examples}).

The theorem prover Zipperposition \cite{sc-15-simon-phd,sc-supind-17} was used
by Bentkamp et al.\ to implement \lsup{}. We further extend their implementation
based on the approach presented in this chapter.

\looseness=-1
We performed an extensive evaluation of our approach (Sect.~\ref{sect:bool:eval}).
In addition to evaluating different configurations of our new rules, we have
compared them to full higher-order provers CVC4, Leo-III,  Satallax, and Vampire.
The results suggest that it is beneficial to natively support Boolean reasoning---the approach outperforms preprocessing-based approaches. Furthermore, it is
very competitive in comparison to state-of-the-art higher-order provers. We discuss the differences between our approach and the
approaches our work is based on, as well as related approaches (Sect.~
\ref{sect:bool:discussion}).


\section{Background} 
\label{sect:bool:background}

We base our work (and parts of the following text) on Bentkamp et al.'s
\cite{bbtvw-21-sup-lam} extensional polymorphic clausal higher-order logic. We
extend the syntax of this logic by adding logical connectives to the signature.
The semantics of the logic is extended by interpreting Boolean type $o$ as a
two-element domain. This amounts to extending the logic to full higher-order
logic. Taking a different perspective, this logic is an extension of the one
presented in Chapter \ref{ch:ehoh} with $\lambda$-abstraction, polymorphic
types, and native logical connectives. For reference, we provide a
description of this polymorphic logic. For the notions that are not defined here,
we assume the definitions introduced in Chapter~\ref{ch:pre}.

\looseness=-1
A {\em signature} is a quadruple $(\Sigmaty, \Vty, \Sigma, \VV)$ where $\Sigmaty$ is a
set of type constructors, $\Vty$ is a set of type variables and $\Sigma$ and
$\VV$ are sets of constants and term variables, respectively. We require nullary
type constructor $o$ as well as binary constructor $\rightarrow$
to be in $\Sigmaty$. A type $\tau, \upsilon$ is either a type variable $\alpha \in
\Vty$ or of the form $\kappa(\tau_1, \ldots \tau_n)$ where $\kappa$ is an
$n$-ary type constructor. We write $\kappa$ for $\kappa()$, $\tau \rightarrow
\upsilon$ for $\rightarrow(\tau, \upsilon)$, and we drop parentheses to shorten 
$\tau_1 \rightarrow (\cdots \rightarrow (\tau_{n-1} \rightarrow \tau_n) \cdots)$ into $\tau_1 \rightarrow \cdots \rightarrow
\tau_n$. Each symbol in $\Sigma$ is
assigned a type declaration of the form $\forallty{\tuplen{\alpha}} \tau$ where all variables
occurring in $\tau$ are among $\tuplen{\alpha}$.

\looseness=-1
Function symbols $\cst{a}, \cst{b}, \cst{f}, \cst{g}, \ldots$ are elements of
$\Sigma$; their type declarations are written as $\cst{f} :
\forallty{\tuplen{\alpha}} \tau$. Set $\VV$ contains both free and bound variables, following
the distinction introduced in Sect.~\ref{sec:pre:hol}. Free term variables from the set $\VV$ are written
$F,G,X,Y, \ldots$ and we denote their types as $X : \tau$. Bound term variables are written $x,y,z,\ldots$,
and their types are similarly denoted. When the type is not
important, we omit type declarations. We assume that symbols $\itrue,
\ifalse,\inot,\iand, \ior,\iimplies,\iequiv$ with their standard meanings and type declarations are elements of
$\Sigma$. Furthermore, we assume that polymorphic symbols $\iforall$ and $\iexists$
with type declarations $\forallty{\alpha} (\alpha \rightarrow o) \rightarrow o$
and $\ieq \; : \forallty{\alpha} {\alpha \rightarrow \alpha \rightarrow o}$ are
in $\Sigma$, with their standard meanings. All these symbols are called \emph{logical
symbols}. We use infix notation for binary logical symbols.

\looseness=-1 Terms are defined inductively as follows. Free ($X : \tau$)  and bound ($x : \tau$) variables   are
terms of type $\tau$. If $\cst{f} : \forallty{\tuplen{\alpha}} \tau$ is in
$\Sigma$ and $\tuplen{\upsilon}$ is a tuple of types, called type arguments, then
$\cst{f}\typeargs{\tuplen{\upsilon}}$ (written as $\cst{f}$ if $n=0$, or if type
arguments can be inferred from the context) is a term of type $\tau \{
\tuplen{\alpha} \mapsto \tuplen{\upsilon} \}$, called constant. If $x$ is a bound variable
of type $\tau$ and $s$ is a term of type $\upsilon$, then $\lamx{s}$ is a term of type
$\tau \rightarrow \upsilon$. If $s$ and $t$ are of type $\tau \rightarrow \upsilon$ and
$\tau$, respectively, then $s \, t$ is a term of type $\upsilon$. We call terms of
Boolean type ($o$) \emph{formulas} and denote them by $f,g,h, \ldots$; we use
$P,Q,R, \ldots$ for free variables whose result type is $o$ and
$\cst{p},\cst{q},\cst{r}$ for constants with the same result type.
%
Formulas whose top-level symbol is not logical are called \emph{atoms}.
Unless stated otherwise, we view terms as
$\alpha\beta\eta$-equivalence classes, with the $\eta$-long $\beta$-reduced form as
the representative.

\looseness=-1
Given a formula $f$, we call its Boolean subterm $f|_p$ a \emph{top-level
Boolean} if for all proper prefixes $q$ of $p$, the head of $f|_q$ is a logical
constant. Otherwise, we call it a \emph{nested Boolean}. For example, in the
formula $f = \cst{h} \, \cst{a} \ieq \cst{g} \, (\cst{p} \iimplies \cst{q}) \ior
\inot\cst{p}$, $f|_1$ and $f|_2$ are top-level Booleans, while $f|_{1.2.1}$ is a
nested Boolean, as well as its subterms. First-order logic allows only top-level
Booleans, whereas nested Booleans are characteristic for higher-order logic. 

% Note that while our calculus works with clausal representation of the problem,
% its higher-order nature allows representing formula $f$ as a singleton clause
% containing only literal $f$.
Our calculus works with the clausal structure described in Sect.~\ref{sec:pre:clauses}.
However, its higher-order nature allows representing formula $f$ as a singleton clause
containing only literal $f$.

\section{The Native Approach} 
\label{sect:bool:native}

\looseness=-1
Zipperposition already had some support for Boolean reasoning before we
started extending \lsup{}. In this section, we first
describe the internals of Zipperposition responsible for reasoning with
Booleans. We continue by describing the rules that we have
implemented. For ease of presentation we divide them into three categories.
\subsection{Support for Booleans in Zipperposition}
\label{subsect:bool:zip-bools}

As mentioned in Chapter~\ref{ch:unif}, Zipperposition is an open source prover
written in OCaml. From its inception, it was designed as a prover that supports
easy extension of its base superposition calculus to various theories, including
arithmetic, induction, and limited support for higher-order logic
\cite{sc-15-simon-phd,sc-supind-17}.

In Zipperposition, applications are represented in flattened, spine notation (Sect.~\ref{sec:ehoh:types-and-terms}).
This means that the higher-order term $(\cst{f} \, \cst{a}) \, \cst{b}$ is
represented the same as the first-order term $\cst{f}(\cst{a}, \cst{b})$. In
addition, Zipperposition uses associativity of $\iand$ and $\ior$ to flatten out
the nested applications of these symbols. For example, terms $\cst{p} \iand
(\cst{q} \iand \cst{r})$ and $(\cst{p} \iand \cst{q}) \iand \cst{r}$ are
internally stored as $\iand \, \cst{p} \, \cst{q} \, \cst{r}$. Zipperposition's
support for $\lambda$-terms is used to represent quantified nested Booleans:
Formulas $\iforall x. \, \, f$ and $\iexists x. \, f$ are represented as
$\iforall \, (\lamx{f})$ and $\iexists \, (\lamx{f})$. After clausification of
the input problem, no nested Booleans will be modified or renamed using fresh
predicate symbols.

The version of Zipperposition preceding our modifications distinguished between non\-equational
and equational literals. Following E \cite{scv-19-e23}, we
modified Zipperposition to represent all literals equationally: a nonequational
literal $f$ is stored as $f \eq \itrue$, whereas $\inot f$ is stored as $f
\noteq \itrue$. Equations of the form $f \eq \ifalse $ and $f \noteq \ifalse$ are
transformed into $f \noteq \itrue$ and $f \eq \itrue$, respectively.

\subsection{Core Rules}
\label{subsect:bool:core}

\looseness=-1
Kotelnikov et al.\ \cite{kotelnikov-15-fool}, to the best of our
knowledge, pioneered the approach of extending a first-order superposition prover to support
nested Booleans. They call including the axiom $\iforall
p. \; p \eq \itrue \ior p \ieq \ifalse$  ``a recipe for disaster''. To combat the
explosive behavior of the axiom, they impose the following two requirements to
the simplification order $\succ$: $\itrue \succ \ifalse$ and
$\itrue$ and $\ifalse$ are two smallest ground terms with respect to $\succ$. If
these requirements are met, there is no self-paramodulation of the clause
and the only paramodulation possible is from the literal $p \eq \itrue$ of the mentioned axiom
into a Boolean subterm of another clause. Finally, Kotelnikov et al.\ replace
the axiom with the inference rule \emph{FOOL paramodulation} (\infname{FP}):
\pagebreak[2]
%
$$ \namedinference{FP}{C[f]}{C[\itrue] \vee f \eq \ifalse} $$
%
\looseness=-1
where $f$ is a nested non-variable Boolean subterm of clause $C$, different from
$\itrue$ and $\ifalse$. In addition, they translate the initial problem containing nested
Booleans  to first-order logic without interpreted Booleans; thus, the symbols $\itrue$ and $\ifalse$ and the type $o$
correspond to proxy symbols and types introduced during the translation. 

We created two rules that are syntactically similar to \infname{FP} but
are adapted for higher-order logic with one key distinction---we do not perform any translation:
%
\begin{align*}
  & \namedinference{Cases}{C[f]}{C[\ifalse] \vee f \eq \itrue}
  && \namedsimp{CasesSimp}{C[f]}{C[\ifalse] \vee f \eq \itrue \quad C[\itrue] \vee f \noteq \itrue}
\end{align*}
%
\looseness=-1
The prover that uses the rules should not include them both at the same time.
In addition, since
literals $f \eq \ifalse$ are represented as negative equations $f \noteq \itrue$, which cannot be used to paramodulate from,
we change the first requirement on the order to $\ifalse \succ
\itrue$, making $\itrue$ the smallest symbol.

\newcommand{\eqneq}{\mathrel{\dot{\eq}}}
These two rules hoist Boolean subterms $f$ to the literal level; therefore,
some results of \infname{Cases} and \infname{CasesSimp} will have literals of the form $f \eq \itrue$ (or
$f \noteq \itrue$) where $f$ is not an atom. This introduces the need for the rule
called immediate clausification (\infname{IC}), where $\tuple{D}{m}$ are defined below:
%
$$ \namedsimp{IC}{C}{D_1 \; \cdots \; D_m} $$
%
Let $s \eqneq t$ denote $\eq$ or $\not\eq$.
We say that a clause is \emph{standard} if all of its literals are of the form $s \eqneq t$,
where $s$ and $t$ are not Booleans or of the form $f \eqneq \itrue$, where the head of $f$
is not a logical symbol. The rule \infname{IC}
is applicable if clause $C = L_1 \llor
\cdots \llor L_n$ is not standard.
The resulting clauses $\tuple{D}{m}$ represent
the result of clausification of the formula $\iforall\,\tuple{x}{}. \; L_1 \ior
\cdots \ior L_n$ where $\tuple{x}{}$ consists of all free variables of $C$.
Using Boolean extensionality, Zipperposition's clausification
algorithm treats Boolean equality as equivalence (i.e., it replaces
$\mathop{\eq}\typeargs{o}$ with $\iequiv$).

\looseness=-1
An advantage of leaving nested Booleans unmodified is that the prover will be able
to prove some problems containing them without using the prolific rules described
above. For example, given two clauses $\cst{f} \, (\cst{p} \, X
\iimplies \cst{p} \, Y) \eq \cst{a}$ and $\cst{f} \, (\cst{p} \,
\cst{a} \iimplies \cst{p} \, \cst{b}) \noteq \cst{a}$, the empty clause can
easily be derived without the above rules. A disadvantage of this approach
is that the proving process will periodically be interrupted by expensive calls
to the clausifier.
\pagebreak[2]

\looseness=-1
If implemented naively, rules \infname{Cases} and \infname{CasesSimp} can result
in many redundant clauses. Consider the following example: let $\cst{p} : o
\rightarrow o$, $\cst{a} : o$ and consider a clause set containing $\cst{p} \,
(\cst{p} \, (\cst{p} \, (\cst{p} \, \cst{a}))) \eq \itrue$. Then, the clause $C
= \cst{a} \eq \itrue \llor \cst{p} \, \ifalse \eq \itrue$ can be derived in
eight ways using the rules, depending on which nested Boolean subterm was chosen
for the inference. In general, if a clause has a subterm occurrence of the form
$\cst{p}^n \, \cst{a}$, where both $\cst{p}$ and $\cst{a}$ have result type $o$,
the clause $\cst{a} \eq \itrue \llor \cst{p} \, \ifalse \eq \itrue$ can be
derived in at least $2^{n-1}$ ways.
% Boolean subterms occurring in different
% eligible literals cause similar explosive behavior.
To combat these issues we
implemented pragmatic restrictions of the rule: only the $f$ which is
the leftmost outermost (or innermost) eligible subterm will be considered. With
this modification $C$ can be derived in only one way. Furthermore,
some intermediate conclusions of the rules will not be derived, pruning the search space.


The clausification algorithm by Nonnengart and Weidenbach \cite{nw-01-small-cnf}
eagerly simplifies the input problem using Boolean equivalences before clausifying it. For example,
the formula $\cst p\iand\itrue$ is replaced by $\cst p$. To simplify nested Booleans we implemented the rule \infname{BoolSimp}
parameterized by a set of rewrite rules E:

\[
\namedsimp{BoolSimp}{C[\sigmaterm{f}]}{C[\sigmaterm{g}]}
\]
where $f \longrightarrow g \in E$ and $\sigma$ is any substitution. In the current implementation of Zipperposition, $E$ contains the rules
described by Nonnengart and Weidenbach \cite[Sect.~3]{nw-01-small-cnf}. This set contains
the rules describing how each logical symbol behaves when given arguments $\itrue$ or $\ifalse$: for example, 
it includes $(\itrue \iimplies p)~\longrightarrow~p $ and $(p \iimplies \itrue) \longrightarrow \itrue$. %full list visa report Sect 4.2
Leo-III implements a similar rule, called \textsf{simp} \cite[Sect.~4.2.1.]{as-18-phd}.


Our decision to represent negative atoms as negative equations was motivated by
the need to alter Zipperposition's earlier behavior as little as possible. 
Namely, negative atoms were not used as literals that can be used
to paramodulate from, and as such added to the laziness of the superposition calculus.
However, it might be useful to consider unit clauses of the form $f \noteq \itrue$
as $f \eq \ifalse$ to strengthen rewriting. To that end, we have introduced the following
rule:
%
$$ \namedsimp{BoolDemod}{f \noteq \itrue \qquad C[\sigmaterm{f}]}{f \noteq \itrue \qquad C[\ifalse]} $$

\subsection{Higher-Order Considerations}
\label{subsect:bool:core}
\looseness=-1
To achieve refutational completeness of higher-order resolution and similar
calculi, it is necessary to instantiate variables with result type $o$,
\emph{predicate variables}, with arbitrary formulas
\cite{as-18-phd,pa-01-classical-ty-thy}. Fortunately, we can approximate the
formulas using a complete set of logical symbols (e.g., $\inot$, $\iforall$, and
$\iand$). Since such an approximation is not only necessary for completeness of
some calculi, but possibly useful in practice, we implemented the \emph{primitive
instantiation} (PI) rule:
%
$$ \namedinference{PI} {C \llor (\lam{\tuple{x}{}}{P \, \tuplen{s}}) \, \dot{\eq}
\, t} {\substcl{\{ P
\mapsto f \}}{C \llor (\lam{\tuple{x}{}}{P \, \tuplen{s}}) \, \dot{\eq} \, t}  } $$
%
where $P$ is a free variable of
the type $\tau_1 \rightarrow \cdots \rightarrow \tau_n \rightarrow o$. 
Choosing a different $f$ that instantiates $P$, we can balance between
explosiveness of approximating a complete set of logical symbols and
incompleteness of pragmatic approaches. We borrow the notion of imitation from
higher-order unification jargon (Sect.~\ref{sec:unif:the-unification-procedure}), and we say
that the term $\lam{\tuple{x}{m}}{\cst{f} \, (Y_1 \, \tuple{x}{m}) \cdots (Y_n
\, \tuple{x}{m}) }$ is an \emph{imitation} of constant $\cst{f} : \tau_1
\rightarrow \cdots \rightarrow \tau_n \rightarrow \tau$ for some variable $Z$ of type $\nu_1
\rightarrow \cdots \rightarrow \nu_m \rightarrow \tau$. Variables $\tuplen{Y}$
are fresh free variables, where each $Y_i$ has the type $\nu_1 \rightarrow
\cdots \rightarrow \nu_m \rightarrow \tau_i$; variable $x_i$ is of type $\nu_i$.
\pagebreak[2]

\looseness=-1
Rule \infname{PI} was already implemented by Simon Cruanes in Zipperposition,
before we started our modifications. Its implementation contains the following modes that generate
sets of possible terms $f$ for $P: \tau_1 \rightarrow \cdots \rightarrow \tau_n
\rightarrow o$: \emph{Full}, \emph{Pragmatic}, and $\mathit{Imit}_{\star}$. In  $\mathit{Imit}_{\star}$,
$\star$ is an element of a set of logical constants $S = \{ \iand, \ior,
\mathop{\ieq}\typeargs{\alpha}, \inot, \iforall, \iexists \}$. Mode \emph{Full}
generates imitations (for $P$) of all elements of $S$. Mode $\emph{Pragmatic}$
generates imitations of $\inot$, $\itrue$, and $\ifalse$; if there exist indices $i,j$
such that $i\not= j$ and  $\tau_i = \tau_j$, then it generates $\lam{\tuplen{x}}{x_i
\ieq x_j}$; if there exist indices $i,j$ such that $i \not= j$, and $\tau_i =
\tau_j = o$, then it generates $\lam{\tuplen{x}}{x_i \iand x_j}$ and
$\lam{\tuplen{x}}{x_i \ior x_j}$; if for some $i$, $\tau_i = o$, then it
generates $\lam{\tuplen{x}}{x_i}$. Mode $\mathit{Imit}_\star$ generates
imitations of $\itrue$, $\ifalse$, and $\star$. In addition, $\mathit{Imit_{\iforall\iexists}}$ generates imitations
of both $\iforall$ and $\iexists$.

\looseness=-1
While experimenting with our implementation we noticed some proof patterns that
led us to come up with the following modifications. First, it often suffices to
perform \infname{PI} only on initial clauses---which is why we allow the rule to
be applied only to the clauses created using at most $k$ generating inferences.
Second, if the rule was used in the proof, its premise is usually only used as
part of that inference---which is why we implemented a version of \infname{PI}
that removes the clause after all possible \infname{PI} inferences have been
performed. We observed that the mode \emph{Imit}$_\star$ is useful in practice
since often approximation of a single logical symbol suffices.

\looseness=-1
The axiom of choice is notoriously difficult to handle efficiently in higher-order provers. Andrews formulates
this axiom as $\iforall p.\, (\iexists
x.\, p \, x) \iimplies p \, (\varepsilon \, p)$, where $\varepsilon :
\forallty{\alpha}(\alpha \rightarrow o) \rightarrow \alpha$ denotes the \emph{choice
operator} \cite{pa-01-classical-ty-thy}. After clausification, this axiom becomes $P \, X \noteq \itrue \llor P
\, (\varepsilon \, P) \eq \itrue$. Since the term $P \, X$ matches any Boolean
term in the proof state, this axiom is very explosive. Therefore, Leo-III
\cite{sb-21-leo3} heuristically recognizes symbols that correspond to choice and deals with
them on the calculus level. Namely, whenever a clause $C = P \, X \noteq \itrue \llor P \, (\cst{f} \,
P) \eq \itrue$ is chosen for processing, $C$ is removed from the proof state and $\cst{f}$ is
added to the set of choice functions $\mathit{CF}$ (which initially contains just
$\varepsilon$). Later, elements of $\mathit{CF}$ will be used to heuristically
instantiate the axiom of choice. We reused the method of recognizing choice
functions, but generalized the rule for creating the instance of
the axiom (assuming $\xi \in \mathit{CF}$ and $X$ and $z$ are fresh variables):
%
$$\namedinference{Choice}
               {C[\xi \, t]}
               { X \, (t \, Y) \noteq \itrue \llor X \, (t \, (\xi \, (\lam{z}{X \, (t \, z)})) ) \eq \itrue }$$
%
Let $D$ be the conclusion of \infname{Choice}. The fresh variable $X$ in $D$ acts as
an arbitrary context around $t$, the chosen instantiation for $P$ from the axiom of choice;
the variable $X$ can later be replaced by imitations of logical symbols to create more
complex instantiations of the choice axiom. To generate useful instances early, we create two instances: $\substcl{\{X
\mapsto \lam{z}{z}\}}{D}$ and $\substcl{\{X \mapsto \lam{z}{\inot\, z}\}}{D}$. Then, depending on
Zipperposition options, $D$ will either be deleted or kept. Note that $D$
will not subsume its instances, as the matching algorithm Zipperposition uses favors performance over completeness and is
thus too weak for this \cite[Sect.~6]{bbtvw-21-sup-lam}
\looseness=-1

Most provers natively support both functional and Boolean extensionality
reasoning: Bhayat et al. \cite{br-20-full-sup-w-combs} modify first-order
unification to return unification constraints consisting of pairs of terms of
functional type, whereas Steen relies on the unification rules of Leo-III's
calculus \cite[Sect.~4.3.3]{as-18-phd} to deal with extensionality. As a
pragmatic extension of the \lsup{} calculus, Bentkamp et al.
\cite{bbtvw-21-sup-lam} propose to alter the core generating inference rules of
superposition (Sect.~\ref{sec:pre:rules}) to support functional extensionality.
Instead of requiring that terms involved in the inference are unifiable, it is
required that they can be decomposed into \emph{disagreement pairs} such that at
least one of the disagreement pairs is of functional type. Disagreement pairs of
terms $s$ and $t$ of the same type are defined inductively using function
$\textsf{dp}$: $\textsf{dp}(s,t) = \emptyset$ if $s$ and $t$ are equal;
$\textsf{dp}(a \, \tuplen{s}, b \, \tuple{t}{m}) = \{(a \, \tuplen{s}, b \,
\tuple{t}{m})\}$ if $a$ and $b$ are different heads; $\textsf{dp}(\lam{x}{s},
\lam{y}{t}) = \{ (\lam{x}{s}, \lam{y}{t}) \}$; $\textsf{dp}(a \, \tuplen{s}, a
\, \tuplen{t}) = \bigcup_{i=1}^{n} \textsf{dp}(s_i, t_i)$. Then the
extensionality rules are stated as follows:
\pagebreak[2]
\begin{align*}
  & \namedinference{AbsSup}
  {s \eq t \llor C \qquad u[s'] \mathrel{\dot{\eq}} v \llor D}
  {\sigmacl{s_1 \noteq s'_1 \llor \cdots \llor s_n \noteq s'_n \llor  u[t] \mathrel{\dot{\eq}} v \llor C \llor D}} \\
  &\namedinference{AbsER}{s \noteq s' \llor C}{\sigmacl{s_1 \noteq s'_1 \llor \cdots \llor s_n \noteq s'_n \cdots \llor  C}} \\
  &\namedinference{AbsEF}{s \eq t \llor  s' \eq u \llor C}{\sigmacl{s_1 \noteq s'_1 \llor \cdots \llor s_n \noteq s'_n \llor t \noteq u \llor  s' \eq u \llor C}}
\end{align*}

\noindent\looseness=-1
In each of the rules, $\sigma$ is an MGU of the types of $s$ and
$s'$, and $\textsf{dp}(\sigmaterm{s} ,\sigmaterm{s'}) = \{ (s_1, s'_1), \ldots,\allowbreak
(s_n, s'_n)\}$. Rules \infname{AbsSup}, \infname{AbsER}, and \infname{AbsEF} are
extensional versions of superposition, equality resolution, and equality
factoring (Sect.~\ref{sec:pre:rules}). All side conditions for extensional rules are the same as for
the standard rules, except that the condition that $s$ and $s'$ are unifiable is
replaced by the condition that $n>0$ and at least one $s_i$ is of functional type. By \infname{Abs} we denote the union of
these three rules. We extend \infname{Abs} to support Boolean extensionality by
requiring that at least one $s_i$ is of functional or type $o$, and adding the
condition ``$\textsf{dp}(f, g) = \{(f,g)\}$ if $f$ and $g$ are different
formulas'' to the definition of $\textsf{dp}$. If another condition of \textsf{dp}'s 
definition is also applicable, the newly added one is preferred. In the rest of the
text \infname{Abs} refers to the version that supports Boolean extensionality.

\looseness=-1
Consider the clause $\cst{f} \, (\inot\cst{p} \ior \inot\cst{q}) \noteq \cst{f} \,
(\inot (\cst{p} \iand \cst{q}))$. This problem is unsatisfiable, as
the arguments of $\cst{f}$ on the different sides of the disequation are Boolean-extensionally
equal. Without the \infname{Abs} rules Zipperposition relies on the
\infname{Cases}(\infname{Simp}) and \infname{IC} rules to derive the empty
clause. On the other hand, \infname{AbsER} generates~$C = \inot\cst{p} \ior \inot\cst{q} \noteq
\inot (\cst{p} \iand \cst{q})$. Then, $C$ gets clausified using
\infname{IC}, effectively reducing the problem to $\inot (\inot\cst{p} \ior
\inot\cst{q} \iequiv \inot (\cst{p} \iand \cst{q}))$, which is first-order.

\begin{sloppypar}
  Zipperposition restricts \infname{AbsSup} by
  requiring that $s$ and $s'$ are not of function or Boolean type. If the terms are of function type, our experience is
  that a better treatment of function extensionality is to apply fresh free
  variables (or Skolem terms, depending on the sign
  \cite{bbtvw-21-sup-lam}) to both sides of a (dis)equation to reduce it to
  a first-order literal; Boolean extensionality is usually better supported by
  applying \infname{IC} on the top-level Boolean term. Thus, for the following
  discussion we can assume $s$ and $s'$ are not $\lambda$-abstractions or formulas. Then, $\infname{AbsSup}$ is applicable if $s$
  and $s'$ have the same head, and a functional or Boolean subterm. To speed up
  retrieval of such terms, we added an index that maps symbols to positions in
  clauses where they appear as a head of a term that has a functional or Boolean
  subterm. This index will be empty for first-order problems, incurring no
  overhead if extensionality reasoning is not needed. One more restriction we implemented is that we do not apply the \infname{Abs} rules if all
  disagreement pairs have at least one side whose head is a variable; those will
  be dealt with more efficiently using the core rules of the superposition
  calculus. To simplify the resulting clauses of \infname{Abs} rules, we also
  eagerly remove literals $s_i \not\eq s'_i$ using a simplifying version of
  \infname{ER} (Sect.~\ref{sec:pre:rules}). In particular, we apply the first
  unifier returned by the terminating, pragmatic variant of unification
  algorithm described in Sect.~\ref{sec:unif:the-unification-procedure}, and
  remove the premise after the conclusion of \infname{ER} has been computed.
\end{sloppypar}

Expressiveness of higher-order logic allows users to define equality using a single axiom,
called Leibniz equality \cite{pa-01-classical-ty-thy}:
%
$ \iforall xy. \,
   (\iforall p. \, p \, x \iimplies p \, y) \iimplies x \ieq y$.
%
Intuitively, it embodies the principle that if two terms are indistinguishable by propositions,
they must be equal.
Leibniz equality often appears in TPTP problems  \cite{gs-17-tptp}. Since modern provers have native support
for equality, it is usually beneficial to recognize and replace occurrences of Leibniz equality.

Before we did our modifications, Zipperposition had a powerful rule that
recognizes clauses that contain variations of Leibniz equality and instantiates
them with native equality. This rule was designed by Simon Cruanes, and to the
best of our knowledge, it has not been documented so far. With his permission
we describe this rule as follows:
\pagebreak[2]
%
$$\namedinference{ElimPredVar}{P \, \overline{s}_n^1 \eq \itrue \llor \cdots \llor
    P \, \overline{s}_n^i \eq \itrue \llor P \, \overline{t}_n^1 \noteq \itrue \llor
    \cdots \llor P \, \overline{t}_n^j \noteq \itrue \llor C} 
    {\sigmacl{P \, \overline{s}_n^1
    \eq \itrue \llor \cdots \llor P \, \overline{s}_n^i \eq \itrue} \llor C} $$
% \pagebreak[2]
where $P$ is a free variable, that does not occur in any $s_k^l$ or $t_k^l$, or
in $C$; $\sigma$ is defined as $\{ P \mapsto \lam{\tuplen{x}}{\pmb{\bigvee}_{k=1}^{j}
(\pmb{\bigwedge}_{l=1}^{n} x_l \eq t^k_l ) } \}$. 

\looseness=-1
To better understand how this rule removes variable-headed negative literals,
consider the clause $C = P \, \cst{a}_1 \, \cst{a}_2 \eq \itrue \llor P \,
\cst{b}_1 \, \cst{b}_2 \not\eq \itrue \llor P \, \cst{c}_1 \, \cst{c}_2 \not\eq
\itrue$. The rule \infname{ElimPredVar}
will generate $\sigma = \{ P \mapsto \lam{xy}{(x \ieq \cst{b}_1
\iand y \eq \cst{b}_2) \ior (x \ieq \cst{c}_1 \iand y \ieq \cst{c}_2)  } \}$.
After applying $\sigma$ to $C$ and subsequent $\beta$-reduction, the negative literal
$  P \, \cst{b}_1 \, \cst{b}_2 \not\eq \itrue$ will reduce to 
$ (\cst{b}_1 \ieq \cst{b}_1 \iand \cst{b}_2 \ieq \cst{b}_2) \ior (\cst{b}_1 \ieq \cst{c}_1 \iand \cst{b}_2 \eq \cst{c}_2) \noteq \itrue $,
which is equivalent to $\ifalse$. Thus, we can remove this literal and all negative literals
of the form $P \, \overline{t}_n \noteq \itrue$ from $C$ and apply $\sigma$ to the remaining ones.

The previous rule removes all variables occurring in disequations in one
attempt. We implemented two rules that behave more lazily, inspired by the ones present in Leo-III and
Satallax:
%
\begin{align*}
&\namedinference{ElimLeibniz$+$}
{P \, \tuplen{s} \eq \itrue \llor P \, \tuplen{t} \noteq \itrue \llor C}
{\sigmacl{s_i \eq t_i \llor C}}
&&
\namedinference{ElimLeibniz$-$}
{P \, \tuplen{s} \noteq \itrue \llor P \, \tuplen{t} \eq \itrue \llor C}
{\substcl{\sigma'}{s_i \eq t_i \llor C}}&
\end{align*}
%
\looseness=-1
where $P$ is a free variable that does not occur in $t_i$, $\sigma = \{ P
\mapsto \lam{\tuplen{x}}{x_i \eq t_i} \}$, and $\sigma' = \{ P \mapsto
\lam{\tuplen{x}}{\neg (x_i \eq t_i)} \}$. \infname{ElimLeibniz} directly applies
Leibniz equality: When the premise of Leibniz equality is clausified it
becomes $P \, X \not\eq \itrue \llor P \, Y \eq \itrue$. The rule then tries to find
a similar two-literal subclause in a clause $C$, and if successful instantiates $C$ with a
substitution that asserts equality of $X$ and $Y$.


This rule differs from
\infname{ElimPredVar} in three ways. First, it can replace a predicate variable
both with equality and disequality. Second, due to its simplicity, it usually
does not require \infname{IC} as the following step. Third, it imposes much
weaker conditions on $P$. However, removing all negative variables in one step
might improve performance  by generating fewer intermediate clauses.  Coming
back to the example of the clause $C = P \, \cst{a}_1 \, \cst{a}_2 \eq \itrue \llor
P \, \cst{b}_1 \, \cst{b}_2 \not\eq \itrue \llor P \, \cst{c}_1 \, \cst{c}_2
\not\eq \itrue$, we can apply \infname{ElimLeibniz$+$} using the substitution
$\sigma = \{ P \mapsto \lam{xy}{x \eq \cst{b}_1} \}$ to obtain the clause $C' =
\cst{a}_1 \eq \cst{b}_1 \llor \cst{a}_1 \not\eq \cst{c}_1$, which is considerably simpler than
the one obtained by \infname{ElimPredVar}.



\subsection{Additional Rules}
\label{subsect:bool:core}
 
Zipperposition's pragmatic, incomplete unification algorithm 
%\cite{OUR UNIFICATION PAPER}
uses a flattened representation of terms with logical operators $\iand$ and $\ior$
as heads to unify terms that are not unifiable modulo $\alpha\beta\eta$-equivalence, but
are unifiable modulo associativity and commutativity of $\iand$ and $\ior$. Let
$\Diamond$ denote either $\iand$ or $\ior$. When the unification algorithm is given
two terms $\Diamond \, \tuplen{s}$ and $\Diamond \, \tuplen{t}$, where neither
of $\tuplen{s}$ nor $\tuplen{t}$ contains duplicates, it performs the
following steps: First, it removes all terms that appear in both
$\tuplen{s}$ and $\tuplen{t}$ from the two argument tuples.
%Let $\tuple{s}{m}$ and $\tuple{t}{m}$ denote the results of previous step.
Next, the remaining terms are sorted first by their head term and then their syntactic weight. Finally,
an attempt is made to unify sorted lists pairwise.
%
As an example, consider the problem of unifying the pair ${\iand} \,
(\cst{p} \, \cst{a}) \; (\cst{q} \, (\cst{f} \, \cst{a})) \unif \; {\iand} \;
(\cst{q} \, (\cst{f} \, \cst{a})) \; (R \, (\cst{f} \, (\cst{f} \,
\cst{a})))$ where $R$ is a free variable. If the arguments of $\iand$ are
simply sorted as described above, we would try to unify $\cst{p} \, \cst{a}$
with $\cst{q} \, (\cst{f} \, \cst{a})$, and fail to find a unifier. However, by
removing the term  $\cst{q} \, (\cst{f} \, \cst{a})$ from the argument lists, we
will be left with the problem $\cst{p} \, \cst{a} \unif  R \, (\cst{f} \, (\cst{f}
\, \cst{a}))$ which has a unifier. This approach enables us to find more unifiers
than by simple syntactic unification.

The winner of the higher-order theorem division of the 2019 edition of CASC \cite{gs-19-casc27}, Satallax \cite{cb-12-satallax}, has one
crucial advantage over Zipperposition: it is based on higher-order
tableaux, and as such it does not require formulas to be converted to clauses. The advantage of tableaux is that once it instantiates a variable with a
term, this instantiation naturally propagates through the whole formula. In
Zipperposition, which is based on \lsup{}, the original formula
is clausified and instantiating a variable in a clause $C$ does not automatically instantiate
it in all clauses that are results of clausification of the same formula as $C$.
To mitigate this issue, we have created extensions of equality resolution and equality factoring
that take Boolean extensionality into account:
%
\begin{align*}
  & \namedinference{BoolER}
  { s \eq s' \llor C}
  {\sigmacl{C}}
  &&
  \namedinference{BoolEF$+-$}
  {P\,\tuplen{s} \eq \itrue \llor s' \not\eq \itrue \llor C}
  {\sigmacl{s' \not\eq \itrue \llor C}} \\
  & \namedinference{BoolEF$-+$}
  {P\,\tuplen{s} \not\eq \itrue \llor s' \eq \itrue \llor C}
  {\sigmacl{s' \eq \itrue \llor C}} &&
  \namedinference{BoolEF$-$$-$}
  {P\,\tuplen{s} \not\eq \itrue \llor s' \not\eq \itrue \llor C}
  {\sigmacl{s' \not\eq \itrue \llor C}}
\end{align*}
%
All side conditions except for the ones concerning the unifiability of terms are
as in the original equality resolution and equality factoring rules. In rule
\infname{BoolER}, $\sigma$ is a unifier of $ s $ and $ \inot s' $. In the $+-$
and $-+$ versions of $\infname{BoolEF}$, $\sigma$ unifies $ P \, \tuplen{s}
$ and $\inot s'$, and in the remaining version it unifies $ P \, \tuplen{s} $ and
$s'$. Intuitively, these rules bring Boolean (dis)equations in the appropriate
form for application of the corresponding base rules. In particular,
for \infname{BoolER} we interpret the equation $s \eq s'$ as $s \not\eq \inot s'$, which
allows us to simulate \infname{ER} inference from Sect.~\ref{sec:pre:rules}. 
For the \infname{BoolEF} family of rules, we convert disequations into equations
using the trick of negating one side to simulate \infname{EF}.
% It suffices to consider 
% literals of the form $s \eq s'$ for \infname{BoolER} as Zipperposition rewrites
% $s \iequiv t \eq \itrue$ and $\inot(s \iequiv t) \not\eq \itrue$ to $s \eq t$ (and does analogous
% rewriting into $ s \not\eq t $).
The rule \infname{BoolER} only considers literals of the form $s \eq t$. 
Equivalences $s \iequiv t = \itrue$ and disequivalences $(s \iequiv t) \not\eq \itrue$ are automatically simplified to $s \eq t$ or $s \not\eq t$.
The example  \verb|SET557^1| in Sect.~\ref{sect:bool:examples} illustrates how 
the \infname{Bool} family of rules helps solve problems that get obfuscated by clausification.

Another approach to mitigate harmful effects of immediate clausification
is to delay it as long as possible. Following the approach by 
Ganzinger and Stuber \cite{gs-05-boolsup}, we represent every input formula $f$ as a unit clause $f \eq \itrue$ and use the following
delayed clausification (\infname{DC}) rules:

\vspace{2\jot}
\begin{tabular}{ccc}
  $\namedsimp{DC$_{\iand}$}
  { (g \iand h) \eq \itrue \llor C}
  { g \eq \itrue \llor C \quad h \eq \itrue \llor C }$ &
  $\namedsimp{DC$_{\ior}$}
  { (g \ior h) \eq \itrue \llor C}
  { g \eq \itrue \llor h \eq \itrue  \llor C }$ &
  $\namedsimp{DC$_{\iimplies}$}
  { (g\iimplies h) \eq \itrue \llor C}
  { g\not\eq \itrue \llor h \eq \itrue  \llor C } $ \\[2\jot]
  $\namedsimp{DC$_{\inot}$}
  { (\inot g) \eq \itrue \llor C}
  { g\not\eq \itrue \llor C}$ &
  $\namedsimp{DC$_{\iforall}$}
  { (\iforall x. \, g) \eq \itrue \llor C}
  { \{x \mapsto Y\}(g) \llor C }$ &
  $\namedsimp{DC$_{\iexists}$}
  { (\iexists x.\, g) \eq \itrue \llor C}
  { \{x \mapsto \cst{sk}\typeargs{\tuple{\alpha}{}} \, \tuplen{Y} \}(g) \llor C }$ 
  \\[2\jot]
  \multicolumn{3}{c}{$\namedsimp{DC$_\eq$}
  { g\eq h \llor C}
  { g\not\eq \itrue \lor h\eq\itrue \llor C \quad g \eq \itrue \lor h\not\eq\itrue \llor C}$}
\end{tabular}
\vspace{2\jot}

\looseness=-1
In \infname{DC}$_\eq$ we require both $g$ and $h$ to be formulas and at least one of
them not to be $\itrue$. In \infname{DC}$_{\iforall}$, $Y$ is a fresh variable, and in
\infname{DC}$_{\iexists}$, $\cst{sk}$ is a fresh symbol and $\tuple{\alpha}{}$ and $\tuplen{Y}$ consists of all the type and term variables occurring
freely in $\iexists x.\, g$. The rules described above are as given by Ganzinger and Stuber (adapted to
our setting), with the omission of rules for negative literals (of the form $ f \not\eq
\itrue$), which are easy to derive and which can be found in their work \cite{gs-05-boolsup}.

\looseness=-1
Naive application of the \infname{DC} rules can result in exponential blowup in
problem size. To avoid this, we rename formulas $f$ that have repeated
occurrences by introducing predicates $\cst{p} \, \tuple{X}{n}$ replacing them, where
$\tuplen{X}$ consists of all free variables of $f$. We keep the count of all nonatomic
formulas occurring as either side of a literal. Before applying the \infname{DC}
rules on a clause $f \mathrel{\dot{\eq}} \itrue \llor C $, we check whether the
number of occurrences of $f$  exceeds the threshold $k$. If it does, based on the
polarity of the literal $f \mathrel{\dot{\eq}} \itrue$, we add the clause
$\cst{p} \, \tuplen{Y} \not\eq \itrue \llor f \eq \itrue$ (if the literal is
positive) or $\cst{p} \, \tuplen{Y} \eq \itrue \llor f \not\eq \itrue$ (if the
literal is negative), where $\tuplen{Y}$ are all free variables of $f$ and
$\cst{p}$ is a fresh symbol. Then, we replace the clause $f \mathrel{\dot\eq}
\itrue \llor C$ by $ \cst{p} \, \tuplen{Y} \mathrel{\dot\eq} \itrue \llor C$.


Before the number of occurrences of $f$ is checked, we first check (using a
fast, incomplete matching algorithm, used for simplification and subsumption) if there is a
formula $g$, for which definition was already introduced, such that
$\sigmaterm{g} = f$, for some substitution $\sigma$. This check can have three
outcomes. First, if the definition $\textsf{q} \, \overline{X}_n$ was already
introduced for $g$ with the same polarity as $f \, \dot{\eq} \,
\itrue$, then $f$ is replaced by $\sigmaterm{\cst{q} \, \tuplen{X}}$. Second, if
the definition was introduced, but with different polarity, we create the clause
defining $g$ with the missing polarity, and replace $f$ with $\sigmaterm{\cst{q}
\, \tuplen{X}}$. Last, if there is no renamed formula $g$ generalizing $f$,
we check if the number of occurrences of $f$ exceeds the threshold $k$.

\looseness=-1
In addition to reusing names for formula definitions, we reuse the Skolem
symbols introduced by the \infname{DC}$_{\iexists}$ rule. When
\infname{DC}$_{\iexists}$ is applied to $f = \iexists x.\, f'$, we check if there is
a Skolem $\cst{sk}\typeargs{\tuple{\alpha}{m}} \, \tuplen{Y}$ introduced for a
formula $g = \iexists x.\, g'$, such that $\sigmaterm{g} = f$. If so, the symbol
$\cst{sk}$ is reused and $\iexists x.\, f'$ is replaced by $\{x \mapsto
\sigma(\cst{sk}\typeargs{\tuple{\alpha}{m}} \, \tuplen{Y}) \}(f')$. Renaming and
name reusing techniques are inspired by the VCNF algorithm described by Reger et
al. \cite{rsv-16-vcnf}.



% \namedinference{LC$_\eq$}
% { f \eq g \llor C}
% { f \not\eq \ifalse \llor g \eq \ifalse \llor C \quad f \eq \ifalse \llor g \not\eq \ifalse \llor C } &&
% \namedinference{LC$_\Rightarrow$}

% Rule \infname{BoolSimp} proved to be very useful in our initial experimentation
% as it normalizes many equivalent terms to the same Boolean term. Stronger normalization can be achieved if nested formulas
% are transformed into \emph{negation normal form} (\emph{NNF}). Formula is in negation normal form if it is built from atoms and their
% negations using only $\wedge$ and $\vee$ \cite{nw-01-small-cnf}. By
% adding
% \kern\abovedisplayskip

% \noindent
% \begin{center}
% \begin{minipage}[t]{.25\textwidth}
%   \begin{center}
%     $\neg\left(p \land q \right) \longrightarrow \neg p \vee \neg q$ \\
%     $\neg\left(p \lor q \right)  \longrightarrow \neg p \land \neg q$
%   \end{center}
%   \end{minipage}%
%   \begin{minipage}[t]{.43\textwidth}
%   \begin{center}
%     $ \neg \left( p \Leftrightarrow q \right) \longrightarrow \left(p \lor q \right) \land \left(\neg p \lor \neg q \right) $ \\
%     $ p \Leftrightarrow q \longrightarrow \left(\neg p \lor q\right) \land \left(p\lor \neg q\right) $
%   \end{center}
%   \end{minipage}%
%   \begin{minipage}[t]{.27\textwidth}
%     \begin{center}
%       $p \Rightarrow q  \longrightarrow \neg p\lor q$
%     \end{center}
%   \end{minipage}
% \end{center}
% \noindent to the set $E$ of rewrite rules of \infname{BoolSimp} we transform a formula to NNF. We call \infname{BoolSimp}
% with set $E$ extended in this way \infname{NnfSimp}. Coming back to our previous example
% of the clause $C = \cst{f} \, (\neg\cst{p} \lor \neg\cst{q}) \noteq
% \cst{f} \, (\neg (\cst{p} \lland \cst{q}))$, \infname{NnfSimp} simplifies this problem greatly:
% after normalizing terms, a simple equality resolution with an empty substitution
% will derive the empty clause.

\looseness=-1
Rules \infname{Cases} and \infname{CasesSimp} deal with Boolean terms,
but we need to rely on extensionality reasoning to deal with $\lambda$-abstractions
whose bodies have type $o$. Using the observation that the formula $\iforall \tuplen{x}. \, f$ 
implies that $\lam{\tuplen{x}}{f}$ is functional-extensionally equal to $\lam{\tuplen{x}}{\itrue}$ (and similarly, if
$\iforall \tuplen{x}. \, \inot f$, then $\lam{\tuplen{x}}{f} \ieq \lam{\tuplen{x}}{\ifalse})$, we
designed the following rule (where $\tuplen{x}$ consists of all loosely bound variables of $f$ ):
%\pagebreak[2]
%
$$ \namedinference{Interpret$\lambda$}{C[\lam{\tuplen{x}}{f}]}
{(\iforall {\tuplen{x}}.\, f) \noteq \itrue \llor C[\lam{\tuplen{x}}{\itrue}] \qquad
(\iforall {\tuplen{x}}.\, \inot f) \noteq \itrue \llor C[\lam{\tuplen{x}}{\ifalse}]} $$
%

\section{Alternative Approaches} 
\label{sect:bool:alternative}

\looseness=-1
An alternative to modifications of the prover needed to support the rules
described above is to treat Booleans as yet another theory. Since the theory of Booleans
is finitely axiomatizable, stating those axioms instead of creating special
rules might seem appealing. Another approach
is to preprocess nested Booleans by hoisting them to the top level.

\ourpara{Axiomatization}
\looseness=-1
A simple axiomatization of the theory of Booleans is given by Bentkamp et al.\ \cite{bbtvw-21-sup-lam}.
Following their approach, we introduce the proxy type $\typ{bool}$, which corresponds to $o$, to the
signature. We define proxy symbols $\cst{t}, \cst{f}, \cst{not}, \cst{and}, \cst{or},
\cst{impl}, \cst{equiv},\allowbreak \cst{forall}, \cst{exists}, \cst{choice}$, and $\cst{eq}$ which 
correspond to the homologous logical constants from Sect.~\ref{sect:bool:background}. In their
type declarations, $o$ is replaced by $\typ{bool}$.
  
\looseness=-1
To make this chapter self-contained we include the axioms from Bentkamp et al. \cite{bbtvw-21-sup-lam}.
Definitions of symbols are computational in nature: Symbols are characterized by their behavior on $\cst{t}$ and $\cst{f}$.
This also reduces interferences between different axioms. The axioms are listed as follows:

\kern\abovedisplayskip
 
\noindent
\begin{minipage}[t]{.2\textwidth}
\begin{center}
$\cst{t} \noteq \cst{f}$ \\
$X \eq \cst{t} \llor X \eq \cst{f}$ \\
$\cst{not} \, \cst{t} \eq \cst{f}$ \\
$\cst{not} \, \cst{f} \eq \cst{t}$ \\
$\cst{and} \, \cst{t} \, X \eq X $ \\
$\cst{and} \, \cst{f} \, X \eq \cst{f} $ \\ 
\end{center}
\end{minipage}%
\begin{minipage}[t]{.35\textwidth}
\begin{center}
$\cst{or} \, \cst{t} \, X \eq \cst{t} $ \\
$\cst{or} \, \cst{f} \, X \eq X $ \\
$\cst{impl} \, \cst{t} \, X \eq X $ \\
$\cst{impl} \, \cst{f} \, X \eq \cst{t} $ \\
$X \noteq Y \llor \cst{eq}\typeargs{\alpha}\;X\;Y \eq \cst{t}$ \\
$X \eq Y \llor \cst{eq}\typeargs{\alpha}\;X\;Y \eq \cst{f}$
\end{center}
\end{minipage}%
\begin{minipage}[t]{.45\textwidth}
  \begin{center}
    $\cst{equiv} \, X\, Y \eq \cst{and} \, (\cst{impl} \, X\, Y) \, (\cst{impl} \, Y \, X) $ \\
    $\cst{forall}\typeargs{\alpha}\,(\lambda x.\; \cst{t}) \eq \cst{t}$ \\
    $Y \eq (\lambda x.\; \cst{t}) \llor \cst{forall}\typeargs{\alpha}\;Y \eq \cst{f}$ \\
    $\cst{exists}\typeargs{\alpha}\;Y \eq \cst{not} \, (\cst{forall}\typeargs{\alpha}\;(\lambda x. \, \cst{not} \, (Y \, x)))$ \\
    $Y \, X \eq \cst{f} \llor Y \, (\textsf{choice}\typeargs{\alpha} \, Y) \eq \cst{t}$ \\
   
  \end{center}
\end{minipage}

\vspace{0.5em}

\ourpara{Preprocessing Booleans}
\looseness=-1
Kotelnikov et al.\ extended VCNF, Vampire's algorithm for clausification, to support
nested Booleans \cite{kotelnikov-16-fool}. As explained in Sect.~\ref{sec:ehoh:preprocessing}, we extended
the clausification algorithm of Ehoh to support nested Booleans inspired by this VCNF extension.
Zipperposition and Ehoh share the
same clausification algorithm, enabling us to reuse the extension, with one
notable difference:
Unlike in Ehoh, not all nested Booleans different from variables, $\itrue$ and $\ifalse$
will be removed. Namely, Booleans that are below $\lambda$-abstraction
and contain $\lambda$-bound variables will not be preprocessed. They cannot be easily hoisted to the level of an atom in which
they appear, since this process might leak any variables bound in the context
in which the nested Boolean appears. Similar preprocessing techniques are used in other higher-order provers
\cite{wskb-16-effective-norm}.

\section{Examples}
\label{sect:bool:examples}

The TPTP library \cite{gs-17-tptp} contains thousands of higher-order benchmarks, many of them hand-crafted
to point out subtle interferences of functional and Boolean properties of higher-order logic. In this section
we discuss some problems from the TPTP library that illustrate the advantages and disadvantages
of our approach.

During most of the 2010s, the core calculus of the best performing
higher-order prover at CASC was tableaux---a striking contrast from the first-order part
of the competition dominated by superposition-based provers. TPTP
problem \verb|SET557^1| might shed some light on why tableaux-based provers
excel on higher-order problems.
This problem conjectures that there is
no surjection from a set to its power set: 
$$ \neg (\iexists x. \, \iforall y. \,
\iexists z. \, x \, z \eq y) $$ 
After negating the conjecture and
clausification this problem becomes $ \cst{sk}_1 \, (\cst{sk}_2 \, Y) \eq Y   $
where $\cst{sk}_1$ and $\cst{sk}_2$ are Skolem symbols. Then, we can use the
\infname{ArgCong} rule \cite{bbtvw-21-sup-lam} which applies
fresh variable $W$ to both sides of the equation, yielding clause $ C = \cst{sk}_1 \, (\cst{sk}_2 \, Y) \, W
\eq Y \, W$. Superposition-based higher-order theorem
provers (such as Leo-III, Vampire, and Zipperposition) will split this clause
into two clauses $C_1 = \cst{sk}_1 \, (\cst{sk}_2 \, Y) \, W \not\eq \top \llor Y
\, W \eq \top $ and $ C_2 =  \cst{sk}_1 \, (\cst{sk}_2 \, Y) \, W \eq \top \llor
Y \, W \not\eq \top $. This clausification step makes the problem
considerably harder. Namely, the clause $C$ instantiated with the substitution $
\{ Y \mapsto \lamx{\inot (\cst{sk}_1 \, x \, x)}, W \mapsto \cst{sk}_2 \,
(\lamx{\inot (\cst{sk}_1 \, x \, x))} \} $ yields the empty clause. However,
if the original clause is split into two as described above, Zipperposition will
rely on the \infname{PI} rule to instantiate $Y$ with an imitation of $\inot$ and on
equality factoring to further instantiate this approximation. These desired
inferences need to be applied on both new clauses and represent only a fraction
of inferences that can be done with $C_1$ and $C_2$, reducing the chance of a
successful proof attempt. Rule \infname{BoolER} imitates the behavior of
a tableaux prover: It essentially rewrites the clause $C$ into $ \inot (\cst{sk}_1 \,
(\cst{sk}_2 \, Y) \, W) \not\eq Y \, W $, which makes finding the necessary substitution easy and
does not require a clausification step.

\looseness=-1
Combining the rule \infname{(Bool)ER} with dynamic clausification is very fruitful, as
the benchmark \texttt{SYO033\^{}1} illustrates. This problem contains the
single conjecture
$$ \iexists x. \, \iforall y.  \, x \, y \iequiv (\iforall z. \, y \,z)$$ The
problem is easily solved if we replace the variable $x$ with the constant $\forall$.
Moreover, the prover does not have to blindly guess this instantiation. Instead,
pretending that bound variables are free, it can obtain it by unifying $  X \, Y
$ with $ \iforall \, Y$ (which is the $\eta$-short form of $\iforall z. \, Y
\,z$). However, when the problem is clausified, all quantifiers are removed.
Then, Zipperposition finds the proof only if an appropriate instantiation mode of
\infname{PI} is used, and if both clauses resulting from clausifying the negated
conjecture are appropriately instantiated. In contrast, dynamic clausification will
derive the clause $ X \, (\cst{sk} \, X) \not\eq \forall \, (\cst{sk} \, X) $
from the negated conjecture in three steps. Then, equality resolution results in
an empty clause, swiftly finishing the proof without any explosive inferences.
This effect is even more pronounced on problems \verb|SYO287^5| and
\verb|SYO288^5|, in which a critical proof step consists of instantiating a variable
with imitations of $\ior$ and $\iand$. In configurations that do not use dynamic
clausification and \infname{BoolER}, Zipperposition times out in any reasonable
time limit; with those two options it solves the mentioned problems in less than
100\,ms.

In some cases, it is better to preprocess the problem. For example, TPTP problem
\verb|SYO500^1.005| contains many nested Boolean terms:
$$ \cst{f}_0 \, ( \cst{f}_1 \, ( \cst{f}_1 \, ( \cst{f}_1 \, ( \cst{f}_2 \, ( \cst{f}_3 \, ( \cst{f}_3 \, ( \cst{f}_3 \, ( \cst{f}_4 \, \cst{a} ) ) ) ) ) ) ) 
    \eq \cst{f}_0 \, ( \cst{f}_0 \, ( \cst{f}_0 \, ( \cst{f}_1 \, ( \cst{f}_2 \, ( \cst{f}_2 \, ( \cst{f}_2 \, ( \cst{f}_3 \, ( \cst{f}_4 \, ( \cst{f}_4 \, ( \cst{f}_4 \, \cst{a} ) ) ) ) ) ) ) ) ) ) )$$
In this problem, all functions $\cst{f}_i$ are of type $o \rightarrow o$, and constant $\cst{a}$ is of type $o$.
FOOL unfolding of nested Boolean terms will result in an exponential blowup in the problem size. However,
superposition-based theorem provers are well equipped for this issue: Their clausification algorithms use smart simplifications and
formula renaming to mitigate these effects. Moreover, when the problem is preprocessed, the prover is aware of the problem
size before the proving process starts and can adjust its heuristics properly. E, Zipperposition, and Vampire, instructed to perform FOOL unfolding,
solve the problem swiftly, using their default modes. However, if the problem is not preprocessed, Zipperposition struggles to prove it using
\infname{Cases(Simp)}, and due to the large number of (redundant) clauses it creates, succeeds only if specific heuristic choices are made.

\section{Evaluation} 
\label{sect:bool:eval}

We performed an extensive evaluation to determine the  usefulness of our approach. As
our benchmark set, we used all 2606 monomorphic theorems from the TPTP 7.2.0 library,
given in THF format. All of the experiments described in this section were
performed on StarExec \cite{sst-14-starexec} servers with Intel Xeon E5-2609 0 CPUs clocked at 2.40 GHz. The evaluation is separated in two parts that
answer different questions: How useful are the new rules? How 
does our approach compare with state-of-the-art higher-order provers?

\ourpara{Evaluation of the Rules}
\looseness=-1
For this part of the evaluation, we fixed a single well-perform\-ing Zipperposition configuration
called \emph{base} (b). Since we are testing a single configuration, we
used the CPU time limit of \NumberOK{15} s---roughly the time a single configuration is
given in a portfolio mode when participating in CASC. Configuration {b} uses the pragmatic variant
pv$_{1121}^{2}$ (Sect.~\ref{sec:unif:evaluation}) of the unification algorithm described in Chapter~\ref{ch:unif}. 
It enables the \infname{BoolSimp} rule,
the \infname{EC} rule, the \infname{PI} rule in \emph{Pragmatic} mode with $k=2$,
rules \infname{ElimLeibniz} and \infname{ElimPredVar}, the \infname{BoolER} rule, and the \infname{BoolEF} rules. 
To evaluate the usefulness of
all rules described above, we enabled, disabled, or changed the
parameters of a single rule, while keeping all other parameters of b intact. 
In figures that contain sufficiently many different configurations, cells are of the form $n (m)$ where $n$ is the total
number of proved problems by a particular configuration and $m$ is the number of
unique problems that a given configuration solved, compared to the other
configurations in the same figure. Intersections of rows and columns
denote the corresponding combination of parameters.
The result for the base configuration is written in \emph{italics}; the best result
is written in \textbf{bold}.

First, we tested different parameters of the \infname{Cases} and \infname{CasesSimp}
rules. In Figure \ref{fig:cases} we report the results. The columns correspond to
three possible options to choose the subterm on which the inference is performed:
{a} stands for any eligible subterm, {lo} and {li} stand
for leftmost outermost and leftmost innermost subterms, respectively. The rows
correspond to two different rules: 
{b} is the base configuration, which uses \infname{CasesSimp}, and {b$_\text{c}$}
swaps this rule for \infname{Cases}. Although the margin is slim, the results show it is usually preferable to select the leftmost-outermost
subterm.

% \newcommand\HEAD[1]{\hbox to \wd\mybox{\hfill\hbox{#1}\hfill}}
% \newcommand\Z{\phantom{0}}
% \newcommand\MIDLINE{\\[.25ex]\hline\rule{0pt}{3ex}}

\begin{figure}[t]
  \begin{center}
    \def\arraystretch{1.1}%
    \relax{\begin{tabular}{@{}l@{\hskip 2em}c@{\hskip 2em}c@{\hskip 2em}c@{\hskip 2em}c}
      \strut                & a           & lo     & li  
      \MIDLINE
      b                     &  \emph{1646}   &  \textbf{1648}   &  1640  \\
      b$_\text{c}$          &  1644   &  1645   &  1644 
    \end{tabular}}
    \caption{Effect of the \infname{Cases}(\infname{Simp}) rule on success rate }
    \label{fig:cases}
  \end{center}
\end{figure}

\looseness=-1
Second, we evaluated all the modes of the \infname{PI} rule with three values for
parameter $k$: 1, 2, and~8 (Figure \ref{fig:pi}). The columns denote, from left to
right: disabling the \infname{PI} rule, \emph{Pragmatic} mode, \emph{Full} mode,
and \emph{Imit}$_\star$ modes with appropriate logical symbols. The rows denote
different values of $k$. The results show that different values for $k$ have a
modest effect on the success rate. The raw data reveal that when we focus our
attention to configurations with $k=2$, mode \emph{Full} can solve ten problems
no other mode (including disabling the \infname{PI} rule) can. Modes
\emph{Imit}$_{\iand}$ and \emph{Pragmatic} solve two  problems whereas
\emph{Imit}$_{\ior}$ solves one problem uniquely. This result suggests that, even
though this is not evident from Figure \ref{fig:pi}, the sets of problems solved 
by different modes somewhat differ.

Figure \ref{fig:elim-leibniz} gives results of evaluating rules that treat
Leibniz equality on the calculus level: EL stands for \infname{ElimLeibniz},
whereas EPV denotes \infname{ElimPredVar}; signs $-$ and $+$ denote that the corresponding rule is
removed from or added to configuration b, respectively. Disabling both rules
severely lowers the success rate. The results suggest that including
\infname{ElimLeibniz} is beneficial to performance.

\begin{figure}[t]
  \begin{center}
    \def\arraystretch{1.1}%
    \relax{\begin{tabular}{@{}l@{\hskip 1.5em}c@{\hskip 1.5em}c@{\hskip 1.5em}c@{\hskip 1.5em}c@{\hskip 1.5em}c@{\hskip 1.5em}c@{\hskip 1.5em}c@{\hskip 1.5em}c@{\hskip 1.5em}c}
      \strut                    & $-$\infname{PI}                 & b$_p$       & b$_f$       & b$_{\iand}$ & b$_{\ior}$  & b$_{\ieq}$  & b$_{\inot}$ & b$_{\iforall\iexists}$ 
      \MIDLINE
      $k=1$                     &  \multirow{3}{*}{1636}      &  \textbf{1648}   & 1628    & 1637  & 1634  & 1630 & 1641 & 1637 \\
      $k=2$                     &                                 &  \emph{1646}   &  1629   & 1636  & 1631  & 1627 & 1638 & 1634 \\
      $k=8$                     &                                 &  1643   &  1625   & 1633  & 1631  & 1623 & 1637 & 1635
    \end{tabular}}
    \caption{Effect of the \infname{PI} rule on success rate}
    \label{fig:pi}
  \end{center}
\end{figure}
\begin{figure}[t]
  \centering
  \begin{minipage}{.48\textwidth}
    \begin{center}
      \def\arraystretch{1.1}%
      \relax{\begin{tabular}{@{}l@{\hskip 2em}c@{\hskip 2em}c@{\hskip 2em}c}
        \strut        & $-$EL       & $+$EL  
        \MIDLINE
        $-$EPV        &  1584 (0)   &  1644 (0)   \\
        $+$EPV        &  1612 (0)   &  \emph{\textbf{1646 (0)}} \\
      \end{tabular}}
      \caption{Effect of Leibniz equality elimination rules }
      \label{fig:elim-leibniz}
    \end{center}
  \end{minipage}
  \hfill
  \begin{minipage}{.45\textwidth}
    \begin{center}
      \def\arraystretch{1.1}%
      \relax{\begin{tabular}{@{}l@{\hskip 2em}c@{\hskip 2em}c@{\hskip 2em}c}
        \strut        & $-$BEF       & $+$BEF  
        \MIDLINE
        $-$BER        &  1644 (2)   &  1643 (0)   \\
        $+$BER        &  1645 (0)   &  \emph{\textbf{1646 (0)}}   \\
      \end{tabular}}
      \caption{Effect of \infname{BoolER} and \infname{BoolEF} rules}
      \label{fig:bool-rules}
    \end{center}
  \end{minipage}
\end{figure}

Similarly, Figure \ref{fig:bool-rules} shows the merits of excluding ($-$) or including ($+$)
\infname{BoolER} (BER) and \infname{BoolEF} (BEF) rules. Our
expectations were that inclusion of those two rules would have a significant impact on
the success rate. It turns out that, in practice, most of the effects of these
rules could be achieved using a combination of the \infname{PI} rule and the
superposition rules.
\pagebreak[2]

\looseness=-1
Combining these two rules with dynamic clausification is more useful: When rule
\infname{IC} is replaced by rule \infname{DC}, the success rate increases
to 1660 problems, compared to 1646 problems solved by b. We also discovered
that reasoning with choice is useful: When rule \infname{Choice} is enabled, the
success rate increases to 1653. We determined that including or excluding the
conclusion $D$ of \infname{Choice}, after it is simplified, makes no difference.
Counterintuitively, disabling the \infname{BoolSimp} rule results in 1640 problems,
which is only 6 problems short of configuration b. Disabling the \infname{Abs} and
\infname{Interpret$\lambda$} rules results in solving 25 and 31 problems less,
respectively. The raw data show that in total, using configurations from Figure
\ref{fig:cases} to Figure \ref{fig:bool-rules}, 1682 problems can be solved.

Last, we compared our approach to alternatives. Axiomatizing Booleans brings
Zipperposition down to a grinding halt: only 1106 problems can be solved using
this mode. On the other hand, preprocessing is fairly competitive: it solved
only 8 problems less than the b configuration.


\ourpara{Comparison with Other Higher-Order Provers} 

\looseness=-1
We compared Zipperposition with all higher-order theorem provers that took part
in higher-order division of the 2019 edition of CASC \cite{gs-19-casc27}: CVC4 1.8 prerelease \cite{cbetal-11-cvc4},
Leo-III 1.4 \cite{sb-21-leo3}, Satallax 3.4
\cite{cb-12-satallax}, and Vampire-THF 4.4
\cite{lkav-13-vampire}. In this part of the evaluation, Zipperposition
used the portfolio mode that runs configurations in different time slices. 
We set the CPU time limit to 180 s, the time allotted to each prover at the 2019 edition of CASC.


Leo-III and Satallax are cooperative theorem provers---they periodically invoke
first-order provers to finish the proof attempt. Leo-III uses CVC4, E, and
iProver \cite{kk-08-iprover} as backends, and Satallax uses Ehoh
% \cite{EHOH CHAPTER}
 as backend. Zipperposition can use Ehoh as backend as well. To test how successful each calculus is, we ran
the cooperative provers in two versions: \emph{uncoop}, which disables backends,
and \emph{coop}, which uses all supported backends.



\begin{figure}[t]
  \label{fig:higher-order-provers}
  \begin{center}
    \def\arraystretch{1.1}%
    \relax{\begin{tabular}{@{}l@{\hskip 2em}c@{\hskip 2em}c@{\hskip 2em}c@{\hskip 2em}c@{\hskip 2em}c}
      & CVC4             & Leo-III & Satallax              & Vampire & Zipperposition
      \MIDLINE
      uncoop    & 1806 (5) &  1627 (0) & 2067 (0)            & 1924 (7) & 1980 (\phantom{0}0)                \\
      coop    & --       &  2085 (3) & \textbf{2214 (9)}   & --       & 2190 (17)
    \end{tabular}}
    \caption{Comparison with other higher-order provers}
  \end{center}
\end{figure}

\looseness=-1
In both uncooperative and cooperative mode, Satallax is the winner. Zipperposition comes
in close second, showing that our approach is a promising basis for further
extensions. Indeed, as will be shown in Chapter \ref{ch:ho-techniques}, with
fine-tuning of the heuristics and further extensions to the calculus, this
approach can outperform all other competitive provers with a large margin.

\section{Discussion}
\label{sect:bool:discussion}

Our work is primarily motivated by the goal of closing the gap between
higher-order ``hammer'' or software verifier frontends and first-order backends.
A considerable amount of research effort has gone into making the translations of
higher-order logic as efficient as possible. Descriptions of hammers like
HOLyHammer \cite{ku-15-holyhammer} and Sledgehammer
\cite{pb-12-sh} for Isabelle contain details of these
translations. Software verifiers Boogie \cite{lr-10-boogie} and Why3
\cite{bfcp-11-why3} use similar translations.

\looseness=-1
Established higher-order provers like Leo-III and Satallax
have been optimized to perform well on TPTP; however, recent evaluations (including the one in Sect.~\ref{sec:ehoh:evaluation}) show that on Sledgehammer
problems they are outperformed by translations to first-order logic
\cite{bbtvw-21-sup-lam, %EHOH CHAPTER,
cbetal-11-cvc4}. Those two provers are built from the ground up as
higher-order provers---treatment of exclusively higher-order issues such as
extensionality or choice is built into them often using explosive rules. Those
explosive rules might contribute to their suboptimal performance on mostly
first-order Sledgehammer problems.

In contrast, the approach taken in this thesis is to start with a first-order prover and gradually extend it
with higher-order features. The work performed in the context of the Matryoshka project~\cite{matryoshka}, 
in which I participated, resulted in adding support for
$\lambda$-free higher-order logic with Booleans to E (Chapter~\ref{ch:ehoh})
and veriT \cite{cbetal-11-cvc4}, and adding support for Boolean-free higher-order
logic to Zipperposition. Many authors of state-of-the-art
first-order provers have implemented some form of support for higher-order reasoning. This
is true both for SMT solvers, witnessed by the recent extension of CVC4 and veriT~\cite{cbetal-11-cvc4},
 and for superposition provers, witnessed
by the extension of Vampire \cite{br-19-restricted-unif}. All of those approaches were
arguably more focused on functional aspects of higher-order logic, such as
$\lambda$-binders and function extensionality, than on Boolean aspects such as
Boolean subterms and Boolean extensionality. A notable exception is work by
Kotelnikov et al.\ that introduced support for Boolean subterms to first-order
Vampire \cite{kotelnikov-15-fool,kotelnikov-16-fool}.

The main merit of our approach is that it combines two successful complementary
approaches, \lsup{} and FOOL paramodulation, to support features of higher-order logic that have not been combined
before in a modular way. It is based on \lsup{}, a calculus that generalizes the highly successful first-order superposition calculus.
It incurs around 1\% of overhead on first-order problems compared with classic
superposition \cite{bbtvw-21-sup-lam}.

\section{Conclusion} 
\label{sect:bool:conclusion}

We presented a pragmatic approach to support Booleans in a modern automatic prover
for clausal higher-order logic. Our approach combines
previous research efforts that extended first-order provers with complementary
features of higher-order logic. It also proposes some solutions for issues that
emerge with this combination. The implementation shows a clear improvement over previous
techniques and a competitive performance.

What our work misses is an overview of heuristics that can be used to curb the
explosion incurred by some of the rules described in this chapter. In the next chapter,
we explore exactly this topic.



