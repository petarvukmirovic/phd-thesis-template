\chapter{Introduction}
\label{introduction}

Half sleeping, half playing Fruit Ninja on my smartphone at one of the
philosophy lectures in my high school I heard my professor exclaim with
excitement: “Mathematical truth is the highest form of truth!”.
Unbothered and uninterested, I continued playing Fruit Ninja.

Many years later I started studying various formal methods in computer science.
There I learned the value of formal, mathematical language and finally
understood him. Not only does using a rigorous mathematical language avoid the
ambiguity of natural language, it allows us to use the formal rules of reasoning
to make conclusions from initial arguments in a trustworthy manner. 

Though the interest for rigorous and formal reasoning dates back to Aristotle,
it intensified at the end of 19th and the beginning of 20th century
\cite{jf-01-modern-logic}. Researchers in that time were mostly interested in
finding a formal language that is expressive enough to describe complex
mathematical theories, yet intuitive and understandable enough to allow
reasonably simple formal reasoning systems. First-order logic seemed to satisfy
both requirements.  This formalism does not only allow one to model simple
logical relations such as ``if it rains, the road is muddy'' (or formally
$\cst{rains} \imp \cst{muddy}$), but also to quantify over objects as in ``all
people are mortal'' ($\forall x.\, \cst{human}(x) \imp \cst{mortal}(x)$).

\paragraph{Automatic Theorem Proving}The initial study of first-order logic and the rules to reason about statements in
it was very prolific. In 1930 G\"odel \cite{kg-30-completeness-theorem} showed
that there is a system of inference rules (calculus) such that for any valid
statement a proof of validity can be constructed in this system. Around the same
time, the first computers were leaving the imagination of the engineers and
began automating many complicated processes.  Naturally, the question as to
whether a computer can decide if a first-order statement is valid (i.e., a
theorem) arose. Church \cite{ac-36-fol-undecidable} and Turing
\cite{tm-37-undecidable} answered this question negatively in 1936.

Despite this negative result, the prospect of automatically proving theorems of
first-order logic remained too enticing. In 1960, Davis and Putnam
\cite{dp-1960-dpll} described an algorithm for checking validity of a
first-order formula. As first-order logic is undecidable, this algorithm
terminated only on valid formulas. Even though it was efficient enough to prove
only simple formulas, it was an impressive achievement for the time.

A bigger breakthrough happened in 1965 when Robinson introduced a calculus that
would shape the future of automatic reasoning --- the resolution calculus
\cite{ar-65-resolution}. Consisting of a single inference rule it was simple and
elegant. It was also \emph{complete}: each theorem could be proven using this
system. Furthermore, it was less explosive than Davis and Putnam's algorithm
since it introduced unification as an inference filter.

Since its introduction, resolution was improved in many ways. Strategies and
heuristics~\cite{lw-65-sos} to curb the explosion of the search space were
introduced, as well as complete, but less explosive variants of the calculus
\cite{cc-73-resolution-book}. Despite this progress, reasoning about equality of
objects was still hard for these methods. This changed with introduction of
superposition~\cite{bg-94-superposition} in the early 1990s.

Superposition is a complete calculus for first-order logic loosely based on
resolution, featuring support for equality on the calculus level. This support
is further optimized by use of term orders and selection functions to prune the
search space. Thanks to efficient implementation, this calculus was used to
prove long-standing open problem known as ``Robbins conjecture''
\cite{mccune-97-robbins}. Provers based on this calculus have been
winning the first-order division of CASC theorem proving competition since its
inception in the 1990s \cite{ss-96-casc}.

Even though the successes of proving Robbins conjecture showed that it is
possible, automatically proving hard mathematical problems is still out of reach
for theorem provers. However, there are other ways in which humans and computers can work together to construct formal
proofs.

\paragraph{Proof Assistants and Hammers} Proof assistants (or interactive
theorem provers) are programs which allow users to describe a theory formally
and to write proofs of the statements about this theory using inference 
rules of the proof assistant. These programs can then check if the given proof
is correct. As their core is usually small and well-tested, proofs that pass the
checking phase are considered trustworthy.

Proof assistants showed indispensible in proving very complex mathematical theorems in a
trustworthy manner. For example, four-color theorem \cite{gg-2008-four-color}
and Kepler conjecture \cite{th-2015-kepler} were proven inside proof assistants,
ending the need to trust hundreds of pages of hard mathematical proofs. Proof
assistants are also used to show correctness of complex software such as
compilers \cite{xl-09-compcert} or operating systems kernels \cite{gk-09-sel4}.
This verified software can be used in safety-critical scenarios such as nuclear
plants or autonomous vehicles.

The proofs need to be spelled out in minute detail to be accepted by a proof assistant.
Even though many proof assistants offer some sort of automation, seemingly simple
statements cannot be automatically proved. This is one of the reasons why large
verification projects can take years to finish.

Integrating proof automation right into the proof assistant is not an easy task.
Most proof assistants are based on variants of higher-order logic, which is more
expressive than first-order logic, but in general does not even allow complete
proof calculi. To provide some level of automation, proof assistant developers
use the efficient first-order provers as follows. First, the proof goal is
translated from higher-order to first-order logic. Then the first-order prover
is run on the translated problem. If the first-order prover finds the proof, the
proof is reconstructed in the proof assistant. This approach is implemented in
modern assistants such as Coq \cite{bc-04-coq}, HOL Light \cite{jh-09-hol-light}
and Isabelle \cite{lc-88-isabelle} using \emph{hammers} CoqHammer
\cite{ck-18-coqhammer}, HOL(y)Hammer \cite{ku-15-holyhammer} and Sledgehammer
\cite{bn-10-sh}, respectively. 

This approach proved useful: around 76\% of the simple proof goals can be
discharged automatically using Sledgehammer \cite{bgkku-16-larning-fact-selector}. However, there is
clearly some untapped potential in the approach as it communicates with
automatic theorem provers through a translation. To bridge this translation gap,
attempts were made to use higher-order automatic theorem provers instead of
first-order ones \cite{ns-13-leo2sh}. This proved unfruitful as state-of-the-art
higher-order theorem provers of the time were primarily designed for small and
tricky problems. On the other hand, problems coming from hammers consist of
mostly first-order formulas, are rather large, and mostly require simple
higher-order proof steps. Thus, from a standpoint of proof assistant ideal
automatic theorem prover fulfills the following wishlist:
\begin{enumerate}
  \item\label{it:fo} Performs as best first-order provers on first-order problems
  \item\label{it:ho} Supports higher-order logic and scales with the amount of higher-order axioms
  \item\label{it:size} Scales well with the size of the problem
\end{enumerate}
This brings us to the topic of this thesis: \emph{How can we develop a theorem provers that fulfills this wishlist?}

\paragraph{Our Approach} To improve the automatic reasoning for higher-order
logic we decided not to start from a blank slate and build a new theorem prover.
Rather, we start from a position of strength --- from state-of-the-art
first-order superposition theorem prover E \cite{scv-19-e23} --- and extend this
prover to higher-order logic. In this way, the points \ref{it:fo} and
\ref{it:size} from the above wishlist are fulfilled for free, as E is very
efficient and deals well with large problems.

We do not extend E to higher-order logic in one atomic step. Instead, we extend
it by adding support for three distinguished features of higher-order logic,
which gradually increase its support for higher-order logic. With the layered
approach we make sure that after every extension to higher-order logic, E's
efficiency on first-order problems did not decrease. In other words, while trying
to fulfill the point \ref{it:ho} from the wishlist we make sure that points
\ref{it:fo} and \ref{it:size} are not affected.

We have split the road to full higher-order logic in three stages. A
distinguished feature of higher-order logic is that it not only supports
quantification over objects, but also over functions that manipulate these
objects. Thus, higher-order logic allows us to make a statement ``all functions
with arguments $0$ and $1$ result in $2$" ($\forall x. \,  x (\cst{0}, \cst{1}) \eq
\cst{2}$). Replacing $x$ with addition function ($\cst{+}$) is enough to
disprove this statement. The first stage is to extend E to support
quantification over functions that are already present in the theory.

Higher-order logic also supports expressing functions that are not explicitly
present in the theory. For example, suppose we want to prove that there is a
function such that returns 4 when given 2 and 5 when given 3 ($\exists x.
x(\cst{2}) \eq \cst{4} \lland x (\cst{3}) \eq \cst{5} $). An example of such
function is the function that adds 2 to its argument. In higher-order logic such
unnamed function can be represented using $\lambda$ abstractions as $\lambda
x.\, x \, \cst{+} \, 2$. Replacing $x$ with this term suffices to prove the
original statement. The second stage is to support automatically synthesizing
such functions.

In higher-order logic we can make statements about statements. More precisely,
we it treats Boolean terms (formulas) as first-class citizens, enabling us to
model some concepts in more natural manner. For example, we can naturally
represent statements such as ``what Jasmin says is true'' as $\forall x. \,
\cst{says}(\cst{jasmin}, x) \imp x$. The last stage is to support Boolean terms
as first-class citizens.

\section{Contributions}

The biggest contribution of this thesis is the extension of two theorem provers,
E and Zipperposition \cite{sc-15-simon-phd}, to full higher-order logic. To
extend those provers, we faced many challenges. Some of them were of engineering
nature, some of them were algorithmic, while others concerned fine-tuning the
heuristics to achieve best possible performance. We discuss those challenges in more detail:
\begin{itemize}
  \item We implemented the complete superposition calculus for $\lambda$-free
  higher-order logic \cite{bbcw-21-lfho} in E, obtaining a prover we call Ehoh.
  We had to modify internal term structure of Ehoh to support $\lambda$-free
  higher-order terms, implement new unification and matching algorithms, extend
  indexing data structures to work with higher-order terms, and to improve the
  performance of heuristics on higher-order problems.
  \item Together with Alexander Bentkamp and other colleagues we developed
  a complete superposition calculus for the second stage described above
  \cite{bbtvw-21-sup-lam}. Due to the complexity of the calculus and many design
  decisions that had to be evaluated in short amount of time we decided to
  implement the calculus in Zipperposition. This prover is less efficient than
  E, but it is implemented in OCaml and it was designed to make prototyping
  ideas and experimenting with calculi much easier. In 2019 we entered the
  higher-order division of CASC theorem proving competition with Zipperposition
  \cite{gs-19-casc27}. It took third place, closely behind Leo-III (1 percent
  point) \cite{sb-21-leo3} and far behind (12 percent points) Satallax
  \cite{cb-12-satallax}.
  \item Through this experience we identified one of the bottlenecks of
  Zipperposition: its unification algorithm. To remove this bottleneck we
  developed a complete higher-order unification algorithm that was more
  efficient and less redundant than current state of the art. 
  \item We also noticed that Zipperposition's support for first-class Booleans
  was lacking compared to competition. We have closely studied the problems on
  which Zipperposition fails, but are easily solved by other provers and created
  a set of incomplete rules that enhance Zipperposition's Boolean support.
  Zipperposition 2 implemented these rules together with the new unification
  algorithm. In 2020, Zipperposition 2 was the winner of higher-order division
  of CASC \cite{gs-21-cascj10}. This time it was 20 percent points better than the second best
  prover, Satallax.
  \item Inspired by this success we implemented the complete superposition
  calculus for full higher-order logic (i.e., the third stage) that we developed with Bentkamp and other
  colleagues. The development of complete calculus helped us understand in which
  ways our pragmatic extension was too weak to prove hard theorems of
  higher-order logic. We further tuned the heuristics and implemented new ways
  to tame the search space explosion. In 2021 we released Zipperposition 2.1
  which implemented the new calculus and the described tweaks. It again won
  CASC, outperforming Zipperposition 2. It was 16 percent points ahead of the
  runner-up, Vampire 4.5 \cite{lkav-13-vampire}. 

  \item Taking all the experience we gathered while working on Zipperposition,
  we further extended Ehoh to full higher-order logic. We again modified term
  structure, implemented new unification and matching algorithms, extended indexing data
  structures, and tweaked heuristics. Even though this extension has larger scope than 
  the original extension from E to Ehoh, strong basis that we have earlier built
  made it manageable.

  \item Following our approach of lifting efficient provers from
  first-order to higher-order logic, we went a step back in an effort to improve
  first-order provers. In particular, we looked at the most efficient approaches
  to simplify problems in propositional logic, a logic that disallows quantification,
  but it is decidable. We lifted some of these approaches to first-order logic
  and implemented them in Zipperposition. 

\end{itemize}

\section{Publications and Thesis Structure}
\section{Related Work}

% \begin{abstract}
% Sample Abstract.
% \end{abstract}

% \blfootnote{This chapter is partly based on \faFileTextO~\emph{M. Beller. Toward an
%     Empirical Theory of Feedback-Driven Development, ICSE'18 (Student Research Competition)}~\cite{BellerSRC2018}.
% }


% \newpage

% \dropcap{T}his is a introductory page.

% \section{Background \& Context}
% In this thesis, you can reference pictures~\Cref{fig:devmodel} using Cleverref and circles \circled{5}.

% \begin{figure}[htb]
% 	\centering
% 	\includegraphics[width=0.65\columnwidth]{development_model_without_papers}
% 	\caption{The stages of the FDD model and their relationship to other
%           Software Engineering concepts.}
% 	\label{fig:devmodel}
% \end{figure}

% We also have lists:

% \begin{enumerate}
%   \item Static Analysis~\circled{3} examines program artifacts or
%     their source code without executing them~\cite{wichmann1995industrial}, while
%  \item Dynamic Analysis~\circled{4} relies on information gathered from their
%    execution~\cite{cornelissen2009systematic}.
% \end{enumerate}

% Or boxes:

% \begin{framed}
% This thesis is concerned with the empirical assessment of the state of the art of how developers
% drive software development with the help of feedback loops.
% \end{framed}

% Or code:
% \begin{lstlisting}[caption={\textsc{TrinityCore}},label={lst:e1}]
%  x += other.x;
%  y += other.y;
%  z += other.y;
% \end{lstlisting}


% I hope this helps you get started!
% Moritz
