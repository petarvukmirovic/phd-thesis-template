\chapter{Introduction}
\label{introduction}

Half sleeping, half playing Fruit Ninja on my smartphone at one of the
philosophy lectures in my high school I heard my professor exclaim with
excitement: “Mathematical truth is the highest and absolute form of truth!”.
Unbothered and uninterested, I continued playing Fruit Ninja.

Many years later I started studying various formal methods in computer science.
There I learned the value of formal, mathematical language and finally
understood him. Not only does using a rigorous mathematical language avoid the
ambiguity of natural language, it allows us to use the formal rules of reasoning
to make conclusions from initial arguments in a trustworthy manner. 

Though the interest for rigorous and formal reasoning dates back to Aristotle,
it intensified at the end of 19th and the beginning of 20th century
\cite{jf-01-modern-logic}. Researchers in that time were mostly interested in
finding a formal language that is expressive enough to describe complex
mathematical theories, yet intuitive and understandable enough to allow
reasonably simple formal reasoning systems. First-order logic seemed to satisfy
both requirements.  This formalism does not only allow one to model simple
logical relations such as ``if it rains, the road is muddy'' (or formally
$\cst{rains} \imp \cst{muddy}$), but also to quantify over objects as in ``all
people are mortal'' ($\forall x.\, \cst{human}(x) \imp \cst{mortal}(x)$).

\paragraph{Automatic Theorem Proving}The initial study of this formalism and the rules to reason about statements in
it was very prolific. In 1930 G\"odel \cite{kg-30-completeness-theorem} showed
that there is a system of inference rules (calculus) such that for any valid
statement a proof of validity can be constructed in this system. Around the same
time, the first computers were leaving the imagination of the engineers and
began to automate many complicated processes.  Naturally, the question as to
whether a computer can decide if a first-order statement is valid (i.e., a
theorem) arose. Church \cite{ac-36-fol-undecidable} and Turing
\cite{tm-37-undecidable} answered this question negatively in 1936.

Despite this negative result, the prospect of automatically proving theorems of
first-order logic remained too enticing. In 1960, Davis and Putnam
\cite{dp-1960-dpll} described an algorithm for checking validity of a
first-order formula. As first-order logic is undecidable, this algorithm
terminated only on valid formulas. Even though it was efficient enough to prove
only simple formulas, it was an impressive achievement for the time.

A bigger breakthrough happened in 1965 when Robinson introduced a calculus that
would shape the future of automatic reasoning --- the resolution calculus
\cite{ar-65-resolution}. Consisting of a single inference rule it was simple and
elegant. It was also \emph{complete}: each theorem could be proven using this
system. Furthermore, it was less explosive than Davis and Putnam's algorithm
since it introduced unification as an inference filter.

Since its introduction, resolution was improved in many ways. Strategies and
heuristics~\cite{lw-65-sos} to curb the explosion of the search space were
introduced, as well as complete, but less explosive variants of the calculus
\cite{cc-73-resolution-book}. Despite this progress, reasoning about equality of
objects was still hard for these methods. This changed with introduction of
superposition~\cite{bg-94-superposition} in the early 1990s.

Superposition is a complete calculus for first-order logic loosely based on
resolution, featuring support for equality on the calculus level. This support
is further optimized by use of term orders and selection functions to prune the
search space. Thanks to efficient implementation, this calculus was used to
prove long-standing open problem known as ``Robbins conjecture''
\cite{mccune-97-robbins}. Provers based on this calculus have been
winning the first-order division of CASC theorem proving competition since its
inception in the 1990s \cite{ss-96-casc}.

Even though the successes of proving Robbins conjecture showed that it is
possible, automatically proving hard mathematical problems is still out of reach
for theorem provers. However, computers can still be of help to construct formal
proofs.

\paragraph{Proof Assistants and Hammers} Proof assistants (or interactive theorem provers) are programs which allow users
to describe a theory formally and to write proofs of the statements about this theory
using inference system rules of the proof assistant. These
programs can then check if the given proof is correct. As their core is usually
small and well-tested, proofs that pass the checking phase are considered
trustworthy.

Proof assistants showed indispensible in proving very complex mathematical theorems in a
trustworthy manner. For example, four-color theorem \cite{gg-2008-four-color}
and Kepler conjecture \cite{th-2015-kepler} were proven inside proof assistants,
ending the need to trust hundreds of pages of hard mathematical proofs. Proof
assistants are also used to show correctness of complex software such as
compilers \cite{xl-09-compcert} or operating systems kernels \cite{gk-09-sel4}.
This verified software can be used in safety-critical scenarios such as nuclear
plants or autonomous vehicles.

The proofs need to be spelled out in minute details to be accepted by a proof assistant.
Even though many proof assistants offer some sort of automation, seemingly simple
statements cannot be automatically proved. This is one of the reasons why large
verification projects can take years to finish.

Integrating proof automation right into the proof assistant is not an easy task.
Most proof assistants are based on variants of higher-order logic, which is more
expressive than first-order logic, but in general does not even allow complete
proof calculi. To provide some level of automation, proof assistant developers
use the efficient first-order provers as follows. First, the proof goal is
translated from higher-order to first-order logic. Then the first-order prover
is run on the translated problem. If the first-order prover finds the proof, the
proof is reconstructed in the proof assistant. This approach is implemented in
modern assistants such as Coq \cite{bc-04-coq}, HOL Light \cite{jh-09-hol-light}
and Isabelle \cite{lc-88-isabelle} using \emph{hammers} CoqHammer
\cite{ck-18-coqhammer}, HOL(y)Hammer \cite{ku-15-holyhammer} and Sledgehammer
\cite{bn-10-sh}, respectively. 

This approach proved useful: around 69\% of the simple proof goals can be
discharged automatically using Sledgehammer \cite{kb-13-mash}. However, there is
clearly some untapped potential in the approach as it communicates with
automatic theorem provers through a translation. To bridge this translation gap,
attempts were made to use higher-order automatic theorem provers instead of
first-order ones \cite{ns-13-leo2sh}. This proved unfruitful as state-of-the-art
higher-order of the time theorem provers were primarily designed for small and tricky
problems. On the other hand, problems coming from hammers consist of mostly first-order formulas, are rather large,
and mostly require simple higher-order proof steps. 

Thus, from a standpoint of proof assistant ideal automatic theorem prover fulfills
the following wishlist:
\begin{itemize}
  \item Performs as best first-order provers
  \item Supports higher-order logic
  \item Works well on large problems
\end{itemize}
This brings us to the topic of this thesis: \emph{How can we develop a theorem provers that fulfills this wishlist?}






% \begin{abstract}
% Sample Abstract.
% \end{abstract}

% \blfootnote{This chapter is partly based on \faFileTextO~\emph{M. Beller. Toward an
%     Empirical Theory of Feedback-Driven Development, ICSE'18 (Student Research Competition)}~\cite{BellerSRC2018}.
% }


% \newpage

% \dropcap{T}his is a introductory page.

% \section{Background \& Context}
% In this thesis, you can reference pictures~\Cref{fig:devmodel} using Cleverref and circles \circled{5}.

% \begin{figure}[htb]
% 	\centering
% 	\includegraphics[width=0.65\columnwidth]{development_model_without_papers}
% 	\caption{The stages of the FDD model and their relationship to other
%           Software Engineering concepts.}
% 	\label{fig:devmodel}
% \end{figure}

% We also have lists:

% \begin{enumerate}
%   \item Static Analysis~\circled{3} examines program artifacts or
%     their source code without executing them~\cite{wichmann1995industrial}, while
%  \item Dynamic Analysis~\circled{4} relies on information gathered from their
%    execution~\cite{cornelissen2009systematic}.
% \end{enumerate}

% Or boxes:

% \begin{framed}
% This thesis is concerned with the empirical assessment of the state of the art of how developers
% drive software development with the help of feedback loops.
% \end{framed}

% Or code:
% \begin{lstlisting}[caption={\textsc{TrinityCore}},label={lst:e1}]
%  x += other.x;
%  y += other.y;
%  z += other.y;
% \end{lstlisting}


% I hope this helps you get started!
% Moritz
