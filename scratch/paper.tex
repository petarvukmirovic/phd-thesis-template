% For VSCode
% !TEX root = ./conf.tex

%% TACAS: 16 pp excluding bibliography

\pagestyle{plain}
\PassOptionsToPackage{hyphens}{url}

\usepackage[T1]{fontenc}
\usepackage[misc]{ifsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{cite}
\usepackage{color}
\usepackage{enumerate}
\usepackage{enumitem}
%\usepackage{stmaryrd}
%\usepackage{textcomp}
\usepackage{url}
%\usepackage{xparse}
%\usepackage{xpatch}
%\usepackage{mathtools}
\usepackage{microtype}
\usepackage{array}
%\usepackage{nicefrac}
%\usepackage{subcaption}
%\usepackage{tabu}
\usepackage{prftree}
%\usepackage{float}
%\usepackage{bm}
%\usepackage{graphicx}
%\usepackage{wrapfig}
\usepackage{multirow}
%\usepackage{soul}
\usepackage{fancyvrb}
\usepackage{siunitx}
%\let\vec=\relax
%\usepackage{fdsymbol}
%\let\longleadsto=\leadsto
\usepackage{pifont}
\usepackage{hyphenat}
\usepackage{algorithmicx}
\usepackage{regexpatch}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage[noend]{algpseudocode}% http://ctan.org/pkg/algorithmicx
\xpatchcmd{\algorithmic}{\labelsep 0.5em}{\labelsep 3em}{\typeout{Success!}}{\typeout{Oh dear!}}

\hyphenation{ab-strac-tion}

%% for algorithms
%\usepackage[plain]{algorithm}
%\usepackage[noend]{algpseudocode}

\usepackage[
   a4paper,
   pdftex,
%   pdftitle={Superposition for Lambda-Free Higher-Order Logic},
%   pdfauthor={Alexander Bentkamp, Jasmin Blanchette, Simon Cruanes, and Uwe Waldmann},
   pdfkeywords={},
   pdfborder={0 0 0},
   draft=false,
   bookmarksnumbered,
   bookmarks,
   bookmarksdepth=2,
   bookmarksopenlevel=2,
   bookmarksopen]{hyperref}

\usepackage{tikz}
\usepackage{mdframed}
%\usepackage{tikz-qtree}
%\usetikzlibrary{arrows,shapes,positioning,shadows,trees,calc}

%%% BEGIN Times font
%\usepackage{mathptmx}
%\usepackage[scaled=.82]{beramono}
%\usepackage[scaled=.865]{helvet}
%%\usepackage{avant}
%\DeclareSymbolFont{letters}{OML}{txmi}{m}{it}
%%% END Times font

\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds\do\=\do\_}
\makeatother

\hyphenation{counter-example counter-examples Isa-belle Ma-try-osh-ka
  frame-work frame-works non-empty sko-lem-iza-tion super-posi-tion
  over-approx-imate over-approx-imated auto-schedule}

% mathcal
\DeclareFontFamily{OT1}{pzc}{}
\DeclareFontShape{OT1}{pzc}{m}{it}{<-> s * [1.10] pzcmi7t}{}
\DeclareMathAlphabet{\mathcalx}{OT1}{pzc}{m}{it}
%\DeclareMathAlphabet{\mathcalx}{OMS}{zplm}{m}{n}

\let\labelitemi=\labelitemii %% CHEAT!

%%% BEGIN poor (wo)man's algorithms
% \newcommand\Function[2]{\textbf{function} \textsc{#1}(#2) \textbf{is} \\}
% \newcommand\Procedure[2]{\textbf{procedure} \textsc{#1}(#2) \textbf{is} \\}
% \newcommand\If[1]{\textbf{if} #1 \textbf{then} \\}
% \newcommand\IfWoThen[1]{\textbf{if} #1 \\}
% \newcommand\Then[1]{\phantom{\textbf{if}} \quad #1 \textbf{then} \\}
% \newcommand\Else{\textbf{else} \\}
% \newcommand\ElsIf[1]{\textbf{else} \textbf{if} #1 \textbf{then} \\}
% \newcommand\ForTo[2]{\textbf{for} #1 \textbf{to} #2 \textbf{do} \\}
% \newcommand\Do{\textbf{do} \\}
% \newcommand\Forever{\textbf{forever} \textbf{do} \\}
% \newcommand\WhileOfDo[1]{\textbf{while} #1}
% \newcommand\ForDownto[2]{\textbf{for} #1 \textbf{downto} #2 \textbf{do} \\}
% \newcommand\While[1]{\textbf{while} #1 \textbf{do} \\}
% \newcommand\Return{\textbf{return} }

% \newcommand\q{\noindent\hbox{}\quad}
% \newcommand\qq{\q\q}
% \newcommand\qqq{\qq\q}
% \newcommand\qqqq{\qqq\q}
% \newcommand\qqqqq{\qqqq\q}
% \newcommand\qqqqqq{\qqqqq\q}
% \newcommand\qqqqqqq{\qqqqqq\q}
% \newcommand\qqqqqqqq{\qqqqqqq\q}
% \newcommand\qqqqqqqqq{\qqqqqqqq\q}

% \newlength\quotexwidth
% \quotexwidth=\textwidth
% \addtolength\quotexwidth{-\leftmargin}

\newcommand\NumberOK[1]{#1}
\newcommand\NumberNOK[1]{\colorbox{red}{#1}}

\newcommand\ourpara[1]{\paragraph{\upshape\bfseries#1.}}

\newenvironment{quotex}
 {\list{}{\rightmargin0pt}%
  \item\relax}
 {\endlist}
%%% END poor (wo)man's algorithms

\newcommand{\cmark}{\text{\ding{51}}}
\newcommand{\xmark}{\text{\ding{55}}}

\newcommand\CRS{\confrep{\texttt{ConjectureRelativeSymbol}}{\texttt{CRS}}}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newcommand\HLINE{\\[.25\jot]\hline\noalign{\vskip.5\jot}}
\newcommand\infname[1]{\textsc{#1}}
\newcommand{\infer}[3]{\prftree[r]{{\small\infname{#1}}}{\strut#2}{\strut#3}}
\newcommand{\inferdouble}[3]{\prftree[d,r]{{\small\infname{#1}}}{\strut#2}{\strut#3}}

\newcommand{\hooklongrightarrow}{\lhook\joinrel\longrightarrow}

\newcommand\Unifarrow{\Longrightarrow}
\newcommand\Matcharrow{\Longrightarrow}
\newcommand\Pdtarrow{\leadsto}
\newcommand\Matchiiarrow{\hooklongrightarrow}

% Most general unifier
\DeclareMathOperator{\mgu}{mgu}

\def\negvthinspace{\kern-0.083333em}
\def\vthinspace{\kern+0.083333em}
\def\vvthinspace{\kern+0.0416667em}
\def\negvvthinspace{\kern-0.0416667em}
%\def\hypsep{\hskip0.75em}
\def\hypsep{\quad}
\def\rulesep{\qquad}

\spnewtheorem{theoremx}[theorem]{Theorem}{\bfseries}{\slshape}
\spnewtheorem{corollaryx}[theorem]{Corollary}{\bfseries}{\slshape}
\spnewtheorem{lemmax}[theorem]{Lemma}{\bfseries}{\slshape}

\newcommand\ehohii{$\lambda$E}

\newcommand{\eq}{\approx}
\newcommand{\noteq}{\not\eq}

\newcommand\UNIF{\mathrel{\smash{\stackrel{\lower.1ex\hbox{\ensuremath{\scriptscriptstyle ?}}}{=}}}}
%\newcommand\UNIF{=^?}
\newcommand\MATCH{\mathrel{{\scriptstyle\lesssim}^{\scriptscriptstyle{?\!}}}}

%\newcommand\UNIF{=^{?\!}}
%\newcommand\MATCH{\lesssim^{?\!}}
%\newcommand\MATCH{\mathrel{\smash{\stackrel{\lower.1ex\hbox{\ensuremath{\scriptscriptstyle ?}}}{\lesssim}}}}

\newcommand\Var{\mathcal{V}\negvthinspace\mathit{ar}}
\newcommand\Term{\mathcal{T}\kern-.7ex\mathit{erm}}
\newcommand\Gfpf{\mathcal{G}\kern-.3ex\mathit{fpf}}
\newcommand\GfpfRTL{\Gfpf'}
\newcommand\Fp{\mathcal{F}\kern-.3ex\mathit{p}}
\newcommand\FpRTL{\Fp'}

%\renewcommand\AA{\textbf{\textsf{A}}}
%\newcommand\BB{\textbf{\textsf{B}}}
%\newcommand\NN{\textbf{\textsf{N}}}

\renewcommand\AA{{\textsf{A}}}
\newcommand\BB{{\textsf{B}}}
\newcommand\NN{{\textsf{N}}}

\definecolor{verylightgray}{gray}{0.9}
\definecolor{pdtgreen}{HTML}{97D077}
\definecolor{pdtorange}{HTML}{D79B00}

\DeclareRobustCommand{\hlorange}[1]{{\sethlcolor{orange}\hl{#1}}}
\DeclareRobustCommand{\hlgreen}[1]{{\sethlcolor{green}\hl{#1}}}
\DeclareRobustCommand{\hlcolor}[2]{{\sethlcolor{#1}\hl{#2}}}

% for Springer theorems
\spnewtheorem{examplex}{Example}{\bfseries}{\rmfamily}
\spnewtheorem{definitionx}[theorem]{Definition}{\bfseries}{\rmfamily}

% probably can be set up differently
%\theoremstyle{definition}
%\newtheorem{definitionx}{Definition}

%\theoremstyle{definition}
%\newtheorem{examplex}{Example}

%-- from my thesis
%\theoremstyle{definition}
%\newtheorem{remark}{Remark}

\newcommand\afterDot{\,} %%% the Springer style is a bit broken---this makes things a bit more tolerable
\def\cpp{C\nobreak\raisebox{.1ex}{+}\nobreak\raisebox{.1ex}{+}}
\newcommand\appE{\ensuremath{\cst{@}{+}\text{E}}}

\newcommand\cst[1]{\mathsf{#1}}

%%%%%%%%%%%%%%%%%%%%%%
% conf and rep
\newcommand{\confrep}[2]{\begin{conf}#1\end{conf}\begin{rep}#2\end{rep}}
\newcommand{\lfhol}[0]{$\lambda$fHOL}
\newcommand{\appvar}[0]{\ensuremath{@}}

\newcommand{\lam}[2]{\ensuremath{\lambda #1.\> #2}}
\newcommand{\lamx}[1]{\lam{x}{#1}}

% stolen from FOBOOLSUP paper
\newcommand{\ifalse}{\pmb\bot}
\newcommand{\itrue}{\pmb\top}
\newcommand{\inot}{\pmb{\neg\,}}
\newcommand{\iand}{\pmb\land}
\newcommand{\ior}{\pmb\lor}
\newcommand{\iimplies}{\pmb\rightarrow}
\newcommand{\iequiv}{\pmb\leftrightarrow}
\newcommand{\iforall}{\pmb\forall}
\newcommand{\iexists}{\pmb\exists}
\newcommand{\ieq}{\pmb\approx}
\newcommand{\ineq}{\pmb{\not\approx}}

\newcommand{\comb}[1]{\ensuremath{\mathsf{#1}}}
\newcommand{\neglit}[1]{\ensuremath{#1 \not\eq \itrue}}
\newcommand{\poslit}[1]{\ensuremath{#1 \eq \itrue}}
\newcommand{\lland}{\mathrel\land}
\newcommand{\llor}{\mathrel\lor}
\newcommand{\ccup}{\mathrel\cup}
\newcommand{\ccap}{\mathrel\cap}

\newcommand{\db}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\dbvar}[1]{\ensuremath{\textbf{\textit{#1}}}}
\newcommand{\internallam}{\ensuremath{\texttt{LAM}}}
\newcommand{\internalat}{\ensuremath{\texttt{@}}}

\newcommand{\lambdabf}{\pmb{\lambda}} %%% TYPESETTING
\newcommand{\betabf}{\pmb{\beta}} %%% TYPESETTING
\newcommand{\etabf}{\pmb{\eta}} %%% TYPESETTING

\begin{document}

\newcommand\CORR{\textsuperscript{(\vthinspace\Letter\vthinspace)}}

\confrep
{\title{Extending a High-Performance Prover to Higher-Order Logic}}
{\title{Extending a High-Performance Prover to Higher-Order Logic \\ (Technical Report)}}
%{\title{Extending a Brainiac Prover to \\ Full Higher-Order Logic}}
%{\title{Extending a Brainiac Prover to \\ Full Higher-Order Logic\\(Technical Report)}}

\author{Petar Vukmirovi\'c\inst{1}\CORR
  \and
  Jasmin Blanchette\inst{1,2}
  \and
  Stephan Schulz\inst{3}
  }

\institute{
  Vrije Universiteit Amsterdam, Amsterdam, the Netherlands \\
%  \email{p.vukmirovic@vu.nl}
   \email{\{p.vukmirovic,j.c.blanchette\}@vu.nl}
%   \and Max-Planck-Institut f\"ur Informatik, Saarland Informatics Campus, Saarbr\"ucken,~Germany \\
%   \email{jasmin.blanchette@mpi-inf.mpg.de}
  \and Universit\'e de Lorraine, CNRS, Inria, LORIA, Nancy, France \\
    \email{jasmin.blanchette@loria.fr}
  \and DHBW Stuttgart, Stuttgart, Germany \\
    \email{stephan.schulz@dhbw-stuttgart.de}
}

\maketitle

\begin{abstract}%
  The automatic discharge of tedious subgoals is high on the wishlist of many
  users of proof assistants. Some proof assistants discharge such goals
  by translating them to first-order logic and invoking an efficient prover on
  them, but much is lost in translation. As an alternative,
  we propose to extend first-order provers with native support for
  higher-order features. Building on our extension of E to $\lambda$-free
  higher-order logic, we now extend E to full higher-order logic.
  The resulting prover is the \NumberOK{strongest} one on benchmarks coming from a
  proof assistant, and the second best on TPTP benchmarks.
  %It also incurs no overhead on first-order problems.  -- sounds like a detail --JB
\end{abstract}

\setcounter{footnote}{0}

\section{Introduction}
\label{sec:ehoh2:introduction}

In the last few decades, proof assistants have become indispensible tools for
developing trustworthy formal proofs. They are used both in academia to verify
mathematical theories \cite{th-05-flyspeck} and in
industry to verify the correctness of hardware \cite{ckmg-99-hw-verification}
and software \cite{xl-09-compcert, gk-09-sel4, rgea-16-certikos}.
%
However, due to the lack of strong built-in proof automation, proving
seemingly simple goals can be a tedious manual task. To mitigate
this, many proof assistants include a subsystem such as CoqHammer,
HOL(y)Hammer, or Sledgehammer \cite{bkpu-16-hammering} that translates
higher-order goals to first-order logic and passes them to efficient
first-order automatic provers. If a first-order prover succeeds, the proof is
reconstructed and the goal is closed.

Unfortunately, the translation of higher-order constructs is clumsy and leads
to poor performance on goals that require higher-order reasoning.
Using native higher-order provers
such as %Leo-III \cite{sb-21-leo3} or   --- the reference is about Leo-II and Satallax --JB
Satallax \cite{cb-12-satallax} as backends
is not necessarily a solution because they are much less optimized than their
first-order counterparts \cite{ns-13-leo2sh}. To bridge this gap, in 2016
we proposed to develop a new generation of higher-order provers that
%are not built from the ground up, but rather
extend the arguably most successful first-order calculus, superposition, to
higher-order logic, starting from a position of strength.

Our research has focused on three milestones:\ supporting $\lambda$-free
higher-order logic, adding support for $\lambda$-terms, and adding support for
first-class Boolean terms. In 2019, we extended the state-of-the-art first-order
prover E \cite{scv-19-e23} with a $\lambda$-free superposition calculus
\cite{section-ehoh}, obtaining a version of E called Ehoh, as a stepping stone
towards full higher-order logic. Together with Bentkamp, Tourret, and Waldmann,
we have since developed calculi, called \emph{$\lambda$-superposition}, corresponding
to the other two milestones \cite{bbtvw-21-sup-lam,bbtv-21-full-ho-sup} and
implemented them in the experimental superposition prover Zipperposition
\cite{sc-15-simon-phd}. This prover is implemented in OCaml and not nearly as
efficient as E. Nevertheless, it has won the higher-order division of the CASC
theorem prover competition \cite{gs-21-cascj10} in 2020 and 2021 by a wide
margin, ending nearly a decade of Satallax domination.

In this chapter, we fulfill a three-year-old promise: We present the extension of
Ehoh to full higher-order logic (Sect.~\ref{sec:ehoh2:logic}) using incomplete variants
of $\lambda$-superposition. We call this prover \ehohii.
%
The $\lambda$-superposition calculi were
previously implemented in Zipperposition, and
extensive experiments with various heuristic choices have been performed
\cite{section-making-ho-work}. In \ehohii{}'s implementation, we used
these experiences to choose a set of effective rules that could easily be
retrofitted into an originally first-order prover. Another principle that guided 
the design of \ehohii{} was \emph{gracefulness}: we made sure that our changes
do not impact the strong first-order performance of E and Ehoh. 

% We
% also used the experience of fine-tuning the calculi
% \cite{section-making-ho-work} in Zipperposition to carefully choose which
% calculus extensions and heuristics to use in E.

One of the main challenges we faced was retrofitting $\lambda$-terms in Ehoh's
term representation (Sect.~\ref{sec:ehoh2:terms}). Furthermore, Ehoh's main inference
engine was designed with the assumption that it will be used with
inferences that compute a most general unifier. We
implementd a higher-order unification procedure \cite{unif-section}
that can return multiple unifiers (Sect.~\ref{sec:ehoh2:unif-match-index}) and
integrated it in the inference engine. Finally, we extended and adapted the
superposition rule, resulting in an incomplete, pragmatic variant of
$\lambda$-superposition (Sect.~\ref{sec:ehoh2:calculus}).

We \NumberOK{evaluated} \ehohii{} on a selection of proof assistants benchmarks
as well as all higher-order theorems in the TPTP library \cite{gs-17-tptp}
(Sect.~\ref{sec:ehoh2:eval}). We found
that \ehohii{} clearly outperforms Ehoh on all benchmarks. It outperformed all other higher-order provers on
proof assistant benchmarks; on TPTP benchmarks it ended up second only to 
to the cooperative version of Zipperposition, which employs Ehoh as a
backend. An arguably fairer comparison without the backend puts \ehohii{} in the
first place for both benchmark suites.
We also compared the performance of \ehohii{} with E on first-order
%%% @PETAR: I made this slightly sentence shorter and more forceful. Please
%%% check if you agree. --JB
problems and found that little overhead has been introduced by the
extension to higher-order logic.

\kern-1pt %% TYPESETTING hack

\section{Logic} %---0.5p
\label{sec:ehoh2:logic}

% TO BE CHANGED AS NECESSARY
Our target logic is monomorphic classical higher-order logic with Hilbert choice.
The following text is partly based on Section 2 of Vukmirovi\'c et al.~\cite{section-making-ho-work}.

Terms $s, t, u, v$ are inductively defined as free variables $F, X, \ldots$, bound variables $x,
y, z, \dotsc$, constants $\cst{f}, \cst{g},\allowbreak \cst{a},\allowbreak \cst{b},
\dotsc$, applications $s \, t$, and $\lambda$-abstractions $\lamx{s}$.
The syntactic distinction between free and bound variables
gives rise to \emph{loose bound variables} (e.g., $y$ in $\lamx{y \, \cst{a}}$)
\cite{tn-93-patterns}.
%We shorten iterated application $s \, t_1 \, \cdots \, t_n$ to $s \,
%\overline{t}_n$ and $\lambda$-abstraction $\lambda x_1. \, \cdots \, \lambda
%x_n. \> s$ to $\lam{\overline{x}_n}{s}$.

We let $s \, \overline{t}_n$ stand for $s \, t_1 \, \ldots \, t_n$ and
$\lam{\overline{x}_n}{s}$ for $\lambda x_1. \ldots \lambda x_n. \> s$. Every
$\beta$-normal term can be written as $\lam{\overline{x}_m}{s \,
\overline{t}_n}$, where $s$ is not an application; we call $s$ the \emph{head}
of the term. If $s$ is a free variable, we call the term \emph{flex}; otherwise,
the term is \emph{rigid}. A term of type $o$, where $o$ is the distinguished Boolean type,
is called a \emph{formula}. A type whose type is of the form $\tau_1 \to
\cdots \to \tau_n \to o$ is called
a \emph{predicate}. Logical symbols are part of the signature and
may thus occur within terms. We write them in bold: $\pmb{\neg}, \iand, \ior,
\iimplies, \iequiv, \ieq, \dots$.

On top of the terms, we define some clausal structure, as is customary in
first-order logic.
A literal $l$ is an equation $s \eq t$ or a
disequation $s \not\eq t$. A clause is a finite multiset of literals,
interpreted and written disjunctively:\ $l_1 \llor \cdots \llor l_n$.
Notice that the clause-level operators are not set in bold.
Predicate literals are encoded as (dis)equations with $\itrue$ based on their sign; for
example, $\cst{even}(x)$ is encoded as $\poslit{\cst{even}(x)}$, and
$\neg\,\cst{even}(x)$ as $\neglit{\cst{even}(x)}$.

\section{Terms}
\label{sec:ehoh2:terms}

\looseness=-1
E has been designed around perfect term sharing %or exa
\cite{ls-01-shared}, a design that we carried on to  Ehoh and \ehohii{}: Any two structurally identical terms are
guaranteed to be the same object in memory. This is achieved through term
\emph{cells}, which represent individual terms. Each cell has (among other fields)
(1)~\texttt{f\_code}, an integer corresponding to the symbol at the head of the term (negative
if the head is a free variable, positive otherwise); (2)~\texttt{num\_args},
corresponding to the number of arguments applied to the head; and (3)~\texttt{args},
a size-\texttt{num\_args} array of pointers to argument terms. We
use the first-order notation $\cst{f}(s_1, \ldots, s_n)$ to denote a cell whose
\texttt{f\_code} corresponds to $\cst{f}$, \texttt{num\_args} equals $n$, and
\texttt{args} points to the cells for $s_1, \ldots s_n$.

Ehoh represents $\lambda$-free higher-order terms using flattened, spine notation.
Thus, the terms $\cst{f}$, $\cst{f} \, \cst{a}$ and $\cst{f} \, \cst{a} \,
\cst{b}$ are represented by the cells $\cst{f}$,
$\cst{f}(\cst{a})$, and $\cst{f}(\cst{a}, \cst{b})$, respectively.
To ensure free variables are
perfectly shared, Ehoh treats applied free variables differently: arguments are
not applied directly to a free variable, but using an internal symbol
\internalat{} of variable arity. For example, the term $X \, \cst{a} \, \cst{b}$ is
represented by the cell $\internalat(X, \cst{a}, \cst{b})$. This ensures that
two different occurrences of the free variable $X$ correspond to the same object,
which makes substitutions more efficient \cite{section-ehoh}.
%%% @PETAR(read): This tries to explain an invisible problem. Either we need to explain
%%% it better, or we just omit it (my favorite option). It's old Ehoh stuff anyway. --JB
%Yet, to
%efficiently access variable arguments we do not use binary application symbol,
%but flattened representation.

\ourpara{Representation of $\lambdabf$-Terms} To support the calculus for full
higher-order logic \cite{bbtv-21-full-ho-sup}, the cell data structure must
be extended to support the $\lambda$ binder. We use the locally nameless
representation \cite{ac-12-locally-nameless} for this purpose: De Bruijn indices
%%% @PETAR(checked): Double-check "possibly loose".
represent (possibly loose) bound variables, whereas we keep the current
representation for free variables.

%%% @PETAR(read): Sounds odd---we wouldn't be extending the term representation of an
%%% already HO prover. --JB
%Unlike other higher-order provers, most of which are written in functional
%programming languages,
Extending the term representation of Ehoh with a new term
kind involves intricate manipulation of the cell data structure. De Bruijn
indices must be represented as other cells with either negative or positive
\texttt{f\_code}. However, this has to be done in such a way that De Bruijn
index can never be mistaken for a function symbol or a variable.

Other than possibly being instantiated during $\beta$-reduction, De Bruijn
indices mostly behave as constants. Therefore, we decided to represent De
Bruijn indices using positive \texttt{f\_code}s: The De Bruijn index of value $i$
will have $i$ as the~\verb|f_code|. To ensure De Bruijn indices are not
mistaken for function symbols, we use the \texttt{properties} bitfield of the
cell, which holds precomputed properties of %shared cell
%%% @PETAR(checked, you are right): I don't understand why you wrote "shared cell". Please double-check. --JB
the cell. We introduce the
property \texttt{IsDBVar} to denote that the cell represents a De
Bruijn index. Any attempt to create a De Bruijn index is performed through
a dedicated library function that sets \texttt{IsDBVar} property for every term it
returns. When given the same De Bruijn index and type, this function is
guaranteed to always return the same object. Finally, we have guarded all the
functions and macros that manipulate function codes with the check if the
property \texttt{IsDBVar} is set. To ensure perfect sharing of every occurrence
of De Bruijn indices, arguments to De Bruijn indices are applied like for free
variables, using \internalat{}.

Extending cells to support $\lambda$-abstraction is easier. Each
$\lambda$-ab\-strac\-tion has the distinguished function code \internallam{} as the head
symbol and two arguments:\ (1)~a De Bruijn index 0 of the type of abstracted variable;
(2)~the body %(matrix)
of the $\lambda$-abstraction. Consider
the term $\lambda x. \, \lambda y.\, \cst{f}\, x \, x$, where both $x$ and $y$ have
the type~$\iota$. This term is represented as $\lambda\,\lambda\, \cst{f} \,
\db{1} \, \db{1}$ in locally nameless representation, where bold numbers
represent De Bruijn indices. In \ehohii{}, the same term is represented by the cell
$\internallam(\db{0}, \internallam(\db{0}, \cst{f}(\db{1}, \db{1})))$,
where all De Bruijn variables have type~$\iota$. 

The first argument of $\internallam$ is
redundant, since it can be deduced from the type of the $\lambda$-abstraction.
However, basic $\lambda$-term manipulation operations often require access to
this term. We store it explicitly to avoid creating it repeatedly.

\ourpara{Efficient $\betabf$-Reduction}
Terms are stored in $\beta\eta$-reduced form. As these two reductions are
performed very often, they ought to be efficient. Ehoh
performs $\beta$-reduction by reducing the leftmost outermost $\beta$-redex
first. To represent $\beta$-redexes, E uses the \internalat{} symbol. Thus,
the term $(\lambda
x.\, \lambda y.\,  (x \, y)) \, \cst{f} \, \cst{a}$ is represented by
$\internalat(\internallam(\db{0}, \internallam(\db{0}, \internalat(\db{1}, \db{0}))),
\cst{f}, \cst{a})$. Another option would have been to add arguments applied to
$\lambda$-term directly to the $\lambda$ representation (as in
$\internallam(\db{0}, \internallam(\db{0}, \allowbreak \internalat(\db{1}, \db{0})),
\cst{f}, \cst{a})$), but this would break the invariant
that $\internallam$ has two arguments. Furthermore, replacing free
variables with $\lambda$-abstractions (e.g., replacing $X$ with $\lambda
x. \, x$ in $\internalat(X, \cst{a})$) would require additional normalization.

A term can be $\beta$-reduced as follows: When a cell of the
form $ \internalat(\internallam(\db{0}, s),t)$ is encountered, the field
\texttt{binding} (normally used to record the substitution for a free variable) of the
cell $\db{0}$ is set to $t$. Then $s$ is traversed to instantiate every
loose occurrence of $\db{0}$ in $s$ with \texttt{binding}, whose loosely
bound De Bruijn indices are shifted by the number of $\lambda$ binders above
the occurrence of $\db{0}$ in $s$ \cite{fk-01-deBruijn}. Next, the same
procedure is performed on the resulting term and its subterms, in
leftmost outermost fashion.

\ehohii{}'s basic $\beta$-normalization mechanism works in this way, but it
features a few optimizations.
First, \ehohii{} recognizes terms of the form $(\lam{\overline{x}_n}{s}) \,\overline{t}_n$ 
and performs parallel replacement of the bound variables $x_i$ with
$t_i$. Since intermediate terms are not constructed, this reduces the number of
recursive function calls and calls to the cell allocator.

Second, in line with the gracefulness principle, we want \ehohii{} to incur
little (if any) overhead on first-order problems and to
excel on higher-order problems with a large first-order component. If
$\beta$-reduction is implemented naively, finding a $\beta$-redex involves
traversing the entire term. On purely first-order terms, $\beta$-reduction
is then a complete waste of time.

To avoid this, we use Ehoh's perfectly shared
terms and their \texttt{properties} field.
%
We introduce the property \texttt{HasBetaReducibleSubterm} which is set if
%%% @PETAR(is is good): "is" or "may be" below? (Overapproximation?) --JB
a cell is $\beta$-reducible.
Whenever a new cell that contains a
$\beta$-reducible term as a direct subterm is shared, the property is set.
Setting of the property is inductively continued when further superterms are
shared. For example, in the term $t = \cst{f} \, \cst{a} \, (\cst{g} ((\lambda x.\,
x)\,\cst{a}))$, the cells for $(\lambda x.\, x)\,\cst{a}$,
$\cst{g}\,((\lambda x.\, x)\,\cst{a})$, and $t$ itself have the property
\texttt{HasBetaReducibleSubterm} set.
%
When it needs to find $\beta$-reducible subterms, \ehohii{} will visit only the
cells with this property set. This further means that on first-order
subterms, a single bit masking operation is enough to determine that no subterm
should be visited.

Along similar lines, we added a property \texttt{HasDBSubterm} that
caches whether the cell contains a De Bruijn subterm. This
%% @PETAR(you are right): Why "also"? What's the other thing?
%also
makes
instantiating De Bruijn indices during $\beta$-normalization faster, since only the
subterms that contain De Bruijn indices must be visited. Similarly, some other
operations such as shifting De Bruijn indices or determining whether a term is closed
(i.e., it contains no loose bound variables) can be sped up or even avoided
if the term is first-order.

\ourpara{Efficient $\etabf$-Reduction}
The term $\lambda x.\, s \, x$ is $\eta$-reduced to $s$ whenever $x$ does not appear
unbound in $s$. Caching the property of $\eta$-reducibility of the term is not
as beneficial as the one for $\beta$-reducibility, because checking this
property at the term's top level is done in $O(|s|)$, compared with constant time
for $\beta$-reducibility.
However, we use the observation that a term cannot be $\eta$-reduced if it has
no $\lambda$-abstraction subterms and introduce a property \texttt{HasLambda}
that notes the presence of $\lambda$-abstraction in a term. Only
terms with this property are visited during $\eta$-reduction.

\ehohii{} performs parallel $\eta$-reduction: It recognizes terms of the form
$\lam{\overline{x}_n}{s \, \overline{x}_n } $ such that none of $x_i$
occurs unbound in $s$. If done naively, reducing terms of this kind requires up to $n$
traversals of $s$ to check if each $x_i$ occurs in $s$. In \ehohii{}, exactly one
traversal of $s$ is required.

\looseness=-1
More specifically, when $\eta$-reducing a cell $\internallam(\db{0}, s)$,
\ehohii{} considers all $\lambda$ binders in $s$ as well. In general,
the cell will be of the form
$\internallam(\db{0}, \dotsc,\allowbreak \internallam(\db{0}, t) \ldots)$,
where $t$ is not a
$\lambda$-abstraction, and $l$ is the number of $\internallam$ symbols above $t$. Then \ehohii{} breaks down the body~$t$ into a maximal
decomposition $u\, (\dbvar{n}\db{{}-1}) \, \ldots \, \db{1} \, \db{0}$.
If $n = 0$, the cell is not $\eta$-reducible.
Otherwise, $u$ is traversed to determine the
minimal index~$\dbvar{j}$ of a loose De Bruijn index,
taking $\dbvar{j} = \infty$ if no such index exists.
%%% @PETAR(I prefer leftmost as we are going top level left-to-right and not
%bottom-up as innermost can suggest): Check "innermost".
%%% @PETAR(you are right, but I will add righmost outermost to be more precise): But it's rightmost then not leftmost! In the example, the very first
%%% binder remains (the one to which the 2's point). --JB
We can then remove the $k = \min\{j,l,n\}$ rightmost outermost $\lambda$ binders in $\internallam(\db{0},
\ldots, \internallam(\db{0}, t) \ldots)$ and replace %the body
$t$ by the variant of
$u \allowbreak\, (\dbvar{n}\db{{}-1}) \allowbreak\, \ldots \, (\dbvar{k}\db{{}+1}) \, \dbvar{k}$
obtained by shifting the loose De Bruijn indices down by~$k$.

To illustrate this convoluted De Bruijn arithmetic, we consider the
term $\lambda x. \, \lambda y. \, \lambda z.\, \cst{f} \, x \, x \, y
\, z$. This term is represented by the cell $\internallam(\db{0},
\internallam(\db{0}, \internallam(\db{0},\allowbreak \cst{f}(\db{2}, \db{2}, \db{1},
\db{0}))))$. \ehohii{} splits $\cst{f}(\db{2}, \db{2}, \db{1}, \db{0})$ into
two parts:\ $u = \cst{f} \, \db{2}$ and the arguments $\db{2}, \db{1},
\db{0}$. Since the minimal index in $u$ is $\db{2}$, we can
omit the De Bruijn indices $\db{1}$ and $\db{0}$ and their $\lambda$ binders,
yielding the $\eta$-reduced cell $\internallam(\db{0}, \cst{f}(\db{0},
\db{0}))$.

The use of the \texttt{HasLambda} property ensures that $\eta$-reduction is not
tried on first-order or $\lambda$-free higher-order terms, whereas parallel
$\eta$-reduction both speeds up $\eta$-reduction and avoids creating a linear
number of intermediate terms. For finding the minimal loose De Bruijn index,
optimizations such as the \texttt{HasDBSub\-term} property are used.

\ourpara{Representation of Boolean Terms}
Ehoh represents Boolean terms using
cells whose \texttt{f\_code}s correspond to internal codes reserved
for logical symbols. Quantified formulas are represented by cells in which the
first argument is the quantified variable and the second one is the body of the
quantified formula. For example, the term $\iforall x.\, \cst{p} \, x$ corresponds
to the cell $\iforall(X, \cst{p}(X))$, where $X$ is a regular free variable.
% as Ehoh has no concept of a bound variable.

This representation is convenient for parsing formulas and clausification, which
is what Ehoh uses it for, but it
causes $\alpha$-normalization issues during the actual proof search: In
full higher-order logic, Booleans can appear as subterms in clauses, as in
$\cst{q}(X) \llor \cst{p}(\iforall(X, \cst{r}(X)))$;
instantiating $X$ in the first literal should not influence $X$ in the second
literal.

To avoid this issue, we use $\lambda$ binders to represent quantified formulas.
Thus, $\iforall x. \, s$ is represented by $\iforall \, (\lambda x. \, s)$.
Quantifiers are then unary symbols that do not directly bind
the variables but delegate this responsibility to a $\lambda$-abstraction.
%
Since \ehohii{} represents bound variables using De Bruijn indices, this solves the
$\alpha$-conversion issues. However, this solution is incompatible with
thousands of decades-old lines of clausification code that assumes the Ehoh
representation of
quantified formulas. Therefore, \ehohii{} converts quantified
formulas only after clausification, for Boolean terms that appear in a
higher-order context (e.g., as argument to a function symbol).

\ourpara{New Term Orders}
The $\lambda$-superposition calculus is parameterized by a term order that is
used to break symmetries in the search space.
We implemented the versions of the Knuth--Bendix order (KBO) and lexicographic
path order (LPO) for higher-order terms with $\lambda$-abstractions described by
Bentkamp et al.~\cite{bbtv-21-full-ho-sup}. These orders encode
encode $\lambda$-terms as first-order terms and then invoke the standard KBO or
LPO. %Trading elegance for efficiency, our
%implementation does not compute the first-order translation.
%Rather,
For efficiency, we implemented separate KBO and
LPO functions that compute the order directly, intertwining the encoding and
the order computation.
%%% @PETAR(yes): Irrelevant detail?
%We kept the
%implementation of $\lambda$-free KBO as \ehohii{} can be run in $\lambda$-free mode.

Ehoh cells contain a \verb|binding| field that can be used to store the
substitution for a free variable. Substitutions can then be applied by following
the \texttt{binding} pointers, replacing each free variable with its instance.
Thus, when Ehoh needs to perform a KBO or LPO comparison of an instantiated term,
it needs only follow the \texttt{binding} pointers.
In full higher-order logic, however, instantiating a variable can trigger a
series of $\beta\eta$-reductions,
changing the shape of the term dramatically. To pevent this, \ehohii{}
computes the $\beta\eta$-reduced instance of the terms
before comparing them using KBO or LPO.

\section{Unification, Matching, and Term Indexing}
\label{sec:ehoh2:unif-match-index}


Standard superposition crucially depends on the concept of a most general unifier
(MGU).
In higher-order logic, such a unifier does not in general exist, and the concept
%%% @PETAR: CSU, infinite
is replaced by that of a complete set of unifiers (CSU), which may be infinite.
Vukmirovi\'c et al.\ \cite{unif-section} designed an efficient
procedure to enumerate a CSU for a term pair. It is implemented in Zipperposition, together with some extensions to term indexing.
In \ehohii{}, we further improve the performance of this procedure by implementing a terminating,
incomplete variant. We also introduce a new indexing data structure.

\ourpara{The Unification Procedure} 
% Two main guiding principles behind our
% unification procedure are laziness and redundancy elimination. The first one is
% embodied in the fact that the procedure makes sure that it never fully
% normalizes the terms or fully applies the substitution if that is not necessary.
% Redundancy elimination is achieved through \emph{oracles}
% \cite{unif-section}: the procedures that solve the unification
% problem for a subclass of higher-order terms on which unification is decidable,
% and for \ehohii{}, unary. If those procedures determine that the given terms
% belong to their class of terms, they will return the most general unifier,
% avoiding redundant computations and in some cases even non-termination.
The unification procedure works by maintaining a list of unification pairs to be solved.
After choosing a pair, it first normalizes it by $\beta$-reducing and
instantiating the heads of both terms in the pair. Then, if either head is a
variable, it computes an appropriate binding for this variable, thereby
approximating the solution.

Unlike in first-order and $\lambda$-free higher-order unification,
in full higher-order case there may be many bindings that lead to a solution. To
reduce this mostly blind guessing of bindings, the procedure features support
for \emph{oracles} \cite{unif-section}. These are procedures that
solve the unification problem for a subclass of higher-order terms on which
unification is decidable and, for \ehohii{}, unary. Oracles help
increase performance, avoid nontermination, and avoid redundant bindings.

Vukmirovi\'c et al.\ described their procedure as a transition system. In
\ehohii{}, the procedure is implemented nonrecursively, and the unifiers are
enumerated using an iterator object that encapsulates the state of the unifier
search. The iterator consists of five fields:
(1)~\emph{constraints}, which holds the unification
constraints;
(2)~\emph{bt\_state}, a stack that contains information necessary to backtrack
to a previous state;
(3)~\emph{branch\_iter}, which stores how far we
are in exploring different possibilities from the current search node;
(4)~\emph{steps}, which remembers how many different unification bindings (such as
imitation, projection, and identification) are applied; and
(5)~\emph{subst},
a stack storing the variables bound so far.

\newcommand{\vn}[1]{\ensuremath{\mathit{#1}}} %variable name
\newcommand{\cn}[1]{\ensuremath{\textsc{#1}}} %const name
\newcommand{\fc}[1]{\ensuremath{\textsc{#1}}} %function call
\algrenewcommand\alglinenumber[1]{{\color{gray} \tiny #1}}

The iterator is initialized to hold the original problem in \vn{constraints},
and all other fields are initially empty. The unifiers are retrieved one by one by
calling the function \fc{ForwardIter}. It returns \fc{True} if the iterator made
progress, in which case the unifier can be read via the iterator's
$\vn{subst}$ field. Otherwise, no more unifiers can be found, and the iterator is
no longer valid. The function's pseudocode is given below, including two
auxiliary functions:
  
\begin{algorithmic}[]
  \Function{NormalizeHead}{$\vn{t}$}
    \If{$\vn{t.head} = \internalat \land \vn{t.args}[0].\vn{is\_lambda}()$}
      \State reduce the top-level $\beta$-redex in $t$
      \State \Return $\fc{NormalizeHead}(t)$
    \ElsIf{$\vn{t.head.is\_var()} \land \vn{t.head.binding} \not= \cn{nil}$}
      \State $\vn{t.head} \gets \vn{t.head.binding}$
      \State \Return $\fc{NormalizeHead}(t)$
    \Else{}
      \State \Return $t$
    \EndIf
  \EndFunction
  
  \vspace{\jot}
  \Function{BacktrackIter}{$\vn{iter}$}
    \If{\vn{iter.bt\_state.empty()}}
      \State clear all fields in \vn{iter}
      \State \Return \cn{False}
    \Else
    \State pop $(\vn{constraints}, \vn{branch\_iter}, \vn{steps}, \vn{subst})$ from $\vn{iter.bt\_state}$
    \State set the corresponding fields of $\vn{iter}$
    \State \Return \cn{True}
    \EndIf
  \EndFunction
  \vspace{\jot}
  \Function{ForwardIter}{$\vn{iter}$}
  \State $\vn{forward} \gets \neg \vn{iter.constraints.empty()} \lor \fc{BacktrackIter}(\vn{iter})$
  \While{$\vn{forward} \land \neg \vn{iter.constraints.empty()}$}
    \State $(\vn{lhs}, \vn{rhs}) \gets $ pop pair from $\vn{iter.constraints}$
    \State $\vn{lhs} \gets \fc{NormalizeHead}(\vn{lhs})$
    \State $\vn{rhs} \gets \fc{NormalizeHead}(\vn{rhs})$
    \State normalize and discard the $\lambda$ prefix of $\vn{lhs}$ and $\vn{rhs}$

    \If{$\neg\vn{lhs.head.is\_var()} \land \vn{rhs.head.is\_var()}$}
      \State swap $\vn{lhs}$ and $\vn{rhs}$
    \EndIf

    \If{$\vn{lhs.head.is\_var()}$}
      \State $\vn{oracle\_res} \gets \fc{Fixpoint}(\vn{lhs, rhs, iter.subst}) $
      \If{$\vn{oracle\_res} = \cn{NotInFragment}$}
        \State $\vn{oracle\_res} \gets \fc{Pattern}(\vn{lhs, rhs, iter.subst}) $
      \EndIf

      \If{$\vn{oracle\_res} = \cn{NotUnifiable}$}
        \State $\vn{forward} \gets \fc{BacktrackIter(iter)}$
      \ElsIf{$\vn{oracle\_res} = \cn{NotInFragment}$}
        \State $\vn{n\_steps}, \vn{n\_branch\_iter}, \vn{n\_binding} \gets$
        \State $\quad \fc{NextBinding}(\vn{lhs}, \vn{rhs}, \vn{iter.steps}, \vn{iter.branch\_iter})$
        
        \If{$\vn{n\_branch\_iter} \not= \fc{BindEnd}$ }
          \State push pair (lhs,rhs) back to \vn{iter.constraints}
          \State push quadruple $(\vn{iter.constraints}, \vn{n\_branch\_iter}, $
          \State                  $\quad \vn{iter.steps}, \vn{iter.subst})$ onto $\vn{iter.bt\_state}$
          \State extend $\vn{iter.subst}$ with $n\_binding$
          \State $\vn{iter.steps} \gets n\_steps$
          \State $\vn{iter.branch\_iter} \gets \cn{BindBegin}$
        \ElsIf{\vn{lhs.head = rhs.head}}
          \State create constraint pairs of arguments of $\vn{lhs}$ and $\vn{rhs}$
          \State $\quad$ and push them to \vn{iter.constrants}
          \State $\vn{iter.branch\_iter} \gets \cn{BindBegin}$
        \EndIf
      \EndIf
    \ElsIf{$\vn{lhs.head} = \vn{rhs.head}$}
      \State create constraint pairs of arguments of $\vn{lhs}$ and $\vn{rhs}$
      \State $\quad$ and push them to \vn{iter.constrants}
    \Else{} $\vn{forward} \gets \fc{BacktrackIter(\vn{iter})}$
    \EndIf
  \EndWhile
  \State \Return $\vn{forward}$
  \EndFunction
\end{algorithmic} 

\looseness=-1
\fc{ForwardIter} begins by backtracking if the previous attempt was successful (i.e., 
all constraints were solved). If it finds a state from which it can continue,
it takes term pairs from $\vn{constraints}$ until
there are no more constraints or it is determined that no unifier exists. The terms
are normalized by instantiating the head variable with its binding and then
reducing the potential top-level $\beta$-redex that appears. This instantiation
and reduction process is repeated
until there are no more top-level $\beta$-redexes and the head is
not a variable bound to some term. Then the term with shorter
$\lambda$ prefix is expanded (only on the top-level) so that both have the
same length of $\lambda$ prefix. Finally, the $\lambda$ prefix is ignored, and we
focus only on the body. In this way, we avoid fully substituting
and normalizing terms and perform just enough operations
to determine the next step of the procedure.

If either term of the constraint is flex, we first invoke oracles to solve the
constraint. \ehohii{} implements the most efficient oracles implemented in
Zipperposition:\ fixpoint and pattern oracle
\cite[Sect.~6]{unif-section}. An oracle can return three results:
(1)~there is an MGU for the pair (\cn{Unifiable}), which is recorded in
\emph{subst}, and the next pair in $\emph{constraints}$ is tried;
(2)~no MGU exists
for the pair (\cn{NotUnifiable}), which causes the iterator to backtrack;
% to previous state;
(3)~if the pairs do not belong to the subclass that oracle
can solve (\cn{NotInFragment}), we generate possible variable bindings---that is,
we guess the approximate form of the solution.

\ehohii{} has a special module that generates bindings (\cn{NextBinding}). This
module is given the current constraint and the values of \emph{branch\_iter} and
\emph{steps}, and it either returns the next binding and the new values of
\emph{branch\_iter} and \emph{steps} or reports that all different variable
bindings are exhausted. The bindings that \ehohii{}'s unification procedure creates
are imitation, Huet-style projection, identification, and elimination (one
argument at a time) \cite[Sect.~3]{unif-section}. A limit on the
total number of applied binding rules can be set, as well as a limit on the
number of individual rule applications. The binding module checks whether limits
are reached using the iterator's \vn{steps} field.

This is the only point in the procedure where the search tree branches an
different possibilities (bindings) are explored.
Thus, when \ehohii{} follows the branch indicated by the
binding module, it records its current state in the backtracking stack. The
values of \vn{branch\_iter} are either \cn{BindBegin}, which denotes that no
binding was created, intermediate values that \cn{NextBinding} uses
to remember how far through bindings it is, and \cn{BindEnd}, which indicates
that all bindings are exhausted.

If all bindings are exhausted, the procedure checks whether the pair is
flex--flex and both sides have the same head. If so, the pair is decomposed and
constraints are derived from the arguments of the pair. Otherwise, the iterator
backtracks.
% @PETAR(actually the procedure applies bindings to same-head flex-flex pairs,
% so the text is OK -- we do get "stuck" enumerating those flex-flex pairs. The
% limits help here): Maybe explain here that you don't want a pragmatic prover
% like lambdaE to get stuck in wildly explosive flex-flex pairs? --JB
%
If the constraint is rigid--rigid, for unification to succeed the heads of both
sides of the constraint must be the same. Unification then continues
with new constraints derived from the arguments. Otherwise, the iterator must be
backtracked.

\ourpara{Matching} In Ehoh, the matching algorithm is mostly used inside
simplification rules such as demodulation and subsumption \cite{ss-02-brainiac}.
As these rules must be efficiently performed, using any complex matching
algorithm is not a viable option. Instead, we implemented a matching algorithm
for the pattern class of terms \cite{tn-93-patterns} to complement Ehoh's
$\lambda$-free higher-order matching algorithm \cite[Sect.~4]{section-ehoh}.
A term is a \emph{pattern} if each of its free variables either
has no arguments (as in first-order logic) or is applied to distinct De Bruijn
indices.

To determine which of the two algorithms to call (pattern or $\lambda$-free), we
added a cached property \texttt{HasNonPatternVar}, which is set for terms of the
form $X \,
\overline{s}_n$ where $n>0$ and either there exists some $s_i$ that is not a De Bruijn
index or there exist indices $i < j$ such that $s_i = s_j$ is
a De Bruijn index. This property is propagated to the superterms when they are
perfectly shared. This allows later checks if a term belongs to the pattern
class to be performed in constant time.

% When we need to determine if one term matches the other one we first check if
% any of them has a non-pattern applied variables. If so, $\lambda$-free
% higher-order matching is tried. This algorithm is adjusted to treat $\lambda$ prefixes
% as above in unification algorithm (ensuring that $\beta$-redexes can never occur)
% and that free variables are never bound to terms that have loose bound variables.
% Else if the terms have only pattern variables,
% pattern matching algorithm is tried. Note that in this sense, first-order order
% terms behave as pattern terms and thus pattern matching algorithm is run on
% them. For example, on terms $(X \, \cst{b}, \cst{f} \, \cst{a} \, \cst{b})$
% $\lambda$-free matching algorithm is invoked; on $(\cst{f} \, (\lambda x. \,
% \lambda y.\, X \, x), \cst{f} \, \cst{g})$ pattern matching is tried.

We have modified the $\lambda$-free higher-order matching algorithm to treat
$\lambda$ prefixes as above in unification procedure---by bringing the prefixes
to the same length and ignoring them afterwards. This ensures that the
algorithm will never try to match a free variable to a $\lambda$-abstraction,
making sure that $\beta$-redexes never appear (as in original $\lambda$-free
higher-order matching). We also modified the algorithm to ensure that free variables
are never bound to terms that have loose bound variables. This algorithm cannot
find many complex matching substitutions (matchers), but it can
efficiently determine whether two terms are variable renamings of each other
or whether a simple matcher can be used as in the case of $(X \, (\lambda x. \,x) \,
\cst{b}, \cst{f} \, (\lambda x. \,x) \, \cst{b})$, where ${X \mapsto \cst{f}}$ is
usually the desired matcher. If this algorithm does not find a matcher and both
terms are patterns, pattern matching is tried.

\ourpara{Indexing} Ehoh, like other modern theorem provers, efficiently retrieves
unifiable or matchable pairs of terms using indexing data structures. To find
terms unifiable with a query term or instances of a query term it uses
\emph{fingerprint indexing} \cite{ss-12-fp-indexing}. We extended this data
structure to support (full) higher-order terms in Zipperposition
\cite[Sect.~6]{unif-section}. We used the same approach in \ehohii{},
and we extended feature vector indices \cite{ss-2013-feature-vector}
in the same way.

Ehoh uses \emph{perfect discrimination trees} \cite{mcc-92-pdts} to find
generalizations of the query term (i.e., terms of which the query term is an instance).
This data structure is a trie
that indexes terms by representing them in a serialized, flattened form.
%\begin{rep}, where each node corresponds to a symbol in a term\end{rep}.
The left branch from the root in Figure \ref{fig:pdt} shows how the first-order terms
$\cst{f}\, \cst{a} \, X$ and $\cst{f}\, \cst{a} \, \cst{a}$ are stored.
In Ehoh, this data structure is extended to support partial application
and applied variables \cite{section-ehoh}.

\begin{figure}[tb]
\centering
\begin{tikzpicture}[level distance=3em, sibling distance=7.5em,
  %  edge from parent path={(\tikzparentnode.south) -- ++(0,-0.5em)
  %   -| (\tikzchildnode.north)},
    every node/.style = {shape=rectangle, rounded corners=.75em, minimum size=1.75em, draw, align=center}]
    \node {\phantom{x}}
      child { node {$\cst{f}$}
        child[sibling distance=2.5em] { node %[fill=verylightgray]
          {$\cst{a}$}
          child {node {$X$}}
          child {node {$\cst{a}$}} }
      }
      child { node {$\cst{g}$}
        child {node {$X \, \cst{a} \, \cst{b}$}
               child {node {$\cst{c}$}}}
      }
      child { node {$\cst{h}$}
        child {node {$\internallam_\iota$}
          child {node {$\internallam_\iota$}
            child {node {$X \, \db{0} \, \db{1}$}}
          }
      }}
      ;
\end{tikzpicture}
\caption{First-order, $\lambda$-free higher-order, and higher-order pattern
  terms in a perfect discrimination tree}
\label{fig:pdt}
\end{figure}

In \ehohii{}, we extended this structure to support $\lambda$-abstractions and
the higher-order pattern matching algorithm.
To this end, we changed the way in which terms are
serialized. First, we require that all terms are fully $\eta$-expanded (except
for arguments of variables applied in patterns). Then, when the term is serialized,
we dedicate a separate node for applied variable terms $X \, \overline{s}_n$,
instead of the node for $X$ followed by nodes for serialization of arguments
$\overline{s}_n$. We serialize $\lambda$-abstraction $\lambda x.\, s$ using
a special node $\internallam_\tau$, where $\tau$ is the type of $x$,
followed by the serialization of $s$. Other than these two changes, serialization
remains as in Ehoh, following the gracefulness principle.
Figure \ref{fig:pdt} shows how $\cst{g} \,
(X\,\cst{a}\,\cst{b}) \, \cst{c}$ and $\cst{h} \, (\lambda x. \, \lambda y. \, X
\, y \, x)$ are serialized.

Since the terms are stored in serialized form, it is hard to manipulate
$\lambda$ prefixes of stored terms during matching. Performing $\eta$-expansion
when serializing terms makes sure that matchable terms have
$\lambda$ prefixes of the same length.

We have dedicated separate nodes for applied variables because access to arguments
of applied variable is necessary for the pattern matching algorithm. Even though
arguments can be obtained by querying the arity $n$ of the variable and taking
next $n$ arguments in the serialization, this is both inefficient and inelegant.
As for De Bruijn indices, we treat them the same as function symbols and assign
them their own nodes.

\newcommand{\tstack}{\ensuremath{\texttt{term\_stack}}}
\newcommand{\tproc}{\ensuremath{\texttt{term\_proc}}}

Following the notation from the extension of perfect discrimination trees to
$\lambda$-free higher-order logic \cite{section-ehoh}, we now describe how enumeration of
generalizations is performed. To traverse the tree, \ehohii{} begins at the root
node and maintains two stacks:\ \tstack{} and \tproc{}, where \tstack{} contains the
subterms of the query term that have to be matched, and \tproc{} contains
processed terms that are used to backtrack to previous states. Initially,
\tstack{} contains the query term, the current matching substitution
$\sigma$ is empty, and the successor node is chosen among the child nodes as
follows:

\begin{enumerate}
\item[A.] If the node is labeled with a symbol $\xi$ (where
  $\xi$ is either a De Bruijn index or a constant) and the top item $t$
  of \tstack{} is of the form $\xi \, \overline{t}_n$, replace $t$ by
  $n$~new items $t_1,\dots,t_n$, and push $t$ onto \tproc{}.
\smallskip
\item[B.] If the node is labeled with a symbol $\internallam_\tau$ and the top
  item $t$ of \tstack{} is of the form $\lambda x.\, s$ and the type of $x$ is
  $\tau$, replace $t$ by $s$, and push $t$ onto \tproc{}.
\smallskip
\item[C.] If the node is labeled with a possibly applied variable $X \, \overline{s}_n$
  (where $n \geq 0$), and the top item of \tstack{} is $t$, the 
  matching algorithm described above is run on $X \, \overline{s}_n$ and $t$.
  The algorithm takes into account $\sigma$ built so far and extends it
  if necessary. If the algorithm succeeds, pop $t$ from \tstack{}, push it onto \tproc{},
  and save the original value of $\sigma$ in the node.
\end{enumerate}
  
Backtracking works in the opposite direction: If the current node is labeled with a De
Bruijn index or function symbol node of arity $n$, pop $n$ terms from \tstack{} and move
top of \tproc{} to \tstack{}. If the node is labeled with
$\internallam_\tau$, pop the top of \tstack{} and move the top of \tproc{} to
\tstack{}. Finally, if the node is labeled with a possibly applied variable, move top of the
\tproc{} to \tstack{} and restore the value of $\sigma$.

As an example of how finding a generalization works, consider the following
states of stacks and substitutions, which emerge when looking for generalizations of 
$\cst{g} \, (\cst{f} \, \cst{a} \, \cst{b}) \, \cst{c}$ in the tree of Figure \ref{fig:pdt}:
%
\[\begin{array}{@{}l@{\kern1em}l@{\kern1em}l@{\kern1em}l@{\kern1em}l@{}}
             & \epsilon                                                & \cst{g}                                                 & \cst{g}.(X \, \cst{a} \, \cst{b})                                                          & \cst{g}.(X \, \cst{a} \, \cst{b}).\cst{c} \\[.5\jot]
  \sigma{:}  & \emptyset                                               & \emptyset                                               & \{X \mapsto \cst{f}\}                                                                      & \{X \mapsto \cst{f}\} \\
  \tstack{:} & [\cst{g} \, (\cst{f} \, \cst{a} \, \cst{b}) \, \cst{c}] & [\cst{f} \, \cst{a} \, \cst{b}, \cst{c}]                & [\cst{c}]                                                                                  & [] \\
  \tproc{:}  & []                                                      & [\cst{g} \, (\cst{f} \, \cst{a} \, \cst{b}) \, \cst{c}] & [\cst{f} \, \cst{a} \, \cst{b}{,}\; \cst{g} \, (\cst{f} \, \cst{a} \, \cst{b}) \, \cst{c}] & [\cst{c}{,}\;\cst{f} \, \cst{a} \, \cst{b}{,}\; \cst{g} \, (\cst{f} \, \cst{a} \, \cst{b}) \, \cst{c}]
  \end{array}\]
%



% \begin{nolinenumbers}%\begin{figure}[h!]

% \end{nolinenumbers}
  

%   ** follow the pseudocode written on the paper at the VU
  
  

% To support multiple unifiers we have changed 

% ** Describe we decided to go for depth-bound approach

% ** Describe the algo

% ** Describe the indexing

\section{Preprocessing, Calculus, and Extensions}

\label{sec:ehoh2:calculus}
% \begin{itemize}  
  % \item The main goal is simplicity, efficiency and maximal compatibility with
  %     FO superposition.
  % \item More or less the same as LFHOL sup, but now computes CSUs instead
  %     of LFHOL MGUs (and (potentially) uses new orders).
  % \item Out of Boolean rules we have Boolean simplification and black-box clausification.
  %     Most importantly -- we have no Boolean hoisting rules : everything is dealt with
  %     ext-sup, and FOOL clausification! It simply works as good as Boolean hoisting.
  % \item IMPORTANT: Prefix optimization is disabled, now we rely on ArgCong and NegExt.
  % \item Many extensions:  ExtSup (With the same indexing), leibniz elimination, predicate enumeration,
  %     FlexElim, Choice instantiation, recognition of induction.
  % \end{itemize}

Ehoh's simple $\lambda$-free higher-order calculus performed well on
Sledgehammer problems and formed a promising stepping stone to full higher-order
logic \cite{section-ehoh}. When implementing support for full higher-order
logic, we were guided by efficiency and gracefulness with respect to Ehoh's calculus rather
than completeness. Whereas Zipperposition provides both complete and incomplete
modes, \ehohii{} only offers incomplete modes.

\ourpara{Preprocessing} Our experience with Zipperposition showed the importance
of flexibility in preprocessing the higher-order problems
\cite{section-making-ho-work}. Therefore, we implemented a flexible
preprocessing module in \ehohii{}. 

To maintain compatibility with Ehoh, \ehohii{} can optionally transform all
$\lambda$-abstractions into named functions. This process is called
\emph{$\lambda$-lifting} \cite{rjmh-82-lifting}. \ehohii{} also removes all
occurrences of Boolean subterms (other than $\itrue, \ifalse$, and free variables)
in higher-order contexts using a FOOL-like transformation \cite{kotelnikov-16-fool}.
For example, the formula $\cst{f}(\cst{p} \iand \cst{q}) \ieq \cst{a}$ becomes
$(\cst{p} \iand \cst{q} \iimplies \cst{f}(\itrue) \ieq \cst{a}) \iand
(\inot(\cst{p} \iand \cst{q}) \iimplies \cst{f}(\ifalse) \ieq \cst{a})$.

\looseness=-1
Many TPTP problems use the \texttt{definition} role to denote the definitions of
symbols. \ehohii{} can treat the definition axioms as rewrite rules, and replace
all occurrences of defined symbols during preprocessing. Furthermore, during
SInE \cite{hv-2011-sine} axiom selection, it can always
include the defined symbol in the trigger relation.

\ourpara{Calculus} \ehohii{} implements the same superposition
calculus as Ehoh with three important changes. First, wherever Ehoh requires
the most general unifier of terms, \ehohii{} enumerates unifiers from a finite subset of the CSU, as explained in
Sect.~\ref{sec:ehoh2:unif-match-index}. Second, \ehohii{} uses versions of the KBO and
LPO orders designed for $\lambda$-terms.
% as described by Bentkamp et al.\ \cite{bbtv-21-full-ho-sup}.

%TODO: @JASMIN: How do we cite the STTT article?
The third difference is more subtle. One of the main features of Ehoh is
\emph{prefix optimization} \cite[Sect.~1]{section-ehoh}: a method which, given a demodulator $s \eq t$, allows
to replace both applied and unapplied occurrences of $s$ by $t$ 
by traversing only first-order subterms of a rewritable term. In $\lambda$-free setting this
optimization is useful, but in presence of $\beta\eta$-normalization, shapes of
terms can change drastically, making it much harder to
track prefixes of terms. This is why we disabled the prefix optimization in \ehohii{}.
To counterbalance this removal, we introduced
the argument congruence rule \infname{AC} %(also called \infname{ArgCong})
in \ehohii{}
and enabled positive and negative functional extensionality (\infname{PE} and
\infname{NE}) by default:

\vskip\abovedisplayskip

\noindent\hbox{}\hfill
  \infer{AC}{s \eq t \llor C}{s \, X \eq t \, X \llor C}%
\hfill
  \infer{NE}{s \noteq t \llor C}{s \; (\cst{sk} \; \overline{X}) \noteq t \; (\cst{sk} \; \overline{X}) \llor C}%
\hfill
  \infer{PE}{s \; X \eq t \; X \llor C}{s \eq t \llor C}%
\hfill\hbox{}

\vskip\belowdisplayskip

\noindent
\infname{AC} and \infname{NE} assume that $s$ and $t$ are of function
type. In \infname{NE}, $\overline{X}$ denote all the free variables occurring in $s$ and
$t$, and $\cst{sk}$ is a fresh Skolem symbol of the appropriate type. \infname{PE}
has a side condition that $X$ may not appear in $s$, $t$, or $C$.

\ourpara{Saturation} E's saturation procedure is based on the assumption that
each attempt to perform an inference will either result in a single clause or
fail due to one of the inference side conditions. Unification procedures that
produce multiple substitutions break this invariant, and the saturation
procedure needed to be adjusted.

For Zipperposition, Vukmirovi\'c et al.\ developed a variant of the saturation
procedure that elegantly supports interleaving computation of unifiers and
scheduling
inferences to be performed \cite{section-making-ho-work}. Since completeness was
not one of the \ehohii{} design goals, we did not implement this version of the
saturation procedure. Instead, in places where previously a single unifier was
expected, \ehohii{} consumes all elements of the iterator used for enumerating a
unifier, converting them to clauses. 

\ourpara{Reasoning about Formulas} Even though most of the Boolean structure is
removed during preprocessing, formulas can reappear at the top level of
clauses during saturation. For example, after replacing $X$ with $\lambda x.
\, \lambda y.\, x \iand y$, the clause $X \, \cst{p} \, \cst{q} \llor \cst{a}
\eq \cst{b}$ becomes $(\cst{p} \iand \cst{q}) \llor \cst{a} \eq \cst{b}$. \ehohii{}
converts every clause of the form $\varphi \llor C$, where $\varphi$ has a logic symbol
as its head (or it is a (dis)equation between two formulas different than $\itrue$),
to an explicitly quantified formula. Then, the clausification algorithm
is invoked on the formula to restore the clausal structure. Zipperposition features
more dynamic clausification modes, but for simplicity we decided
not to implement them in \ehohii{}.

The $\lambda$-superposition calculus for full higher-order logic
\cite{bbtv-21-full-ho-sup} includes many rules that act on Boolean subterms,
which are necessary for completeness. Other than Boolean simplification rules,
which use simple tautologies such as $\cst{p} \iand \itrue \iequiv\cst{p}$ to simplify terms, we
have implemented none of the Boolean rules of this calculus in \ehohii{}. First, we have
observed that complicated rules such as \infname{FluidBoolHoist} and
\infname{FluidLoobHoist} are hardly ever useful in practice and usually only
contribute to an uncontrolled increase in the proof state size. Second, simpler
rules such as \infname{BoolHoist} can usually be simulated by pragmatic rules
that perform functional extensionality reasoning, described below.

To make up for excluding Boolean rules, we use an incomplete, but more
easily controllable and intuitive rule, called \emph{primitive instantiation}. This
rule instantiates applied predicate variables with approximations of formulas
that are ground instances of this variable. We use the approximations described
by Vukmirovi\'c and Nummelin~\cite[Sect.~3.3]{bool-chapter} and
implemented them in a similar manner.

\ehohii{}'s handling of the Hilbert choice operator is inspired by Leo-III's
\cite{sb-21-leo3}. \ehohii{}
recognizes clauses of the form $\neg P \, X \lor P \, (\cst{f} \, P)$ which
essentially denote that $\cst{f}$ is a choice symbol. Then, when subterm
$\cst{f} \, s$ is found during saturation, $s$ is used to instantiate the choice
axiom for $\cst{f}$. 
%
Similarly, Leibniz equality \cite{bool-chapter} is eliminated by
recognizing clauses of the form  $\neg P \, \cst{a} \llor P \, \cst{b} \llor C$. These clauses are then
instantiated with $P \mapsto \lambda x. \, x \eq \cst{a}$ and $P \mapsto \lambda
x. \, x\noteq\cst{b}$, which results in $\cst{a} \eq \cst{b} \lor C$.

Finally, \ehohii{} treats induction axioms specially. Just like
Zipperposition \cite[Sect.~4]{section-making-ho-work}, it abstracts literals
from the goal clauses and uses these abstractions to instantiate induction
axioms. Since Zipperposition supports dynamic calculus-level clausification,
instantiation of induction axioms happens during saturation, when these axioms
are processed. In \ehohii{}, this instantiation is performed statically,
immediately after clausification. After \ehohii{} collects all the abstractions,
it traverses the clauses and instantiates those that have applied variable of
the same type as the abstraction.

\ourpara{Extensionality} \ehohii{} takes a pragmatic approach to reason about
functional and Boolean extensionality: It uses \emph{abstracting} rules
\cite{bbtvw-21-sup-lam} which simulate basic superposition calculus rules,
but do not require unifiability of the partner terms in the inference. More
precisely, assume a core inference needs to be performed between two
$\beta$-reduced terms $u$ and $v$, such that they can be represented as
$u=C[s_1, \ldots, s_n]$ and $v=C[t_1, \ldots, t_n]$, where $C$ is the most general
(green \cite{bbtvw-21-sup-lam}) common context of $u$ and $v$, not all of
$s_i$ and $t_j$ are free variables, and for at least one $i$, $s_i \not= t_i$,
$s_i$ and $t_i$ are not (applied) free variables, and they are of Boolean or
function type. Then, the conclusion is formed by taking the conclusion $D$ of
the core inference rule (which would be created if $s$ and $t$ are unifiable)
and adding literals $s_1 \noteq t_1 \llor \cdots \llor s_n \noteq t_n$.

These rules are particularly useful because \ehohii{} has no rules that dynamically
process Booleans in FOOL-like fashion, such as \infname{BoolHoist}. For example,
given the clauses $\cst{f} \, (\cst{p} \iand \cst{q}) \eq \cst{a}$ and
$\cst{g} \, (\cst{f} \, \cst{p}) \noteq \cst{b}$, the abstracting version of
superposition rule would result in $\cst{g} \, \cst{a} \noteq \cst{b} \llor (\cst{p}
\iand \cst{q}) \noteq \cst{p}$. In this way, the Boolean structure bubbles up to
the top level and is further processed by clausification. We noticed that this
aleviates the need for the other Boolean rules in practice.


\section{Evaluation}
\label{sec:ehoh2:eval}

In this section, we try to answer two questions about \ehohii{}: \emph{How does
\ehohii{} compare against other higher-order provers (including Ehoh)?} \emph{Does
\ehohii{} introduce any overhead compared with Ehoh?} To answer
these questions, we ran provers on problems from the TPTP library \cite{gs-17-tptp} and on
benchmarks generated by Sledgehammer (SH) \cite{pb-12-sh}.
The experiments were carried out on StarExec Miami
\cite{sst-14-starexec} nodes equipped with Intel Xeon E5-2620 v4 CPU clocked at
2.10~GHz. For the TPTP part, we used the CASC-28\footnote{\url{http://www.tptp.org/CASC/28/}} time limits:\ 120~s
wallclock and 960~s CPU. For SH and to answer the other question, we used Sledgehammer's
default time limit:\ 30~s wallclock and CPU.

\ourpara{Comparison with Other Provers}
To answer the first question, we let
\ehohii{} compete with the top contenders in the higher-order division of
CASC-28:\ cvc5 \NumberOK{0.0.7},\footnote{\url{https://cvc5.github.io/}}
Ehoh 2.7
\cite{section-ehoh}, Leo-III \NumberOK{1.6.6} \cite{sb-21-leo3}, Vampire 4.6
\cite{br-20-full-sup-w-combs}, and Zipperposition 2.1
\cite{section-making-ho-work}. We also include Satallax 3.5
\cite{cb-12-satallax}. We used all \NumberOK{2899} higher-order
theorems in TPTP 7.5.0 as well as 5000 SH higher-order benchmarks
originating from the Seventeen benchmark suite \cite{desharnais-et-al-202x}.
On SH benchmarks,
cvc5, Ehoh, \ehohii{}, Vampire, and Zipperposition were run using custom schedules provided by
their developers, optimized for single-core usage and low timeouts.
Otherwise, we used the corresponding CASC configurations.
Note that Ehoh 2.7 can parse not only $\lambda$-free but also full higher-order logic,
which is supported through $\lambda$-lifting.
We included two versions of Zipperposition:\ \emph{coop} uses Ehoh 2.7 as a backend to finish proof attempts,
whereas \emph{uncoop} does not use this feature. The results
are shown in Figure \ref{fig:all-provers}.

Both Ehoh and \ehohii{} were run in the automatic scheduling mode. Compared to Ehoh,
\ehohii{} features a redesigned module for automatic scheduling, it
can use multiple CPU cores, and its heuristics have been trained better on higher-order problems.

\ehohii{} dramatically improves the higher-order reasoning capabilities
compared to Ehoh. It solves \NumberOK{20\%} more problems on TPTP benchmarks
and \NumberOK{6\%} more problems on SH benchmarks, where Ehoh was already very
successful.

\ehohii{} was mainly designed as efficient backed to proof assistants. As such it 
excels on SH benchmarks. On TPTP, it outperforms all higher-order provers other than Zipperposition-coop.
If Zipperposition's Ehoh backend is disabled,
\ehohii{} outperforms Zipperposition by a wide margin.
This comparison is arguably fairer; after all,
\ehohii{} does not use an older version of Zipperposition
as a backend.
%
These results suggest that \ehohii{} already implements most of the necessary features
for a high-performance higher-order prover
but could benefit from the kind of fine-tuning that
Zipperposition underwent in the last two years.

Remarkably, there is a substantial disparity in the set of problems solved by
\ehohii{} and Zipperposition-coop. The raw data show thats \ehohii{} solves
\NumberNOK{166} SH problems and \NumberNOK{21} TPTP problems that Zipperposition-coop
does not. The lower number of uniquely solved TPTP problems is most likely 
due to Zipperposition's being heavily optimized on TPTP.

\ourpara{Comparison with the First-Order E} Both Ehoh and \ehohii{} can be compiled
in a mode that disables most of the higher-order reasoning. This mode is
designed for users that are interested only in E's first-order capabilities and
care a lot about performance.
To answer the second
evaluation question, about assessing overhead of \ehohii{},
we chose all the \NumberNOK{1138} unique problems used at CASC
from 2019 to 2021 in first-order theorem division and ran Ehoh and \ehohii{}
both in this first-order (FO) mode and in higher-order (HO) mode.

We fixed a single
configuration of options, because Ehoh and \ehohii{}'s
automatic scheduling methods could select different configurations
and we would not be measuring the overhead but the quality of the chosen
configurations. We chose the \emph{boa} configuration
\cite[Sect.~7]{section-ehoh},
which is the configuration that E~2.2 used the most often in its automatic scheduling mode.
The results are shown in Figure~\ref{fig:ehoh}.

Counterintuitively, the higher-order versions of both provers outperform
the first-order counterparts. However, the differerence is so small that it can
be attributed to the changes to memory layout that affect the order in which
clauses are chosen. Similar effects are seen when comparing the first-order versions.
We would expected the less heavily modified version of E, Ehoh, to perform better,
but due to subtle effects \ehohii{} wins.


\begin{figure}[t]
  \noindent\hbox{}\hfill
  \begin{minipage}[ht!]{.66\linewidth}
    \centering
    \def\arraystretch{1.1}%
    \relax{\begin{tabular}{l@{\hskip 1em}@{\hskip 0.5em}c@{\hskip 1em}c} \toprule
             
                            & TPTP       & SH        \\
      \midrule
      cvc5                  & 1931       & 2577      \\
      Ehoh                  & 2105       & 2611      \\
      \ehohii{}             & 2523       & {\bf 2766}\\
      Leo-III               & 2282       & 1601      \\
      Satallax              & 2320       & 1719      \\
      Vampire               & 2203       & 2240      \\
      Zipperposition-coop   & {\bf 2583} & 2754      \\
      Zipperposition-uncoop & 2483       & 2181      \\
      \bottomrule
    \end{tabular}}
    \caption{Comparison of higher-order provers}
    \label{fig:all-provers}
  \end{minipage}\hfill\hfill
  \begin{minipage}[ht!]{.30\linewidth}
    \centering
    \def\arraystretch{1.1}%
    \relax{\begin{tabular}{l@{\hskip 1em}@{\hskip 0.5em}c} \toprule
                      & TPTP     \\
      \midrule
      Ehoh -- FO      & 535      \\
      Ehoh -- HO      & 538      \\
      \ehohii{} -- FO & {\bf 539}\\
      \ehohii{} -- HO & {\bf 539}\\
      \bottomrule
    \end{tabular}}
    \caption{Evaluation of \ehohii{}'s overhead}
    \label{fig:ehoh}
  \end{minipage}\hfill\hfill
\end{figure}


\section{Discussion and Related Work} %---0.5p
\label{sec:ehoh2:discussion}

On the trajectory to \ehohii{}, we developed, together with colleagues, three
superposition calculi:\ for $\lambda$-free higher-order logic
\cite{bbcw-21-lfho}, for a higher-order logic with $\lambda$-abstraction
but no Booleans \cite{bbtvw-21-sup-lam}, and for full higher-order logic
\cite{bbtvw-21-sup-lam}. These milestones allowed us to carefully
estimate how the increased reasoning capabilities of each calculus influence its
performance.

Extending first-order provers with higher-order reasoning capabilities has been
attempted by other researchers as well.
Barbosa et al.\ extended
the SMT solvers CVC4 (now cvc5) and veriT to higher-order logic in an incomplete
way \cite{brotb-19-ho-smt}.
Bhayat and Reger first extended Vampire to
higher-order logic using combinatory unification \cite{br-20-full-sup-w-combs},
an incomplete approach, before they designed and implemented a complete
higher-order superposition calculus based on \textsf{SKBCI}-com\-binators \cite{br-19-comb-unif}.
The advantage is that combinators can be supported as a thin layer on top of
$\lambda$-free terms.
This calculus is also implemented in Zipperposition.
However, in unpublished experiments, we found that $\lambda$-superposition
performs substantially better, corroborating the CASC results, so we decided to
make a more profound change to Ehoh and implement $\lambda$-superposition.

Possibly the only actively maintained higher-order prover built from the bottom
up as a higher-order prover is Leo-III \cite{sb-21-leo3}, as the long-time
winner of CASC higher-order division, Satallax \cite{cb-12-satallax}, is no
longer maintained. We give a further overview of other traditional higher-order
provers and the calculi they are based in the paper about Ehoh
\cite[Sect.~9]{section-ehoh}.

\section{Conclusion} %---0.5p
\label{sec:ehoh2:conclusion}

In 2019, the reviewers of our paper introducing Ehoh \cite{section-ehoh} were
skeptical that extending it with support for full higher-order logic would be
feasible. One of them wrote:
%
\begin{quote}
A potential criticism could be that this step from E to Ehoh is just extending
FOL by those aspects of HOL that are easily in reach with rather straightforward
extensions (none of the extensions is indeed very complicated), and that the
difficult challenges of fully supporting HOL have yet to be confronted.
\end{quote}
%
We ended up addressing the theoretical ``difficult challenges'' in other work
with colleagues. In this chapter, we faced the practical challenges pertaining to the extension
of Ehoh's data structures and algorithms to support full higher-order logic and
demonstrated that such an extension is possible. \NumberOK{Our evaluation} shows
that this extension makes \ehohii{} the best higher-order prover on
benchmarks coming from interactive theorem proving practice, which has been our goal since 2017. \ehohii{}'s
performance lags slightly behind Zipperposition on TPTP problems, possibly because Zipperposition
does not assume a clausal structure and can perform subtle formula-level
inferences. In the future, we plan to implement the same features in \ehohii{}.
We have also only started tuning \ehohii{}'s heuristics on higher-order
problems.
%%% @PETAR(read): By submission time, CASC will already have taken place!
% We plan to train the prover better. ahead of the next CASC competition.

\let\Acksize=\footnotesize

\def\ackname{\Acksize Acknowledgment}

\ourpara{\ackname}
{\Acksize
Ahmed Bhayat and Martin Suda provided us
with Sledgehammer-optimized configurations for Vampire. Andrew Reynolds did the
same for cvc5. We thank all of them.
%
This research has received funding from the European Research Council (ERC)
under the European Union's Horizon 2020 research and innovation program (grant
agreement No.\ 713999, Matryoshka).
%
Blanchette has received funding from the Netherlands Organization for
Scientific Research (NWO) under the Vidi program (project No.\
016.Vidi.189.037, Lean Forward).
}

\bibliographystyle{splncs03}
\bibliography{bib}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "conf"
%%% End:
