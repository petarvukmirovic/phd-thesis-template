%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required

%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
\documentclass[smallcondensed,draft]{svjour3}     % onecolumn (ditto)
%\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage[utf8]{inputenc}
\usepackage[misc]{ifsym}
\usepackage[final]{graphicx}
\usepackage{array}
\usepackage{multirow}
\usepackage{cite}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{prftree}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumerate}
\usepackage{stmaryrd}
\usepackage{booktabs}
\usepackage{algorithmicx}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage[noend]{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage[
   a4paper,
   pdftex,
   pdftitle={Making Higher-Order Superposition Work},
   pdfauthor={Petar Vukmirovi\'c, Alexander Bentkamp, Jasmin Blanchette, Simon Cruanes, Visa Nummelin, and Sophie Tourret},
   pdfkeywords={},
   pdfborder={0 0 0},
   draft=false,
   bookmarksnumbered,
   bookmarks,
   bookmarksdepth=2,
   bookmarksopenlevel=2,
   bookmarksopen]{hyperref}

\usepackage{mathptmx}      % use Times fonts if available on your TeX system

\DeclareSymbolFont{letters}{OML}{txmi}{m}{it} %% for Greek

% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
\journalname{Journal of Automated Reasoning}

\makeatletter

\def\vthinspace{\kern+0.083333em}
\newcommand\CORR{{(\vthinspace\Letter\vthinspace)}}

\def\orcid#1{{\href{http://orcid.org/#1}{\protect\raisebox{-1.25pt}{\protect\includegraphics{orcid.pdf}}}}}

% mathcal
\DeclareFontFamily{OT1}{pzc}{}
\DeclareFontShape{OT1}{pzc}{m}{it}{<-> s * [1.10] pzcmi7t}{}
\DeclareMathAlphabet{\mathcalx}{OT1}{pzc}{m}{it}
%\DeclareMathAlphabet{\mathcalx}{OMS}{zplm}{m}{n}

%%% HACK Springer's style to get bold subsection headings --JB
\def\subsection{\@startsection{subsection}{2}{\z@}%
    {-21dd plus-8pt minus-4pt}{10.5dd}
     {\normalsize\bfseries\boldmath}}

\spnewtheorem{definition}[theorem]{Definition}{\bfseries}{\rmfamily}
\spnewtheorem{exa}[theorem]{Example}{\bfseries}{\rmfamily}


\begin{document}

\title{Making Higher-Order Superposition Work}
% \subtitle{Do you have a subtitle?\\ If so, write it here}

%\titlerunning{Short form of title}        % if too long for running head

\author{Petar~Vukmirović~\orcid{0000-0001-7049-6847}\and
Alexander~Bentkamp~\orcid{0000-0002-7158-3595}\and
Jasmin~Blanchette~\orcid{0000-0002-8367-0936}\and
Simon~Cruanes~\orcid{0000-0003-3969-5850}\and
Visa~Nummelin~\orcid{0000-0003-0078-790X} \and
Sophie~Tourret~\orcid{0000-0002-6070-796X}}

\authorrunning{Petar Vukmirović et al.} % if too long for running head

\institute{
  Petar~Vukmirovi\'c \CORR \and
  Alexander Bentkamp\and
  Jasmin Blanchette\and
  Visa Nummelin\at%
    Vrije Universiteit Amsterdam, Amsterdam, the Netherlands\\
    \email{\{p.vukmirovic,a.bentkamp,j.c.blanchette,visa.nummelin\}@vu.nl}
  \and
  Jasmin Blanchette\and Sophie Tourret \at%
    Universit\'e de Lorraine, CNRS, Inria, LORIA, Nancy, France
    \\
    \email{\{jasmin.blanchette,sophie.tourret\}@inria.fr}
  \and
  Jasmin Blanchette\and Sophie Tourret \at  Max-Planck-Institut f\"ur Informatik, Saarbr\"ucken,~Germany \\
  \email{\{jblanche,stourret\}@mpi-inf.mpg.de}
  \and
  Simon Cruanes \at Imandra, Austin, Texas, USA \\
  \email{simon@imandra.ai}
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
Superposition is among the most successful calculi for first-order logic. Its
extension to higher-order logic introduces new challenges such as infinitely
branching inference rules, new possibilities such as reasoning about
Booleans, and the need to curb the explosion of specific higher-order rules. We
describe techniques that address these issues and extensively evaluate their
implementation in the Zipperposition theorem prover. Largely thanks to their use,
Zipperposition won the higher-order division of the CASC-J10 competition.
\end{abstract}

\newcommand{\ifalse}{\pmb\bot}
\newcommand{\itrue}{\pmb\top}
\newcommand{\inot}{\pmb{\neg\,}}
\newcommand{\iand}{\pmb\land}
\newcommand{\ior}{\pmb\lor}
\newcommand{\iimplies}{\pmb\rightarrow}
\newcommand{\iequiv}{\pmb\leftrightarrow}
\newcommand{\iforall}{\pmb\forall}
\newcommand{\iexists}{\pmb\exists}
\newcommand{\ieq}{\pmb\approx}
\newcommand{\ineq}{\pmb{\not\approx}}

\newcommand{\comb}[1]{\ensuremath{\mathsf{#1}}}
\newcommand{\neglit}[1]{\ensuremath{#1 \eq \ifalse}}
\newcommand{\poslit}[1]{\ensuremath{#1 \eq \itrue}}
\newcommand{\skbci}{\textsf{SKBCI}}
\def\negvthinspace{\kern-0.083333em}


\newcommand{\cst}[1]{\ensuremath{\mathsf{#1}}}
\newcommand{\var}[1]{{\mathit{#1}}}
\newcommand{\typ}[1]{{\mathit{#1}}}
\newcommand\foralltynospace[1]{\mathsf{\Pi}#1.}
\newcommand\forallty[1]{\foralltynospace{#1}\;}
\newcommand{\typeargs}[1]{{\negvthinspace\langle#1\rangle\negvthinspace}}
\let\oldDelta=\Delta
\let\oldSigma=\Sigma
\renewcommand\Sigma{\mathrm{\oldSigma}}
\newcommand{\Sigmaty}{\Sigma_\mathsf{ty}}
\newcommand{\VV}{\mathscr{V}}
\newcommand{\Vty}{\VV_\mathsf{ty}}


% Equality predicate
\newcommand{\eq}{\approx}
\newcommand{\noteq}{\not\eq}
\newcommand{\lland}{\mathrel\land}
\newcommand{\llor}{\mathrel\lor}
\newcommand{\ccup}{\mathrel\cup}
\newcommand{\ccap}{\mathrel\cap}

\newcommand{\tuple}[2]{\overline{#1}_{#2}}
\newcommand{\tuplen}[1]{\overline{#1}_{n}}

\newcommand{\lam}[2]{\ensuremath{\lambda #1.\> #2}}
\newcommand{\lamx}[1]{\lam{x}{#1}}

% Names of inference rules
\newcommand{\infname}[1]{\textsc{#1}}

% Inference rule
\newcommand{\namedinference}[3]{\prftree[r]{\relax{\infname{#1}}}{\strut#2}{\strut#3}}
\newcommand{\inference}[2]{\namedinference{}{\strut#1}{\strut#2}}

% Simplification rule
\newcommand{\namedsimp}[3]{\prftree[d][r]{\relax{\infname{#1}}}{\strut#2}{\strut#3}}
\newcommand{\simp}[2]{\namedinference{}{\strut#1}{\strut#2}}
\newcommand{\lsupraw}{$\lambda$-super\-po\-si\-tion}
\newcommand{\lsup}{{\color{black}\lsupraw}}
\newcommand{\lsup}{{\color{black}Boolean-free \lsupraw}}
\newcommand{\osup}{{\color{black}Boolean \lsupraw}}

\newcommand\NumberOK[1]{#1}
\newcommand\NumberNOK[1]{\colorbox{red}{#1}}
\newcommand{\ourpara}[1]{\paragraph{\upshape\bfseries#1}}

\hyphenation{su-per-po-si-tion Zip-per-posi-tion tab-leau tab-leaux Bent-kamp ab-strac-tion
  Bool-ean}

\newcommand{\confrep}[2]{#2}

\section{Introduction}
\label{sec:satfol:ho-tech:intro}

%In recent decades,
Superposition-based first-order automatic theorem provers
have emerged as useful reasoning tools. They dominate at the annual CASC
\cite{gs-2016-casc} theorem prover competition, having always won the
first-order theorem division. They are also used as backends to proof assistants
\cite{ck-18-coqhammer,ku-15-holyhammer,pb-12-sh}, automatic
higher-order theorem provers \cite{sb-21-leo3}, and software verifiers
\cite{fp-13-why3}.

The superposition calculus has only recently been extended
to higher-order logic (more precisely, extensional simple type theory
\cite{henkin-1950-completeness}), resulting in
\emph{\lsup} \cite{bentkamp-et-al-2021-lamsup-journal}, which we developed
together with Waldmann, as well as \emph{combinatory superposition}
\cite{br-20-full-sup-w-combs} by Bhayat and Reger. Although these two
calculi do not support an interpreted Boolean type,
they can be extended by ad hoc rules \cite{our-bool-paper} that support
most of the Boolean reasoning necessary in practice.

Both higher-order superposition calculi were designed to gracefully
extend first-order reasoning. As most steps in higher-order
proofs tend to be essentially first-order, extending the most successful first-order
calculus to higher-order logic seemed worth trying.
Our first attempt at testing this idea was in 2019:
Zipperposition~1.5, based on \lsup, finished third
in the higher-order theorem division of CASC-27 \cite{gs-19-casc27},
12~percentage points behind the winner, the tableau prover Satallax 3.4 \cite{cb-2013-satallax}.

Studying the competition results, we found that higher-order tableaux have
some advantages over higher-order superposition. To bridge the gap, we developed
techniques and heuristics that simulate tableaux in the
context of saturation. We implemented them in Zipperposition~2, which took part
in CASC-J10 \cite{gs-21-cascj10} in 2020. This time, our prover won the division, proving 84\% of
the problems, a whole 20~percentage points ahead of the runner-up, Satallax
3.4.

In this article, we describe the main techniques that explain this reversal
of fortunes. They cover most parts of a modern higher-order theorem prover, from
preprocessing to additional calculus rules to heuristics to backend
integration. We use a newer version of Zipperposition, based on a newer
calculus:\ Instead of \lsup{} augmented with ad hoc Boolean rules,
we work with \emph{\osup} \cite{bbtv-21-full-ho-sup}, a principled extension of superposition to full
higher-order logic, including an interpreted Boolean type.

Many higher-order problems extensively use symbol definitions to simplify
their representation. We describe several ways to exploit the definitions,
%of which the most successful is
such as turning them into rewrite rules (Sect.~\ref{sec:satfol:ho-tech:preprocessing}).
%Interesting patterns can be observed in various higher-order problem encodings.
%We show how we can exploit these to simplify problems (Sect.~\ref{sec:satfol:ho-tech:preprocessing}).
%
By working on formulas rather than clauses, tableau techniques take a more
holistic view of a higher-order problem.
Through its support for delayed clausification and, more generally,
calculus-level formula manipulation, \osup{} enables us to
simulate most successful tableau techniques in a saturating prover
(Sect.~\ref{sec:satfol:ho-tech:formulas}). This calculus also supports \emph{Boolean selection
functions}, a mechanism that allows us to choose on which Boolean subterms
to perform inferences first.
We implemented some Boolean selection functions and
evaluated them (Sect.~\ref{sec:satfol:ho-tech:bool-select}).

The main drawback of both \lsup{} variants compared with combinatory
superposition is that they rely on rules that enumerate possibly infinite sets
of unifiers. We describe a mechanism that interleaves infinitely
branching inferences with the standard saturation process
(Sect.~\ref{sec:satfol:ho-tech:infinite-branching}). The prover
retains the same behavior as
before on first-order problems, smoothly scaling with
increasing numbers of higher-order clauses.
%
We also propose some heuristics to curb the explosion induced by highly
prolific calculus rules (Sect.~\ref{sec:satfol:ho-tech:explosiveness}).

Using first-order backends to finish the proof is common practice in
higher-order reasoning. Since \lsup{} coincides with standard
superposition on first-order clauses, invoking backends may
seem redundant; yet Zipperposition is nowhere as efficient as E
\cite{scv-19-e23} or Vampire \cite{lkav-13-vampire}, so invoking a more
efficient backend does make sense. We describe how to achieve a balance
between allowing native higher-order reasoning and
delegating reasoning to a backend (Sect.~\ref{sec:satfol:ho-tech:backends}).
%
Finally, we compare Zipperposition~2 with other provers on all monomorphic
higher-order TPTP benchmarks \cite{gs-17-tptp} to perform a more extensive
evaluation than at CASC (Sect.~\ref{sec:satfol:ho-tech:comparison}). Our evaluation
corroborates the competition results.

This article is an extended version of a paper accepted at CADE-28
\cite{making-ho-work}. Compared with the conference paper, it
describes a new preprocessing technique, explores the effects of Boolean
selection functions, evaluates %some previously unevaluated
more techniques,
introduces new benchmark sets, and presents more examples.

\section{Background and Setting}
\label{sec:satfol:ho-tech:background}

We focus on monomorphic higher-order logic, without the axiom of infinity 
or the axiom of at least two individuals. However, the techniques can easily be
extended with polymorphism. Indeed, Zipperposition already supports some
of them polymorphically.

\ourpara{Higher-Order Logic}
We define terms $s, t, u, v$ inductively as free variables $F, X$, bound
variables $x, y, z, \dotsc$, constants $\cst{f}, \cst{g},\allowbreak
\cst{a},\allowbreak \cst{b}, \dotsc$, term applications $s \, t$, and
$\lambda$-abstractions $\lamx{s}$. The syntactic distinction between free and
bound variables yields \emph{loose bound variables} (e.g., $y$ in $\lamx{y \,
\cst{a}}$) \cite{tn-93-patterns}.
%We shorten iterated application $s \, t_1 \, \cdots \, t_n$ to $s \,
%\overline{t}_n$ and $\lambda$-abstraction $\lambda x_1. \, \cdots \, \lambda
%x_n. \> s$ to $\lam{\overline{x}_n}{s}$.
We let $s \, \overline{t}_n$ stand for $s \, t_1 \, \ldots \, t_n$ and
$\lam{\overline{x}_n}{s}$ for $\lambda x_1. \ldots \lambda x_n. \> s$. The $n$-fold application of
a unary term $s$ to a term $t$ is denoted by $s^n \, t$. Every
$\beta$-normal term can be written as $\lam{\overline{x}_m}{s \,
\overline{t}_n}$, where $s$ is not an application; we call $s$ the \emph{head}
of the term. If the type of a term $t$ is of the form $\tau_1 \to \cdots \to
\tau_n \to o$, where $o$ is the distinguished Boolean type and $n \ge 0$, we
call $t$ a \emph{predicate}. A term of type $o$ is called a \emph{formula}.

A literal $l$ is an equation $s \eq t$ or a disequation $s \not\eq t$. A clause is
a finite multiset of literals, interpreted and written disjunctively $l_1 \llor
\cdots \llor l_n$. Logical symbols that may occur within terms are written in
boldface: $\pmb{\neg}, \iand, \ior, \iimplies, \iequiv, \dots$. Quantified
formulas are expressed using (a type-indexed
family of) constants $\iforall$ and $\iexists$ as $\iforall \,
(\lambda x. \, t)$ and $\iexists \, (\lambda x. \, t)$, usually abbreviated to
$\iforall x.\,t$ and $\iexists x.\,t$. Following %the convention of
\osup{}, predicate literals are encoded as equations with $\itrue$ or
$\ifalse$: for example, $\cst{even}(x)$ becomes $\poslit{\cst{even}(x)}$, and
$\neg\,\cst{even}(x)$ becomes $\neglit{\cst{even}(x)}$.

\ourpara{Higher-Order Calculi}
\looseness=-1
The \osup{} calculus \cite{bbtv-21-full-ho-sup} is a refutationally
complete inference system and redundancy criterion for higher-order logic with
rank-1 polymorphism, Hilbert choice, and functional and Boolean extensionality.
The calculus relies on
\emph{complete sets of unifiers}
(\emph{CSUs}). The CSU for $s$ and $t$ with respect to a finite set of variables
$V$, denoted by $\mathrm{CSU}_V(s,t)$, is a set of unifiers of $s$~and~$t$ such
that for any unifier $\varrho$ of $s$~and~$t$, there exist substitutions $\sigma
\in \mathrm{CSU}_V(s,t)$ and $\theta$ such that $\varrho(X) = \sigma(\theta(X))$
for all variables $X \in V$. The set $V$ is used to distinguish
important variables from auxiliary variables (which may arise in intermediary
states of the unification procedure). We usually omit it.

Unlike \lsup, this calculus 
does not require axioms defining the logical symbols to cope with formulas.
Instead, it includes Boolean inference rules that mimic
superposition from such axioms into Boolean subterms,
while avoiding the explosion incurred by adding these axioms to the proof state. It
also includes rules that simulate Boolean inferences below applied variables.
Both sets of rules are disabled or replaced with incomplete, ad hoc rules
described by Vukmirović and Nummelin \cite{our-bool-paper} in most configurations
of the CASC portfolio.
A new feature of the calculus that we explore in detail is
the ability to select Boolean subterms
to restrict Boolean and superposition inferences.

In contrast to both \lsup{} variants, combinatory superposition can
avoid enumerating CSUs by
using a form of first-order unification.
Essentially, it enumerates higher-order terms
using rules that instantiate applied variables with partially applied
combinators from the complete combinator set $\{\cst{S}, \cst{K}, \cst{B},
\cst{C}, \cst{I}\}$. This calculus is the basis of Vampire~4.5
\cite{br-20-full-sup-w-combs}, which finished
closely behind Satallax 3.4 %~and~3.5
at CASC-J10.

\looseness=-1
A different, very successful calculus is Satallax's SAT-guided tableaux
\cite{backes-brown-2011}. Satallax was the leading higher-order prover of the
2010s. Its simple and elegant tableaux avoid deep superposition-style rewriting
inferences.
Nevertheless, our working hypothesis for the past six years has been
that superposition would likely provide a stronger basis for higher-order
reasoning.
Other competing higher-order calculi include SMT (implemented in CVC4
\cite{brotb-19-ho-smt, cbetal-11-cvc4}) and extensional paramodulation (implemented in Leo-III \cite{sb-21-leo3}).


\ourpara{Zipperposition}
Zipperposition \cite{sc-15-simon-phd,bentkamp-et-al-2021-lamsup-journal} is a higher-order
theorem prover that implements both \lsup{} variants, combinatory
superposition, and other superposition-like calculi.
The prover was conceived as a testbed for rapidly
experimenting with extensions of first-order superposition, but over time it
has assimilated many of E's techniques and heuristics and become quite powerful.

Several of our techniques extend the \emph{given clause procedure}, the standard
saturation procedure pioneered by McCune and Wos \cite[Sect.~2.3]{mcw-1997-otter}. It partitions the proof
state into a set $P$ of \emph{passive} clauses and a set $A$ of \emph{active}
clauses. Initially, $P$ contains all input clauses, and $A$ is empty. At each
iteration, a \emph{given} clause is moved from $P$ to $A$ (i.e., it is
\emph{activated}), all inferences between it and clauses in $A$ are performed,
and the conclusions are added to $P$. Because Zipperposition fully simplifies
clauses only when they are activated, it implements a DISCOUNT-style loop
\cite{adf-1995-discount}.

\ourpara{Experimental Setup}
To assess our techniques, we carried out experiments with Zipperposition~2. We
used two sets of benchmarks:\ all 2851~monomorphic higher-order problems from the
TPTP library \cite{gs-17-tptp} version~7.4.0 (labeled \emph{TPTP})
and 1253 Sledgehammer-generated
monomorphic higher-order problems (labeled \emph{SH}).
Although some techniques support polymorphism, we
uniformly used monomorphic benchmarks.

We fixed a \emph{base} configuration
of Zipperposition parameters as a baseline for all comparisons. This is an
incomplete, pragmatic configuration of \osup{} using heuristics expected to perform
well on a wide range of problems.
%Note that
%we use a different baseline configuration than in our earlier paper \cite{making-ho-work}.
%%% That goes without saying. We use a different calculus! Now clarified in intro. --JB
In each experiment, we varied
the parameters associated with a specific technique to evaluate it. The
experiments were run on StarExec Miami \cite{sst-14-starexec} servers, equipped with
Intel Xeon E5-2620 v4 CPUs clocked at 2.10 GHz. Unless otherwise stated, we used a
CPU time limit of 15~s, roughly the time each configuration is given in the
CASC portfolio mode. The raw evaluation results are available online.%
\footnote{\url{http://doi.org/10.5281/zenodo.5007440}}


\section{Preprocessing Higher-Order Problems}
\label{sec:satfol:ho-tech:preprocessing}

The TPTP library contains thousands of higher-order problems. Despite their
diversity, they have a markedly different flavor from the TPTP first-order
problems. Notably, they extensively use the \verb|definition| role to identify
universally quantified equations (and equivalences) that define symbols.
%
Definitions $s \eq t$ (or $(s \iequiv t) \eq \itrue$) can be replaced by rewrite
rules $s \rightarrow t$,
using the orientation given in the input problem. If there are multiple
definitions for the same symbol, only the first one is replaced by a rewrite rule.
Then, whenever a clause is picked in the given clause procedure, it will be rewritten
using the collected rules.
Alternatively, we can rewrite
the input formulas as a preprocessing step. This ensures that the input
clauses will be fully simplified when the proving process starts and no
defined symbols will occur in clauses, which usually helps the heuristics.

Since the TPTP format enforces no constraints on
definitions, rewriting might diverge. To ensure
termination, we limit the number of applied rewrite steps. In
practice, most TPTP problems are well behaved: Only one
definition is given for each symbol, and the definitions are acyclic.

Turning the defining equations into rewrite rules, unfolding the definitions, and
$\beta$-reduc\-ing the result can eliminate all of a problem's higher-order features, making
it amenable to first-order methods. However, this can inflate the problem
beyond recognition and compromise the refutational completeness of
superposition.

\begin{exa} 
  Removing higher-order features of a problem can have adverse effects.
  Consider the TPTP problem \texttt{NUM636\^{}3}, which defines the predicate $\cst{m}$
  as $\lambda x.\, \cst{s} \, x \ineq x$ and states its conjecture as $\iforall
  x.\, \cst{m} \, x $, where $\cst{s}$ is the standard Peano-style natural number
  successor constructor. When this definition is kept as is, the
  prover can superpose from either $\cst{m}$ or its definition into the
  (clausified) induction axiom, which is also given in the problem, and quickly prove
  the conjecture, without using any advanced inductive reasoning. In contrast,
  when the definition is
  unfolded and the problem is $\beta$-reduced, both $\cst{m}$ and the
  corresponding $\lambda$-abstraction disappear, forcing the prover to guess the
  correct instantiation for the induction axiom.
\end{exa}

We describe two techniques to mitigate these issues. The first one is based on the observation that in practice,
the explosion associated with definition unfolding mostly
manifests itself on definitions of nonpredicate symbols. In some cases, it is
preferable to rely on superposition's term order and powerful simplification
engine to rewrite the proof state rather than to blindly rewrite definitions. On
the other hand, superposition's reasoning with equivalences is often inadequate
\cite{bbtv-21-full-ho-sup, gs-05-boolsup}. Thus, it makes sense to treat only
predicate definitions as rewrite rules.

The second technique aims at preserving completeness: We can try to force the term order that
parameterizes superposition to orient as many definitions as possible and rely on
demodulation to simplify the proof state. Usually, the Knuth--Bendix order (KBO)
\cite{db-1970-kbo} is used. It compares terms by first comparing their weights,
which is the sum of all the weights assigned to the symbols it contains. Given a
symbol weight assignment $\mathcal{W}$, we can update it so that it orients
acyclic definitions from left to right assuming that they are of the form $
\cst{f} \, \overline{X}_m \eq \lambda \overline{Y}_n. \, t$, where the only free
variables in $t$ are $\overline{X}_m$, no free variable is repeated or appears
applied in $t$, and $\cst{f}$ does not occur in $t$. Then we traverse the
symbols $\cst{f}$ that are defined by such equations following the dependency
relation, starting with a symbol $\cst{f}$ that does not depend on any other
defined symbol. For each $\cst{f}$, we set $\mathcal{W}(\cst{f})$ to $w + 1$,
where $w$ is the maximum weight of the right-hand sides of $\cst{f}$'s
definitions, computed using $\mathcal{W}$. By construction, for each equation
the left-hand side is heavier. Thus, the equations are orientable from left to
right.



\begin{exa} 
  Many of the problems in the TPTP library's \verb|LCL| category encode modal logic
  in higher-order logic. More complex modal operators (such as
  implication and equivalence) are defined in terms of basic connectives (such as negation
  and disjunction). Some of the definitions present in the problems are
  $\cst{mnot} := \lambda p\, x. \, \inot \, p \, x$, $\cst{mor} := \lambda p\, q\, x.
  \, p \, x \ior q \, x$,  and $\cst{mimplies} = \lambda p\, q. \allowbreak\, \cst{mor} \,
  (\cst{mnot} \, p) \, q$. Assuming that the weight of $\lambda$, bound
  variables, and basic connectives is 2, we can orient equations using
  the above described approach as follows. Starting from symbols that do not
  depend on the other ones, we set $\mathcal{W}(\cst{mnot}) = 11$ and
  $\mathcal{W}(\cst{mor}) = 17$. Then, we use these values to set
  $\mathcal{W}(\cst{mimplies}) = 37$. Clearly, these weights
  enable us to orient all definitions from left to right.
\end{exa}


% Many higher-order problems, especially those generated from proof assistants,
% contain hundreds of needless axioms.
% To filter out axioms that are unlikely to be useful in a proof attempt, many
% theorem provers rely on the SInE algorithm \cite{hv-2011-sine}. SInE starts with
% the set of symbols occurring in the conjecture and tries to find axioms that
% define the properties of these symbols. Then it looks for the definitions of
% newly found symbols until it reaches a fixpoint. Axioms annotated with
% \verb|definition| ease this search because they explicitly record the
% dependency between a symbol and its characterization. To exploit
% this information, we modified SInE to optionally include the definitions of
% symbols in the conjecture, regardless of whether they are filtered out or not.
% Similarly, we implemented mode of SInE which selects only conjecture and axioms
% annotated with \verb|definition|.

\ourpara{Evaluation and Discussion}

% The \textit{base} configuration treats all axioms annotated with
% \texttt{definition} as rewrite rules applied as preprocessing.
% In addition, we tested
We designed and evaluated the following strategies for handling
\texttt{definition} axioms:

\begin{enumerate}[\rm no-RW$+$KBO~]
  \item[\rm pre-RW~] rewrite all definitions as a preprocessing step;
  \item[\rm in-RW~] rewrite all definitions during the saturation, as an inprocessing step;
  \item[\rm $o$-RW~] rewrite only predicate definitions, during preprocessing;
  \item[\rm $o$-RW$+$KBO~] like $o$-RW but with adjusted KBO weights for the remaining
    definitions;
  \item[\rm no-RW~] no special treatment of definitions;
  \item[\rm no-RW$+$KBO~] like $no$-RW but adjusting KBO weights for all definitions.
\end{enumerate}

The results are given in Figure~\ref{fig:rewrite}. In all the figures, each
cell gives the number of proved problems, and cells marked with $\star$
correspond to the base configuration. The highest number in a category is typeset in
\relax{bold}. SH benchmarks are not
included because they do not contain the \texttt{definition} role.

\newcommand{\unknownres}{\ensuremath{{\varnothing}}}
\newcommand{\colalign}{\phantom{0}}

\begin{figure}[t]
  \centering
  \def\arraystretch{1.1}%
  \relax{\begin{tabular}{@{}l@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{}} \toprule
                & pre-RW                 & in-RW        & $o$-RW               & $o$-RW$+$KBO    & no-RW        & no-RW$ + $KBO  \\ \midrule
           TPTP & {\bf 1635}$^\star$     & 1619         & 1620                 & 1621            & 1298         & 1296
           \\ \bottomrule
         \end{tabular}}
       \captionof{figure}{Impact of the definition rewriting method}
       \label{fig:rewrite}
     \end{figure}



% {\tt Points to discuss:
% \begin{itemize}
%   \item On TPTP problems, there are only 2 problem proved by no-RW$+$KBO but not no-RW --- both after 11s: probably because of some strange effects
%   \item On TPTP problems, there are 4 problems proved by o-RW$+$KBO but not o-RW --- 2 of them under 1 second: suppressing superposition into rhs of definitions
%   \item 13 problems (some of them of rating 1) proved by oRW but not regular one
%   %  zip-jar-hot_defs_oRW(13) : NUM/NUM638^4.p,SEU/SEU800^1.p,NUM/NUM701^4.p,NUM/NUM699^4.p,SEU/SEU603^1.p,NUM/NUM647^4.p,SEU/SEU695^2.p,SEU/SEU630^2.p,NUM/NUM648^4.p,SEU/SEU540^1.p,SYO/SYO531^1.p,SEU/SEU594^1.p,SEU/SEU648^1.p
%   \item SH problems do not use definition tag, so this is just noise.
% \end{itemize}
% }

%%% @PETAR: I find this para very confusing. I don't know when you're talking
%%% about the old paper's numbers and the present one's. Lert's keep it simple
%%% and avoid mentions to the old paper. It's subsumed.
%%%
%%% Also, careful with words like "counterintuitively". What's counterintuitive
%%% to you might be intuitive to the reader. E.g. my experience is that hardly
%%% any well-meant change to a prover makes a big difference on performance,
%%% so I certainly wasn't surprised.
%
%In our earlier paper \cite{making-ho-work}, only an
%approximation of KBO weight adjustment was implemented: no checks for loops and
%no topological sorting was performed. For this article, we implemented the
%feature as described above. Counterintuitively, this did not substantially
%influence the performance of Zipperposition:

The four configurations in which definitions are treated as rewrite rules
perform much better than the other two. In contrast, adjusting KBO weights gives
no substantial improvement: Looking at raw data, we found only
\NumberOK{2}~problems proved by $o$-RW$+$KBO but not by $o$-RW in which the
feature was used in the proof. For no-RW and no-RW$+$KBO, the
\NumberOK{2}-problem difference may be just noise. Even though it proves fewer
problems, the configuration $o$-RW has some advantages over pre-RW: It proves
\NumberOK{16} problems that pre-RW does not, \NumberOK{3} of which have a TPTP
difficulty rating (the ratio of eligible %TPTP
provers that cannot prove the problem) of~1.

% \looseness=-1
Rewriting after clausification avoids getting stuck rewriting parts of the
proof state that might not contribute to the proof. In practice, we noticed that
rewriting can be so expensive that the prover can spend all
allotted CPU time in the preprocessing phase. The evaluation results confirm this
observation: There are \NumberOK{64} problems proved by in-RW but not by
pre-RW. Moreover, there are \NumberOK{41} problems that can be proved only
by in-RW but not by any other above described configuration. % from Figure~\ref{fig:rewrite}.

\section{Reasoning about Formulas}
\label{sec:satfol:ho-tech:formulas}

Higher-order logic identifies formulas with terms of Boolean type. To prove a problem, we often
need to instantiate a variable with the right predicate.
Finding this predicate can be easier if the problem is not clausified.
Consider the conjecture $\iexists f. \, f \, \cst{p} \, \cst{q} \iequiv \cst{p}
\iand \cst{q}$. Expressed in this form, the formula is easy to prove by taking
$f := \lambda x \, y. \> x  \iand y$. By contrast, guessing the right
instantiation for the negated, clausified form $ \neglit{F \, \cst{p} \,
\cst{q}} \llor \neglit{\cst{p}} \llor \neglit{\cst{q}},
\poslit{F \, \cst{p} \, \cst{q}} \llor \poslit{\cst{p}}$, $\poslit{F \, \cst{p}
\, \cst{q}} \llor \poslit{\cst{q}}$ is more challenging.
One of the strengths of higher-order tableau provers is that they do not clausify the input
problem. This might partly explain Satallax's dominance in the THF division of CASC
competitions until CASC-J10.

% Traditionally, superposition provers clausify the input problem, simplify the
% initial clause set, and start the saturation loop. We propose to integrate
% clausification tightly in the saturation loop.
% %
% Instead of clausifying the input, we represent the input formula $\varphi$
% as a higher-order clause $\poslit{\varphi}$. Then we use Vukmirović and Nummelin's
% \emph{lazy clausification} rules \cite[Sect.~3.4]{our-bool-paper},
% which extend \lsup{}. These incrementally clausify top-level
% logical symbols; for example, a clause $C' \llor \neglit{(\cst{p} \iand \cst{q})}$ yields a
% new clause $C' \llor \neglit{\cst p} \llor \neglit{\cst q}$.
% Lazy clausification rules admit advanced
% forms of formula renaming and Skolem symbol reusing that are not available in
% standard clausification algorithms.
% \looseness=-1
The \osup{} calculus supports \emph{delayed clausification rules}
that insert problems into the proof state in their original,
nonclausified form, and clausify them gradually. 
Delayed clausification
allows the prover to analyze the syntactic structure of formulas during saturation,
whereas the more traditional approach of \emph{immediate clausification}
applies a standard clausification algorithm \cite{nw-01-small-cnf}
both as a preprocessing step and whenever predicate variables are instantiated.

An earlier evaluation of the \osup{} calculus \cite{bbtv-21-full-ho-sup} showed
that the \emph{outer} variant of delayed clausification substantially increases this calculus's performance.
The outer variant clausifies
top-level logical symbols, proceeding from the outside inwards;
for example, a clause $C \llor \neglit{(\cst{p} \iand \cst{q})}$ is transformed into $C \llor
\neglit{\cst p} \llor \neglit{\cst q}$. The calculus also supports \emph{inner} delayed
clausification, which uses only the core calculus rules to clausify problems.
Even though this is the laziest approach to clausification, the earlier
evaluation showed that this approach is inefficient. Thus, we focus only on the
outer rules.

\looseness=-1
Delayed clausification rules can be used as inference rules (which add conclusions
to the passive set) or as simplification rules (which delete premises and add
conclusions to the passive set).
%
Inferences give more flexibility, since all
intermediate clausification states will be stored in the proof state, at the
cost of producing many clauses. Simplifications produce fewer clauses,
but risk destroying informative syntactic structure.
Since clausifying equivalences can destroy a lot of syntactic structure
\cite{gs-05-boolsup}, we never apply simplifying rules
on them.

Delayed clausification can interfere with clause splitting techniques.
Zipperposition supports a lightweight variant of AVATAR \cite{av-2014-avatar},
an architecture that partitions the search space by
splitting clauses into variable-disjoint subclauses. This lightweight AVATAR is described
by Ebner et al.\ \cite[Sect.~7]{2021-ebt-unifying-splitting}. Combining it
with delayed clausification makes it possible to split a %higher-order
clause $(\varphi_1 \ior \cdots \ior \varphi_n) \eq \itrue$, where
the $\varphi_i$'s are arbitrarily complex formulas that share no free
variables with each other, into clauses $\varphi_i \eq \itrue$.
%
To finish the proof, it suffices to derive the empty clause under each assumption
$\varphi_i \eq \itrue$. Since the split is performed at the formula level, this
technique resembles tableaux, but it exploits the strengths of superposition,
such as its powerful redundancy criterion and simplification machinery, to
close the branches.

\newcommand{\instset}{\ensuremath{\mathit{Inst}}}
% When delayed clausification is used, the prover can make
% heuristic choices based on information obtained from the formulas on which
% clausification rules are applied.
% In particular, we look for $\lambda$-abstractions whose bodies are formulas
% in each active clause and store them in a set
% $\mathit{Inst}$. Optionally, $\mathit{Inst}$ can contain \emph{primitive
% instantiations} \cite{our-bool-paper}---that is, imitations (in the sense
% of higher-order unification) of logical symbols that approximate the shape of
% a formula that can instantiate a predicate variable. These instantiations resemble
% the ones tableau provers perform during the proof search.

Beyond splitting, interleaving clausification and saturation allows us to simulate another tableau-inspired
technique. Whenever dynamic clausification substitutes a fresh variable $X$ for
a predicate variable $x$ in a clause of the form $(\iforall x.\, \varphi) \eq
\itrue \llor C$, yielding $\varphi\{x \mapsto\nobreak X\}
\eq \itrue \llor C$, we can create additional clauses in which $x$ is replaced
with $t \in \instset$, where $\instset$ is a set of heuristically chosen terms.
This set contains $\lambda$-abstractions whose bodies are formulas and that
occur in activated clauses, and \emph{primitive instantiations}
\cite{our-bool-paper}---that is, imitations (in the sense of higher-order
unification) of logical symbols that approximate the shape of a predicate that
can instantiate a predicate variable.

Since a new term $t$ can be added to $\mathit{Inst}$ after a clause with a
quantified variable of $t$'s type has been activated, we 
remember the clauses $\varphi\{x \mapsto X\} \eq \itrue\allowbreak \llor C$ and instantiate
them when $\mathit{Inst}$ is extended.
Conveniently, these instantiated clauses are not recognized as subsumed by
Zipperposition, which uses an optimized, incomplete higher-order subsumption
algorithm.

Given a disequation $\cst{f}\,\tuple{s}{n} \not\eq \cst{f}\,\tuple{t}{n}$, the
\emph{abstraction} of $s_i$ is $\lambda x.\, u \ieq v$, where $u$ is obtained by
replacing all occurrences of $s_i$ in $\cst{f}\,\tuple{s}{n}$ with $x$ and $v$ is
obtained by replacing all occurrences of $s_i$ in
$\cst{f}\,\tuple{t}{n}$ with $x$. For \confrep{}{an equation }$\cst{f}\,\tuple{s}{n} \eq
\cst{f}\,\tuple{t}{n}$, the analogous abstraction is $\lambda x.\, \inot (u \ieq
v)$.
%
%
Adding abstractions of the literals occurring in the conjecture to $\mathit{Inst}$ can
provide useful instantiations for formulas such as induction principles of
datatypes. As the conjecture is negated\confrep{}{ in refutational theorem proving},
the equation's polarity is inverted in the
abstraction. 

\begin{exa}
\label{ex:dat056-2}
The clausified conjecture of the problem \texttt{DAT056\^{}2}
\cite{ns-13-leo2sh} from the TPTP library is $\cst{ap} \, \cst{xs}
\, (\cst{ap} \, \cst{ys} \, \cst{zs}) \not\eq \cst{ap} \, (\cst{ap} \, \cst{xs} \,
\cst{ys}) \, \cst{zs}$, where $\cst{ap}$ is the \confrep{}{list }append operator defined
recursively on its first argument and $\cst{xs}$, $\cst{ys}$, and $\cst{zs}$ are
of list type. Abstracting $\cst{xs}$ from the disequation yields $t = \lambda \mathit{xs}.\, \cst{ap} \, \mathit{xs} \, (\cst{ap} \, \cst{ys} \, \cst{zs})
\allowbreak\ieq\allowbreak \cst{ap} \, (\cst{ap} \, \mathit{xs} \, \cst{ys}) \, \cst{zs}$, which is added
to $\mathit{Inst}$.
Included in the problem is the induction axiom
for the list datatype: $\iforall p. \, p \, \cst{nil} \iand (\iforall x \,
\mathit{xs}. \, p \, \mathit{xs} \iimplies\allowbreak p \, (\cst{cons} \, x \,
\mathit{xs})) \iimplies\allowbreak \iforall \mathit{xs}. \, p \, \mathit{xs}$, where
$\cst{nil}$ and $\cst{cons}$ have the usual meanings.
Instantiating $p$ with $t$ and
using the $\cst{ap}$ definition, we can prove
$\iforall \mathit{xs}. \, \cst{ap} \, \mathit{xs} \, (\cst{ap} \, \cst{ys} \, \cst{zs})
\ieq \cst{ap} \, (\cst{ap} \, \mathit{xs} \, \cst{ys}) \, \cst{zs}$,
from which we easily derive a contradiction.
\end{exa}


\ourpara{Evaluation and Discussion}

\begin{figure}
\centering
  \def\arraystretch{1.1}%
  \begin{tabular}{@{}l@{\hskip 1.5em}l@{\hskip 0.5em}@{\hskip 1em}c@{\hskip 1em}c@{}} \toprule
    & & $+$\relax{LA} & $-$\relax{LA} \\ \midrule
    TPTP &
%    \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{TPTP}}}&
      \relax{IC}  & 1616  & 1635$^\star$    \\
    & \relax{DCI} & 1507  & 1532\phantom{$^\star$}    \\
    & \relax{DCS} & 1668  & {\bf 1703}\phantom{$^\star$} \\ \midrule
    SH &
%    \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{SH}}} &
      \relax{IC}  & \colalign425 & \colalign452$^\star$    \\
    & \relax{DCI} & \colalign362 & \colalign385\phantom{$^\star$}    \\
    & \relax{DCS} & \colalign441 & \colalign\textbf{457}\phantom{$^\star$} \\ \bottomrule
  \end{tabular}
  \caption{Impact of clausification and~lightweight AVATAR}
  \label{fig:avatar-clause}
\end{figure}

The base configuration (\emph{base}) uses immediate clausification (\relax{IC}) and 
disables lightweight AVATAR ($-$\relax{LA}). To test
the merits of delayed clausification, we vary \emph{base}'s parameters along two axes: We
choose immediate clausification (\relax{IC}), delayed clausification as inference
(\relax{DCI}), or delayed clausification as simplification (\relax{DCS}), and we
either enable ($+$\relax{LA}) or disable ($-$\relax{LA}) lightweight AVATAR.
Neither of the configurations uses instantiation with terms from $\instset$.

Figure~\ref{fig:avatar-clause} shows that using delayed clausification as
simplification greatly increases the success rate, regardless of whether
lightweight AVATAR is used. Using delayed clausification as inference has the
opposite effect on both problem sets, presumably due to the large number of
clauses it creates. By manually inspecting the  proofs found by the \relax{DCS}
configuration, we noticed that a main reason for its success is that it does
not simplify away equivalences.
%
Overall, the lightweight AVATAR harms performance, but the sets of
problems proved with and without it are vastly different. For example,
the \relax{IC}$+$\relax{LA} configuration proves \NumberOK{38} problems not
proved by \relax{IC}$-$\relax{LA} (i.e., \emph{base}) on TPTP benchmarks and
\NumberOK{14} such problems on SH benchmarks.

\looseness=-1
The Boolean instantiation technique presented above requires delayed
clausification.
We assessed it in the best configuration from
%To assess it, we enabled it in the best configuration from
Figure~\ref{fig:avatar-clause}, \relax{DCS}$-$\relax{LA}. With this change ($+$\relax{BI}),
Zipperposition proves \NumberOK{1700} TPTP problems and \NumberOK{456} SH
problems.
On TPTP, even though $+$\relax{BI} solves \NumberOK{3} problems less
than \relax{DCS}$-$LA, it is very useful: \NumberOK{41}
problems can be proved with $+$\relax{BI} but not with \relax{DCS}$-$\relax{LA}. 
Conversely, \NumberOK{44} problems are solved with \relax{DCS}$-$\relax{LA},
but not with $+$\relax{BI}, which suggests that Boolean instantiation can be 
explosive.
%
One of the problems Boolean instantiation helps solve is \texttt{NUM636\^{}2} (a
re-encoding of \texttt{NUM636\^{}3}).
It conjectures that $\iforall x.\, \cst{s} \, x \ineq x$, where $x$ ranges over
Peano-style numbers specified by $\cst{z}$ and $\cst{s}$. The given axioms are
the induction principle $\forall p.\, p \, \cst{z} \,\iand\, \forall x. \, (p \,
x \iimplies p \, (\cst{s} \, x)) \iimplies \forall x. \, p\, x$, injectivity
$\forall x y. \, \cst{s}\,x \ieq \cst{s}\,y \iimplies x \ieq y$, and
distinctness $\forall x. \, \cst{s}\,x \ineq \cst{z}$. The conjecture is easily
proved if Boolean instantiation is enabled: Even though the conjecture literal
cannot be abstracted, instantiating $p$ with the term $\lambda x.\, \cst{s}
\, x \ineq x$ used in the encoding of the (nonclausified) conjecture leads to a proof in just
\NumberOK{22} given clause loop iterations. Zipperposition also finds a
proof using the \relax{DCI}$-$\relax{LA} configuration, but this requires
\NumberOK{294} iterations.

%Raw data reveals that Boolean instantiation
The $+$\relax{BI} configuration proves \NumberOK{18} TPTP problems no other
configuration from Figure~\ref{fig:avatar-clause} can prove. Among these is
\texttt{DAT056\^{}2} (Example~\ref{ex:dat056-2}). In contrast, on SH benchmarks, only
\NumberOK{6} problems are proved using $+$\relax{BI} and not
\relax{DCS}$-$\relax{LA}. For all these problems, Boolean instantiation does not
appear in the proof, suggesting that this result is due to the randomness in the
evaluation environment. The fact that \relax{BI} has no effect on SH benchmarks
is to be expected because Sledgehammer does
not include lemmas whose name contains the substring \texttt{.induct} and that
contain predicate variables. Therefore, \relax{BI} applies to fewer clauses.

\section{Exploring Boolean Selection Functions}
\label{sec:satfol:ho-tech:bool-select}

Superposition calculi are parameterized by a literal selection function and a
term order that help prune considerable swaths of the search space without
jeopardizing completeness. The core inferences apply only to a clause's
\emph{eligible} literals, defined as either the clause's selected
literals or, if none are selected, the clause's literals that are maximal with
respect to the term order. To further restrict which terms can be targeted by
an inference, the \osup{} calculus introduces \emph{Boolean selection
functions}. 

A Boolean selection function chooses \emph{green subterms} of Boolean type
(different than $\top$ or $\bot$ and not occurring at either side of a positive
literal) in a clause and gives rise to a notion of eligibility that considers
the formula structure. Green subterms correspond to the first-order skeleton of
a higher-order term; that is, they do not occur in positions under applied
variables, quantifiers, or $\lambda$-abstractions.

%In particular, green
%subterms are subterms arising inside first-order-like contexts, and for first
%other problems, all subterms are green.


\begin{definition}[Green subterms and green positions]
  \,Green subterms and green positions are defined inductively as
  follows: $t$ is a green subterm of $t$ at green position $\varepsilon$; if $t$ is a green subterm of $u_i$ at green position $p$
  and $\cst{f}$ is a constant different from $\iforall$ and $\iexists$, then
  $t$ is a green subterm of $\cst{f} \, \tuplen{u}$ at green position $i.p$,
  assuming $i \leq n$.
\end{definition}

\begin{exa} The green subterms of the term $F \, \cst{a} \, \iand \, \cst{p} \,
(\iforall \, (\lambda x. \, \cst{q} \, x)) \, \cst{b}$ are the term itself, $F \, \cst{a}$, $\cst{p} \, (\iforall \, (\lambda x. \, \cst{q} \, x))
\, \cst{b}$, $\iforall \, (\lambda x. \, \cst{q} \, x)$, and $\cst{b}$.
\end{exa} 
%
Green positions are lifted to clauses as follows: If $p$ is the green position
of a subterm in $s$, and $s$ occurs in a literal $l \in \{s \eq t{,}\; s \not\eq
t\}$ of $C$, the green position of the same subterm in the clause is denoted by $l.s.p$.
\osup{} mandates additional restrictions on the Boolean selection function:
$\itrue$, $\ifalse$ and variable-headed terms cannot be selected; for literals
$s \eq t$, neither $s$ not $t$ cannot be selected; if a term $s$ contains a
variable $X$ as a green subterm, and $X\> \tuplen{u}$, with $n \ge 1$, is a
maximal term of the clause, $s$ cannot be selected.

\begin{definition}[Eligibility]
  \,Given a substitution $\sigma$ and term order $\succ$, we say a literal $l$
  is (strictly) eligible with respect to $\sigma$ in $C$ if it is selected in
  $C$ or there are no selected literals and no selected Boolean subterms in
  $C$ and $l\sigma$ is (strictly) maximal in $C\sigma$ with respect to the
  term order.
%
  The eligible subterms of a clause $C$ with respect to a substitution
  $\sigma$ are inductively defined as follows:
  Any subterm selected by the Boolean selection function is eligible.
  %Any selected subterm is eligible.
  For a strictly eligible literal $s \eq t$ with $t\sigma \not\succ
  s\sigma$, $s$ is eligible. For an eligible literal $s \not\eq t$ with
  $t\sigma \not\succ s\sigma$, $s$ is eligible. If a subterm $t$ is eligible
  and the head of $t$ is not $\ieq$ or $\ineq$, all direct green subterms of
  $t$ are eligible. If a subterm $t$ is eligible and $t$ is of the form $u
  \ieq v$ or $u \ineq v$, then $u$ is eligible if $v\sigma \not\succ u\sigma$
  and $v$ is eligible if $u\sigma \not\succ v\sigma$.
\end{definition}

The above definitions of green subterms and eligibility were originally introduced
with Boolean $\lambda$-superposition \cite{bbtv-21-full-ho-sup}.
The Boolean selection function plays a similar role as the literal
selection function in standard superposition.
Literal selection functions eliminate some of the nondeterminism present in the superposition
calculus by focusing on selected parts of the search space. Boolean selection functions achieve
the same goal, but in a different context: They eliminate nondeterminism that is not
present in standard superposition, namely, the choice of subformula on which 
the Boolean calculus rules are to be applied. As with literal selection functions,
selecting few (and smaller) subterms can give rise to fewer possible inferences
and reduce clause proliferation.

%
This notion of eligibility opens up possibilities for reasoning with
formulas that are hard to simulate with the existing superposition machinery.
For example, given a formula $\varphi \iimplies \psi$, selecting the antecedent
simulates forward reasoning, whereas selecting the consequent simulates backward
reasoning. The new eligibility also makes it possible to restrict the proof
search to a small, promising part of a formula. Note that
literal selection can override Boolean selection: Selecting a literal 
might make some of its green subterms eligible, regardless
of Boolean selection.

In our previous work \cite{nbtv-2021-foboolsup}, we left this area of new
possibilities largely unexplored. We designed simple functions that selected
smallest, largest, innermost, or outermost terms, but they did not impact
performance much. Here, we propose alternatives.
%
Intuitively, a well-performing literal selection
function might succeed at taming the combinatorial explosion if the
selected literal can take part in few inferences
\cite{hrsv-16-selsel}. However, Boolean selection functions
introduce another factor to consider:\ the context in which the selected
subterm occurs. This suggests the following definition:

\begin{defi}[Contextualized Boolean selection function]
  \label{def:context-bool-sel}
  \,Let $\mathit{ctx}(C)$ be a function that maps a clause $C$ to a set of green positions
  $p$ such that $C|_p$ is a selectable Boolean subterm, and let
  $\vartriangleright$ be a partial order on pairs of terms and green positions.
  The \emph{context
  Boolean selection function} $\mathit{Sel\/}_{\mathit{ctx}}^{\,\vartriangleright}(C)$
  selects all terms $t$ such that $t=C|_p$, $p \in \mathit{ctx}(C)$, and
  $(t,p)$ is maximal with respect to $\vartriangleright$.
\end{defi}

In the above definition, the function $\mathit{ctx}$ lets
us choose the context in which the Boolean subterm appears. Then, among
the terms in the chosen context, we choose the ones that are maximal with
respect to $\vartriangleright$.

Ganzinger and Stuber considered Boolean subterm selection for their extension of
first-order superposition with interpreted Boolean type \cite{gs-05-boolsup}. Unlike our calculus, their calculus requires
selection of subterms occurring in negative green positions, defined below.

\begin{defi}[Polarity of green positions]
 \,Negative and positive green positions in a clause $C = l_1 \llor \cdots \llor
 l_n$ are defined inductively as follows: For each
 $1 \leq i \leq n$, the green position $l_i.s$ is positive if $l_i = \poslit{s}$ and negative if $l_i
 = \neglit{s}$. If $p$ is positive (negative) and $C|_p = s \,
 \tuplen{t}$ where $s$ is either $\iand$ or $\ior$, then each
 $p.i, 1 \leq i \leq n$, is positive (negative). If $p$ is positive and $C|_p =
 \inot \, s$, then $p.1$ is negative; if $p$ is negative and $C|_p = \inot \, s$,
 then $p.1$ is positive. If $p$ is positive and $C|_p = s \iimplies t$, then
 $p.1$ is negative and $p.2$ is positive; if $p$ is negative and $C|_p = s
 \iimplies t$, then $p.1$ is positive and $p.2$ is negative.
\end{defi}
Note that the polarity of $p$ is undefined whenever $C|_p$ is not a green Boolean subterm
or it occurs under a (dis)equivalence or an uninterpreted symbol.
To assess how the function $\mathit{ctx}$ affects performance, we use the following
selection functions that consider green positions of selectable Boolean terms:
%
\begin{enumerate}[\rm\texttt{Backward}~]
  \item[\rm\texttt{Any}~] select all green positions;
  \item[\rm\texttt{Pos}~] select all positive green positions;
  \item[\rm\texttt{Neg}~] select all negative green positions;
  \item[\rm\texttt{Forward}~] select all green positions $p = q.1$ such that $C|_q = s \iimplies t$;
  \item[\rm\texttt{Backward}~] select all green positions $p = q.2$ such that $C|_q = s \iimplies t$;
  \item[\rm\texttt{Deep}~] select all green positions of maximal length;
  \item[\rm\texttt{Shallow}~] select all green positions of minimal length.
\end{enumerate}

\looseness=-1
We also introduce three partial orders for selecting subterms from a given
context. For all three orders, if exactly one of the subterms has a logical head,
then the subterm with the nonlogical head is larger, because logical symbols
are more explosive. Otherwise, the orders use the following criteria:
%
\begin{enumerate}[$\vartriangleright_\text{ground}$~]
  \item[$\vartriangleright_\text{ground}$~] If exactly one of the subterms is ground, make the
  ground subterm larger; otherwise, if exactly one of the subterms is of the form $s \ieq
  t$, make this subterm larger.

\smallskip

  \item[$\vartriangleright_\text{depth}$~] If one of the subterms has larger subterm depth
  (longest valid green subterm position), make this subterm larger; otherwise, if one of the
  subterms has less distinct variables, make this subterm larger.

\smallskip

  \item[$\vartriangleright_\text{def}$~] If exactly one of the subterms is of the form
  $\cst{p}\,\tuplen{X}$ where $\tuplen{X}$ is a tuple of free variables, make
  the other subterm larger; otherwise, if exactly one of the subterms is of the form $X \,
  \tuplen{s}$, make the other subterm larger.
\end{enumerate}

In case of a tie, the subterm with the smaller syntactic weight is made larger,
and if both subterms have the same weight, the term that occurs in a position further
to the left (i.e., that has a lexicographically smaller position) is made larger.

These orders follow the design principle enunciated by Hoder et
al.~\cite{hrsv-16-selsel} that ground or deep terms and terms with repeated
variables are ``less unifiable'' with the similar observation for higher-order
logic that reasoning about interpreted symbols or applied variables is usually
explosive.

\begin{exa}
  Selecting the right Boolean subterm can help avoid elaborating
  high\-er-order inferences. Consider the unsatisfiable clause set consisting of $\cst{p} \, (\lambda y.\, X
  \, (\lambda x.\, x) \, \cst{a}) \iimplies\allowbreak \inot(\cst{p} \, (\lambda y. \allowbreak\, X
  \, y \, \cst{a}))$, $\cst{p} \, (\lambda y.\, \cst{a})$, and
  $\cst{p} \, (\lambda y. \, y^{100} \, \cst{b})$.
  Note that $\cst{p} \, (\lambda y.\, X
  \, (\lambda x.\, x) \, \cst{a})$ and $\cst{p} \,
  (\lambda y.\, \cst{a})$ have infinitely many unifiers of the form $\{ X \mapsto \lambda fx.
  \, f^i\, (x) \}, i \geq 0$, whereas $\cst{p} \, (\lambda y. \, X
  \, y \, \cst{a})$ and $\cst{p} \, (\lambda y. y^{100} \, \cst{b})$ have only one unifier. 
  If \texttt{Forward} context selection is enabled,
  $\cst{p} \, (\lambda y.\, X \, (\lambda x.\, x) \, \cst{a})$ is made the target of superposition inference, 
  forcing computation of at least 100 unifiers (under the assumption that
  unifiers are returned in order of increasing $i$) before we get to refute
  $\inot(\cst{p} \, (\lambda y. y^{100} \, \cst{b}))$.
  In contrast, \texttt{Backward} context selection allows us to
  superpose from $\cst{p} \, (\lambda y. \, y^{100} \, \cst{b})$ into $\cst{p} \, (\lambda y. \, X
  \, y \, \cst{a})$, avoiding this explosion. 
\end{exa}

\ourpara{Evaluation and Discussion}

\begin{figure}
\centering
\def\arraystretch{1.1}%
 \begin{tabular}{@{}l@{\hskip 1.5em}l@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{}} \toprule
  &                                    & \texttt{Any} & \texttt{Pos} & \texttt{Neg}        & \texttt{Forward} & \texttt{Backward} & \texttt{Deep}     & \texttt{Shallow}  \\ \midrule
  TPTP &
    $\vartriangleright_\text{ground}$  & 1538         & 1550         & 1547                & 1534             & {\bf 1554}        & 1539              & 1538     \\
  & $\vartriangleright_\text{depth}$   & 1542         & 1550         & 1528                & 1542             & 1550              & 1547              & 1535     \\
  & $\vartriangleright_\text{def}$     & 1543         & 1551         & 1540                & 1540             & 1551              & 1545              & 1537  \\ \midrule

%  \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{SH}}} &
  SH &
    $\vartriangleright_\text{ground}$  & \colalign386 & \colalign379  & \colalign386       & \colalign386     & \colalign379      & \colalign{\bf387} & \colalign{\bf387}     \\
  & $\vartriangleright_\text{depth}$   & \colalign377 & \colalign376  & \colalign384       & \colalign378     & \colalign376      & \colalign379      & \colalign376      \\
  & $\vartriangleright_\text{def}$     & \colalign379 & \colalign374  & \colalign{\bf387}  & \colalign379     & \colalign380      & \colalign377      & \colalign381  \\ \bottomrule
 \end{tabular}
 \caption{Impact of the Boolean selection function}
 \label{fig:bool-sel}
\end{figure}

When the input problem is clausified using immediate clausification, almost
all Boolean structure is lost. In this case, we expect Boolean selection to
have a modest effect. To better assess this feature, in this evaluation we use
\relax{DCI}$-$\relax{LA} from Sect.~\ref{sec:satfol:ho-tech:formulas} as the baseline
configuration. To avoid interference of literal and Boolean selection, we
additionally forbid the literal selection function from selecting a literal if
it contains a selectable Boolean subterm.

The results of evaluating 21 concrete selection functions obtained
by instantiating the contextualized Boolean selection function are shown in
Figure~\ref{fig:bool-sel}. Rows denote the used partial order $\vartriangleright$,
while columns denote the function $\mathit{ctx}$.

\looseness=-1
On TPTP benchmarks, Boolean selection helps tame the explosion caused by
dynamic clausification used as inference: \NumberOK{All but one} selection functions
outperform the \relax{DCI}$-$\relax{LA} baseline of \NumberOK{1532} proved problems. Coming back to the problem
\texttt{NUM636\^{}2} from Sect.~\ref{sec:satfol:ho-tech:formulas}, using Boolean selection
can reduce the number of given clause loop iterations from
\NumberOK{294} to \NumberOK{71}.

The results suggest that selection of term context has more impact
than the partial term order. % The number of proved problems fluctuates more when
%changing the function $\mathit{ctx}$ than when changing the partial order.
Also, the best results are obtained when a context more specific than \texttt{Any}
is chosen. Remarkably, functions using \texttt{Pos} context
perform better than the ones using \texttt{Neg} context
on TPTP but the opposite is observed on SH.


Using different Boolean selection functions yields vastly different sets of
proved problems on TPTP benchmarks: In total, there are \NumberOK{103} problems
proved by some configuration from Figure~\ref{fig:bool-sel} but not by
\relax{DCI}$-$\relax{LA}. However, there are only \NumberOK{16} such SH problems.
It would seem that the advanced formula reasoning facilitated by the Boolean
selection formulas is usually not required by Sledgehammer problems.

% {\tt
%   \begin{itemize}
%      \item To evaluate bool selection functions we used DCI from above, but
%            disabled literal selection function from selecting a literal if it
%            has a selectable Boolean subterm.
%      \item For TPTP it seems that selection is a good idea: all configurations
%      are better than naked DCI (only 6 for SH).
%      \item Overall, it seems that position has bigger influence that the
%      selection function.
%      \item On SH there are 14 problems provable by some of the Boolean
%            selection functions but not naked CSI. Sel3\_deep proves 12 problems
%            not proved by DCI. Interestingly, the best function Sel3\_neg proves
%            only 7 problems not proved by DCI.
%      \item On TPTP much better: 110 problems not proved by DCI but proved by
%      some of the selection functions. sel3\_pos proves 74 problems not proved
%      by DCI, sel1 backward 73.
%      \item TODO: Add any context and maybe $\vartriangleright_4$ which uses weight only
%      \item What we also observed is that it on TPTP problem it is reasonable to have any selection
%            function and that already stops the explosion. On SH quality is more important.
%      \item
%   \end{itemize}
% }

\section{Enumerating Infinitely Branching Inferences}
\label{sec:satfol:ho-tech:infinite-branching}

\looseness=-1
As an optimization and to simplify the code, Leo-III
\cite{as-18-phd} and Vampire 4.4 \cite{br-19-restricted-unif} (which uses
\confrep{}{\emph{restricted combinatory unification}, }a predecessor of combinatory
superposition) compute only a finite subset of the possible conclusions for
inferences that require enumerating a CSU. Not only is this a source of
incompleteness, but choosing the cardinality of the computed subset is a
difficult heuristic choice. Small sets can result in missing the unifier
necessary for the proof, whereas large sets make the prover spend too long in
the unification procedure, generate useless clauses, and possibly get sidetracked into the wrong parts of the search space.

We propose a modification to the given clause procedure to seamlessly
interleave unifier computation and proof state exploration. Given a complete
unification procedure, which may yield infinite streams of unifiers, our
modification fairly enumerates all conclusions of inferences relying on
elements of a CSU. Under some reasonable assumptions, it behaves exactly like
the standard given clause procedure on purely first-order problems.
We also describe heuristics that help achieve a similar
performance as when using incomplete, terminating unification procedures
without sacrificing completeness.

Given that we cannot decide whether there exists a next CSU
element in a stream of unifiers, the request for the next conclusion might not
terminate,
effectively bringing the theorem prover to a halt. Our modified given clause procedure
expects the unification procedure to return a lazily computed stream
\cite[Sect.~4.2]{co-1999-funds}, where each element is either $\emptyset$ or
a singleton set containing a unifier. To avoid getting stuck waiting for a unifier
that may not exist, the unification procedure should return
$\emptyset$ after it performs some number of operations without finding a unifier.

The complete unification procedure by Vukmirović et al.\
\cite{our-unif-paper} returns such a stream. Other procedures such
as Huet's \cite{gh-75-unification} and Jensen and
Pietrzykowski's \cite{jp-76-unif} can easily be adapted to meet this
requirement. Based on the stream of unifiers interspersed with $\emptyset$, we
can construct a stream of inferences similarly interspersed with $\emptyset$.
Any finite prefixes of this stream can be computed in finite time.

To support such streams in the given clause procedure, we extend it to
represent the proof state not only by the active ($A$) and passive ($P$) clause
sets, but also by a priority queue $Q$ containing the inference streams,
similar to the ``to do'' set $T$ present in the abstract Zipperposition loop of 
Waldmann et al.\  \cite[Sect.~4]{wtrb-20-sat-framework}.
Each stream is associated with a weight, and $Q$ is sorted in order of
increasing weight\confrep{}{, a low weight corresponding to a high priority}.
When they introduced \lsup, Bentkamp et al.\
\cite{bentkamp-et-al-2021-lamsup-journal} described an older version of this extension. Here
we present a newer version in more detail, including heuristics to postpone
unpromising streams. The pseudocode of the modified procedure is as follows:
\vspace{2.5\jot}
%
\newcommand\ParamMode{\ensuremath{K_{\mathrm{fair}}}}
\newcommand\ParamMaxStreams{\ensuremath{K_{\mathrm{best}}}}
\newcommand\ParamRetry{\ensuremath{K_{\mathrm{retry}}}}
\newcommand{\assign}[2]{\State \ensuremath{\mathit{#1} \gets #2}}
\newcommand{\assignSameLine}[2]{\ensuremath{\mathit{#1} \gets #2}}
\algrenewcommand\algorithmicindent{1em}
\begin{algorithmic}[0]
    \Function{ExtractClause}{$Q$, $\mathit{stream}$}
      \assign{maybe\_clause}{\text{pop and compute the first element of } \mathit{stream} }
      \If{$\mathit{stream}$ is not empty}
        \State add $\mathit{stream}$ to $Q$ with an increased weight
      \EndIf
      \State \Return $\mathit{maybe\_clause}$
    \EndFunction

    \vspace{2\jot}

    \Function{HeuristicProbe}{$Q$}
      \assign{i}{0}
      \assign{collected\_clauses}{\emptyset}
      \While{$i < \ParamMaxStreams$ and $Q \not= \emptyset$}
        \assign{j}{0}
        \assign{maybe\_clause}{\emptyset}
        \While{$j < \ParamRetry$ and $Q \not= \emptyset$ and $\mathit{maybe\_clause} = \emptyset$}
          \assign{stream}{\text{pop the lowest-weight stream in } Q}
          \assign{maybe\_clause}{\textsc{ExtractClause}(Q, \mathit{stream})}
          \assign{j}{j+1}
        \EndWhile
        \assign{collected\_clauses}{\mathit{collected\_clauses} \mathrel\cup \mathit{maybe\_clause}}
        \assign{i}{i+1}
      \EndWhile
      \State \Return $\mathit{collected\_clauses}$
    \EndFunction

    \vspace{2\jot}

  \Function{FairProbe}{$Q$, $\mathit{num\_oldest}$}
    \assign{collected\_clauses}{\emptyset}
    \assign{oldest\_streams}{\text{pop } \mathit{num\_oldest} \text{ oldest streams from } Q}
    \For{$\mathit{stream}$ in $\mathit{oldest\_streams}$}
      \assign{collected\_clauses}{collected\_clauses \mathrel\cup \textsc{ExtractClause}(Q, \mathit{stream})}
    \EndFor
    \State \Return $\mathit{collected\_clauses}$
  \EndFunction

  \vspace{2\jot}

  \Function{ForceProbe}{$Q$}
    \assign{collected\_clauses}{\emptyset}
    \While {$Q \not= \emptyset$ and $\mathit{collected\_clauses} = \emptyset$}
      \assign{collected\_clauses}{\textsc{FairProbe}(Q, |Q|)}
    \EndWhile

    \If{$Q = \mathit{collected\_clauses} = \emptyset$}
      \assign{status}{\textsf{Satisfiable}}
      % \Return (\textsf{Satisfiable}, $\mathit{collected\_clauses}$)
    \Else{}
      \assign{status}{\textsf{Unknown}}
      % \Return (\textsf{Unknown}, $\mathit{collected\_clauses}$)
    \EndIf

    \State \Return $(\mathit{status}, \mathit{collected\_clauses})$
  \EndFunction

  \vspace{2\jot}

  \Function{GivenClause}{$P$, $A$, $Q$}

  \assign{i}{0}
  \assign{status}{\textsf{Unknown}}
  \While{$\mathit{status} = \textsf{Unknown}$}
    \If{$P = \emptyset$}
      \assign{(status, \,forced\_clauses)}{\textsc{ForceProbe}(Q)}
      \assign{P}{P \mathrel\cup \mathit{forced\_clauses}}
    \Else
      \assign{given}{\text{pop a chosen clause from }P \text{ and simplify it}}
      \If{$\mathit{given}$ is the empty clause}
        \assign{status}{\textsf{Unsatisfiable}}
      \Else
        \assign{A}{A \mathrel\cup \{\mathit{given}\}}
        \For{$\mathit{stream}$ in streams of inferences between $\mathit{given}$ and $\mathit{other} \in A$}
          \If{$\mathit{stream}$ is not empty} \assign{P}{P \mathrel\cup \textsc{ExtractClause}($Q$, \mathit{stream})}
          \EndIf
        \EndFor
        \assign{i}{i+1}
        \If{$i \; \mathrm{mod} \; \ParamMode = 0$} \assign{P}{P \mathrel\cup \textsc{FairProbe}(Q, i/\ParamMode )}
        \Else{} \assign{P}{P \mathrel\cup \textsc{HeuristicProbe}(Q)}
        \EndIf
      \EndIf
    \EndIf
  \EndWhile

  \State \Return $\mathit{status}$

  \EndFunction
\end{algorithmic}
% \caption{Our modified given clause procedure}
% \label{fig:gc-pseudocode}
%
% \end{figure}
\vspace{2.5\jot}

Initially, all input clauses are put into $P$, and $A$ and $Q$ are empty. Unlike
in the standard given clause procedure, inference results are represented as clause
streams. The first element is inserted into $P$, and the rest of the stream is
stored in $Q$ with some positive integer weight computed from the inference rule.

% The functions \textsc{Probe} and \textsc{ForceProbe} extract some conclusions
% from the inference streams and store them in $P$. \textsc{Probe} heuristically
% chooses some clauses, whereas \textsc{ForceProbe} is used when $P$ is empty to
% find one clause if previous probes failed to produce any clauses, as a fallback.
% By choosing clauses from several streams at the same time,
% we give more freedom to the prover's existing clause selection heuristics,
% which select a clause from $P$.

% \looseness=-1
% \textsc{Probe} has two modes of operation, controlled by a parameter \ParamMode~(by
% default, $\ParamMode = 70$). In every $\ParamMode$th invocation, \textsc{Probe} extracts
% conclusions from an increasing number of oldest streams. This amounts to
% dovetailing, which achieves fairness, analogously to the
% pick--given ratio \cite{ss-02-brainiac,mcw-1997-otter} in the
% given clause procedure. For the other
% invocations, \textsc{Probe} selects the $\ParamMaxStreams$ highest-priority streams (by
% default, $\ParamMaxStreams = 10$) and extracts elements from, or \emph{probes}, each stream,
% discarding $\emptyset$ values, until a clause is found or until
% $\ParamRetry$ (by default, $\ParamRetry=20$) $\emptyset$ values have been discarded.
% Setting $\ParamRetry>1$ ensures that promising streams are given a fair chance to
% produce a clause, even if they rely on a complicated unifier. Each
% probed stream~$S$ is put back in $Q$ with a new priority.%

\looseness=-1
To eventually consider inference conclusions from streams in $Q$ as given
clauses, we extract elements from, or \emph{probe}, streams and move any obtained
clauses to $P$. Analogously to the traditional pick--given ratio
\cite{ss-02-brainiac,mcw-1997-otter}, we use a parameter
\ParamMode{} (by default, $\ParamMode = 70$) to ensure fairness: Every \ParamMode{}th iteration,
\textsc{FairProbe} probes an increasing number of oldest streams, which achieves
dovetailing. In all other iterations, \textsc{HeuristicProbe} attempts to
extract up to \ParamMaxStreams{}~clauses from the most promising streams (by default,
$\ParamMaxStreams = 7$).
In each attempt, the most promising stream in $Q$ is chosen. If its first
element is $\emptyset$, the rest of the stream is inserted into $Q$ and a new stream is
chosen. This is repeated until either \ParamRetry{} occurrences of $\emptyset$ have  been
met (by default, $\ParamRetry = 20$) or the stream yields a singleton. Setting $\ParamRetry > 0$ increases
the chance that \textsc{HeuristicProbe} will return $\ParamMaxStreams$ clauses, as desired. Finally, if $P$ becomes empty, \textsc{ForceProbe}
searches relentlessly for a clause in $Q$, as a fallback.

\looseness=-1
The function \textsc{ExtractClause} extracts an element from a nonempty stream
not in $Q$ and inserts the remaining
stream into $Q$ with an increased weight, calculated as follows.
Let $n$ be the number of times the stream was chosen for
probing. If probing results in $\emptyset$, the stream's weight is increased by
$\max\,\{2{,}\; n-16\}$. If probing results in a clause $C$ whose penalty is
$p$, the stream's weight is increased by $p \cdot \max\,\{1{,}\; n-64\}$. The
penalty of a clause is a number assigned by Zipperposition based on
features such as the depth of its derivation and the rules used in it.
The constants $16$ and $64$ increase the chance that newer clause-producing streams are picked,
which is desirable because their first clauses are expected to be useful.

\looseness=-1
All three probing functions are invoked by
\textsc{GivenClause}, which contains the saturation loop. It differs
from the standard given clause procedure in three ways:
First, the proof state includes $Q$ in addition to $P$ and $A$. Second,
new inferences involving the given clause are added to $Q$ instead of being
performed immediately. Third, inferences in $Q$ are periodically performed
lazily to fill $P$.

\newcommand\infstream[1]{[#1]}

\begin{exa} 
  \begin{sloppypar}
  Consider the unsatisfiable two-clause problem $\{ X \, (\cst{f} \,
  \cst{a}) \not\eq \cst{f} \, (X \, \cst{a}) \lor \cst{p} \, (X \, \cst{a}),\allowbreak
  \neg \cst{p} \, (\cst{f}^{100} \, \cst{a})  \}$ and a selection function which
  selects negative literals.
  Let $P \mid A \mid Q$ denote
  the state of the given clause loop (i.e., the contents of the passive and active set
  and of the stream queue), and let $\infstream{ a_1, a_2, \ldots }$
  denote an infinite stream of elements.
  \end{sloppypar}

  The given clause loop begins in the state $X \, (\cst{f} \, \cst{a}) \not\eq
  \cst{f} \, (X \, \cst{a}) \lor \cst{p} \, (X \, \cst{a}), \neg \cst{p} \,
  (\cst{f}^{100} \, \cst{a}) \mid \emptyset \mid \emptyset$. If the clause $\neg
  \cst{p} \, (\cst{f}^{100} \, \cst{a})$ is chosen for processing, since $Q$ is empty
  and no inferences with the chosen clause are possible, the state becomes $X \,
  (\cst{f} \, \cst{a}) \not\eq \cst{f} \, (X \, \cst{a}) \lor \cst{p} \, (X \,
  \cst{a}) \mid \neg \cst{p} \, (\cst{f}^{100} \, \cst{a}) \mid \emptyset$. When
  the clause $X \, (\cst{f} \, \cst{a}) \not\eq \cst{f} \, (X \, \cst{a}) \lor
  \cst{p} \, (X \, \cst{a})$ is chosen, a new stream which enumerates results of
  equality resolution (on its first literal) is created. There are infinitely many
  conclusions of this inference, since there are infinitely many unifiers for the
  first literal of the form $\{ X \mapsto \lambda x. \, \cst{f}^i \, x \}$, for
  $i \geq 0$. Thus, the stream is $\infstream{ \{\cst{p} \, \cst{a}\}, \{\cst{p} \, (\cst{f}\,\cst{a})\}, \ldots }$,
  possibly with $\emptyset$s interspersed. With the standard given clause procedure,
  there would have been no way to represent this infinitary result.
  %The creation of the
  %inference result stream is the most striking difference between our
  %and standard given clause procedure.

  When the stream is created, its first element is popped and put into $P$. Then, based on the
  parameters that control inference stream probing, some number of clauses from
  the stream are computed and moved to $P$. After two iterations, the state might be
  $ \cst{p} \,
  \cst{a}, \cst{p} \, (\cst{f} \, \cst{a}), \cst{p} \, (\cst{f} \, (\cst{f} \, \cst{a}))   \mid X \,
  (\cst{f} \, \cst{a}) \not\eq \cst{f} \, (X \, \cst{a}) \lor \cst{p} \, (X \,
  \cst{a}), \neg \cst{p} \, (\cst{f}^{100} \, \cst{a}) \mid \infstream{ \{\cst{p} \, (\cst{f}^3\,\cst{a})\}, \ldots }$. 
  
  In the next iterations, some clause of the form $\cst{p} \, (\cst{f}^{i} \,
  \cst{a})$, where $i < 100$, is chosen, but no inferences with it can be
  performed. Then, the stream created in the second iteration is probed, and its
  results fill the set $P$. Ultimately, the clause $\cst{p} \, (\cst{f}^{100} \,
  \cst{a})$ is chosen, at which point $\bot$ is quickly derived.
\end{exa}

\textsc{GivenClause} eagerly stores the first element of a new inference stream
in $P$ to imitate the standard given clause procedure. If the underlying
unification procedure behaves like the standard first-order unification
algorithm on higher-order logic's first-order fragment, our given clause
procedure coincides with the standard one. The unification procedure by
Vukmirović et al.\ terminates on the first-order and other fragments
\cite{tn-93-patterns}. To avoid computing
complicated unifiers eagerly, it immediately returns $\emptyset$ for a problem that does not
belong to one of the fragments that admit efficient unifier computation.


The design of our given clause procedure was guided by folklore knowledge about
higher-order theorem proving. First, in our experience most steps in
long higher-order proofs involve first-order literals. The unification
procedure and inference scheduling ensure that first-order inference
conclusions are put in the proof state as early as possible. Second, some
inference rules are expected to be largely useless. We initialize the stream
penalty differently for each rule, allowing old streams of more useful
inferences to be queried before newly added, but potentially less useful
streams. Finally, if we use a unification procedure that has aggressive
redundancy elimination, we will often find the necessary unifier within the
first few unifiers returned. Similarly, if a stream keeps returning
$\emptyset$, it is likely that it is blocked in a nonterminating computation
and should be ignored. Our heuristics to increase the stream penalties take
both observations into account.

\ourpara{Evaluation and Discussion}

When the unification procedure of Vukmirović et al.\ was implemented  in
Zipperposition, it was observed that this is the only competing
higher-order prover that proves all Church numeral problems from the TPTP,
never spending more than 5~s on a problem \cite{our-unif-paper}. On these hard
unification problems, the stream system allows the prover to explore the proof
state lazily.

Consider the TPTP problem \verb|NUM800^1|, which requires finding
a function $F$ such that $F \, \cst{c_1} \, \cst{c_2}
\ieq \cst{c_2} \iand F \, \cst{c_2} \, \cst{c_3} \ieq \cst{c_6}$, where
$\cst{c}_n$ abbreviates the Church numeral for~$n$, $\lambda s\, z. \>
s^n \, z$. To prove
\confrep{it}{the problem}, it suffices to take $F$ to be the multiplication operator
$\lambda x \, y \, s \, z. \> x \, (y \, s) \, z$.
However, this unifier is only one out of many available for each occurrence of
$F$.

% TODO: If I add a new benchmark set, make sure this sentence is fixed
In an independent evaluation setup on a set of 2606 TPTP version 7.2.0
problems almost identical to the one we use, Vukmirović et al.\
\cite[Sect.~7]{our-unif-paper}
compared a complete, nonterminating variant of the unification procedure and a
pragmatic, terminating variant. The
pragmatic variant was used directly---all the inference conclusions were put
immediately in $P$, bypassing $Q$. The complete variant, which relies on
possibly infinite streams and is much more prolific, proved only 15  problems
less than the most competitive pragmatic variant. Furthermore, it proved 19
problems not proved by the pragmatic variant.
%
This shows that our given clause procedure, with its heuristics, allows the
prover to defer exploring less promising branches of the unification and uses
the full power of a complete higher-order unifier search to solve unification
problems that cannot be proved by a restricted procedure.

The parameters \ParamMode{}, \ParamRetry{}, and \ParamMaxStreams{} can greatly
influence the behavior of the given clause procedure, even when the same
unification procedure is used. Figure~\ref{fig:streams}
presents the effects of these parameters on TPTP and SH.
Selecting a low number of best clauses seems to
perform well on both benchmark sets. However, on SH benchmarks, which require
overwhelmingly first-order unifiers, visiting older streams should be delayed
a lot.

As with Boolean selection functions, changing these three parameters causes
a substantial difference in the set of proved problems. For example, the
configuration that performs the worst on TPTP benchmarks proves \NumberOK{12}
problems that the configuration performing the best on TPTP cannot prove; moreover, there
are \NumberOK{29} TPTP problems that are proved by some set of parameters
other than $\ParamMode=\ParamMaxStreams=16, \ParamRetry=2$. On SH, these
effects are much weaker; most reasonable combinations
of parameters perform similarly.

Among the competing higher-order provers, only Satallax uses infinitely
branching calculus rules. It maintains a queue of ``commands'' that contain
instructions on how to create a successor state in the tableau. One
command describes infinite enumeration of all closed terms of a given function
type. Each execution of this command makes progress in the enumeration. Unlike
evaluation of streams representing elements of CSU, each command execution
is guaranteed to make progress in enumerating the next closed functional
term, so there is no need to ever return $\emptyset$.



\begin{figure}
\centering
\begin{subfigure}[b]{1\textwidth}
  \centering
  \begin{tabular}{@{}l@{\kern.5em}l@{\qquad}c@{\kern.75em}c@{\kern.75em}c@{}l@{}c@{\kern.75em}c@{\kern.75em}c@{}l@{}c@{\kern.75em}c@{\kern.75em}c@{}}\toprule
  &&&&&&& \ParamMode \\[.5\jot]
  & & & 2 & & \hbox{\qquad} & & 16 & & \hbox{\qquad} & & 128 & \\[.25\jot]
  \cline{3-5}\cline{7-9}\cline{11-13}
  \\[-1.5\jot]
  &&& \ParamRetry &&&& \ParamRetry &&&& \ParamRetry \\[.5\jot]
  %                                     2                        16                     256
  &                         & 2    & 16   & 128  & & 2         & 16   & 128  & & 2    & 16   & 128 \\\midrule
  & $2$                     & 1643 & 1645 & 1645 & & 1661      & 1661 & 1658 & & 1669 & 1664 & 1664 \\[0.5\jot]
  $\ParamMaxStreams$ & $16$ & 1647 & 1646 & 1609 & & {\bf1670} & 1654 & 1602 & & 1665 & 1659 & 1597 \\[0.5\jot]
  & $128$                   & 1646 & 1644 & 1583 & & 1661      & 1656 & 1577 & & 1665 & 1658 & 1576 \\ \bottomrule
  \end{tabular}
  \caption{TPTP benchmarks}
  \label{fig:streams-tptp}
\end{subfigure}
\par\bigskip
\begin{subfigure}[b]{1\textwidth}
  \centering
  \begin{tabular}{@{}l@{\kern.5em}l@{\qquad}c@{\kern.75em}c@{\kern.75em}c@{}l@{}c@{\kern.75em}c@{\kern.75em}c@{}l@{}c@{\kern.75em}c@{\kern.75em}c@{}}\toprule
  &&&&&&& \ParamMode \\[.5\jot]
  & & & 2 & & \hbox{\qquad} & & 16 & & \hbox{\qquad} & & 128 & \\[.25\jot]
  \cline{3-5}\cline{7-9}\cline{11-13}
  \\[-1.5\jot]
  &&& \ParamRetry &&&& \ParamRetry &&&& \ParamRetry \\[.5\jot]
  %                                          2                                               16                                      128
  &                         & 2            & 16            & 128          & & 2            & 16            & 128          & & 2                  & 16            & 128 \\\midrule
  & $2$                     & \colalign460 & \colalign455  & \colalign454 & & \colalign465 & \colalign463  & \colalign458 & & \colalign466       & \colalign461  & \colalign461 \\[0.5\jot]
  $\ParamMaxStreams$ & $16$ & \colalign458 & \colalign453  & \colalign445 & & \colalign464 & \colalign459  & \colalign441 & & \colalign{\bf468}  & \colalign459  & \colalign442 \\[0.5\jot]
  & $128$                   & \colalign456 & \colalign452  & \colalign430 & & \colalign465 & \colalign458  & \colalign428 & & \colalign{\bf468}  & \colalign459  & \colalign425 \\ \bottomrule
  \end{tabular}
  \caption{SH benchmarks}
  \label{fig:streams-sh}
\end{subfigure}
\caption{Impact of the stream enumeration parameter}
\label{fig:streams}
\end{figure}

\section{Controlling Prolific Rules}
\label{sec:satfol:ho-tech:explosiveness}

To support higher-order features
such as function extensionality and quantification over functions,
many refutationally complete calculi employ highly prolific rules.
For example, \lsup{} includes a
\infname{FluidSup} rule \cite{bentkamp-et-al-2021-lamsup-journal} that very often applies to two
clauses if one of them contains a term of the form $F \, \overline{s}_n$,
where $n > 0$.
We describe three mechanisms to keep rules like these under control.

% \begin{enumerate}
%   \item Penalize the streams of expensive inferences.
%   \item Defer selecting the resulting clauses for processing.
%   \item Restrict the applicability of the rules.
% \end{enumerate}

% Complete provers that implement \lsup{} %, such as Zipperposition,
% have two main heuristic choices: which inference stream to query for clauses
% and which clauses  from the passive set to choose for processing. The first two mechanisms
% control these two choice points. The third mechanism
% sacrifices completeness by applying the inference rules only to a subset of chosen clauses.

First, \emph{we limit applicability of the prolific rules}. In practice, it
often suffices to apply prolific higher-order rules only to initial or shallow
clauses---clauses with a shallow derivation depth. Thus, we added an option to
forbid the application of a rule if the derivation depth of any premise exceeds a
limit.

\newcommand\ParamPenaltyIncrease{\ensuremath{K_\mathrm{incr}}}
Second, \emph{we penalize the streams of expensive inferences}. The weight of
each stream is given an initial value based on characteristics of the inference
premises such as their derivation depth. For prolific rules such as
\infname{FluidSup}, we increment this value by a parameter \ParamPenaltyIncrease. Weights for
less prolific variants of this rule, such as \infname{DupSup} \cite{bentkamp-et-al-2021-lamsup-journal}, are increased by a
fraction of $\ParamPenaltyIncrease$ (e.g., $\lfloor \ParamPenaltyIncrease/3 \rfloor$).

Third, \emph{we defer the selection of prolific clauses}. To select the given
clause, most saturating provers evaluate clauses according to some criteria and
choose the clause with the lowest evaluation. To make this choice efficient,
passive clauses are organized into a priority queue ordered by their
evaluations. Like E, Zipper\-position maintains multiple
queues, ordered by different evaluations, that are visited in a round-robin
fashion. It also uses E's two-layer evaluation functions, a variant of which
has recently been implemented in Vampire \cite{gs-20-clausesel}.
%
The two layers are \emph{clause priority} and \emph{clause weight}. Clauses
with higher priority are preferred, and the weight is used for tie-breaking.
Intuitively, the first layer crudely separates clauses into priority classes,
whereas the second one uses heuristic weights to prefer clauses within a
priority class. To control the selection of prolific clauses, we introduce new
clause priority functions that take into account features specific to
higher-order clauses.

The first new priority function, \verb|PreferHOSteps| (\verb|PHOS|), assigns a
higher priority if rules specific to higher-order superposition calculi
were used in the clause derivation. Since most of the other clause priority
functions tend to defer higher-order clauses, having a clause queue that prefers
them might be useful to find some proof more
efficiently. A simpler function, which prefers clauses containing
$\lambda$-abstractions, is \verb|PreferLambda| (\verb|PL|).

\verb|PreferHOSteps| separates clauses created using first-
and higher-order inference rules crudely. However, within higher-order
inference rules there are the ones which make clauses simpler and are thus more
preferable. An example of such a rule is
%
\[\namedinference{\infname{ArgCong}}{C \lor s \eq t}{C \lor s \, \tuplen{X}
\eq t \, \tuplen{X}}\]
%
where $s$ is of the type $\alpha_1 \rightarrow \cdots \rightarrow \alpha_k
\rightarrow \beta$, $\beta$ is a base type, $n \leq k$, free variables
$\tuplen{X}$ are fresh, and literal $s \eq t$ is strictly eligible. When $n =
k$, in most cases, the resulting clause has a first-order literal $s \, \tuplen{X}
\eq t \, \tuplen{X}$ in place of the literal $s \eq t$ of functional type, which usually makes the clause more useful. To
prefer clauses that are only mildly higher-order, we designed the function
\verb|PreferEasyHO| (\verb|PEHO|). It prefers clauses that are the result of
\infname{ArgCong}, have equations between terms of functional type or
between higher-order patterns, or have literals containing logical symbols, in that
order of priority.

A higher-order inference that applies a complicated substitution to a clause is
usually followed by a $\beta\eta$-normalization step. If
$\beta\eta$-normalization greatly reduces the size of a clause, it is likely
that this substitution simplifies the clause (e.g., by removing a variable's
arguments). The new priority function \verb|ByNormalizationFactor| (\verb|BNF|)
is designed to exploit this observation. It prefers clauses that were produced
by $\beta\eta$-normalization, and among those it prefers the ones with larger
size reductions.

Another new priority function is \verb|PreferShallowAppVars| (\verb|PSAV|). This
prefers clauses with lower depths of the deepest occurrence of an applied
variable---that is, $C[X \, \cst{a}]$ is preferred over $C[\cst{f}\,(X \,
\cst{a})]$. The intuition is that applying a substitution to an applied
variable often reduces the variable to a term with a constant head,
yielding a less explosive clause, and the gain is greater for variables closer
to the top level.  Among the functions that rely
on properties of applied variables, we implemented \verb|PreferDeepAppVars|
(\verb|PDAV|), which returns the priority opposite of \verb|PSAV|, and
\verb|ByAppVarNum| (\verb|BAVN|), which prefers clauses with fewer occurrences
of applied variables.



\ourpara{Evaluation and Discussion}

\begin{figure}[t]
  \centering
  % \def\arraystretch{1.1}%
  \relax{\begin{tabular}{@{}l@{\hskip 2em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{}} \toprule
            & \texttt{CP}                & \texttt{BAVN}  & \texttt{PL}  & \texttt{PSAV}       & \texttt{PHOS} & \texttt{PEHO}  & \texttt{BNF} & \texttt{PDAV}      \\ \midrule
   TPTP     & {1635}$^\star$             & {\bf 1640}     & 1604         & {1635}              & 1609          & 1617           & 1575         & 1533 \\[0.5\jot]
   SH       & {\bf \colalign452}$^\star$ & \colalign451   & \colalign417 & {\colalign450}      & \colalign439  & \colalign407   & \colalign411 & \colalign302 \\ \bottomrule
  \end{tabular}}
  \caption{Impact of the priority function}
  \label{fig:priorities}
\end{figure}
\begin{figure}[t]
  \centering
  \def\arraystretch{1.1}%
  \relax{\begin{tabular}{@{}l@{\hskip 2em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{}} \toprule
              & $\infty$                     & $16$          & $8$           & $4$             & $2$             & $1$      \\ \midrule
   TPTP       & {\bf 1635}$^\star$           & 1625          & 1632          & 1629            & 1628            & 1618 \\[0.5\jot]
   SH         & {\bf \colalign452}$^\star$   & \colalign438  & \colalign435  & \colalign439    & \colalign435    & \colalign440 \\ \bottomrule

  \end{tabular}}
  \caption{Impact of the \infname{FluidSup} weight increment \ParamPenaltyIncrease}
  \label{fig:penalties}
\end{figure}

In the base configuration (\emph{base}), Zipperposition visits several clause
queues. The configuration uses queues that prefer the clauses that stem from
the conjecture, the ones that have at least one positive literal, the ones that
have been moved from active to passive set, and so on. One of the queues uses the constant
priority function \texttt{ConstPrio} (\texttt{CP}), meaning that it assigns the same
priority to every clause. As this queue is the most often visited one in \emph{base},
changing its priority function should affect the result noticeably.
To evaluate the new priority functions, we replaced
\texttt{CP} with one of the new functions in this queue,
%the queue ordered by \texttt{CP} with the queue ordered by one of the new
%functions,
leaving the clause weight intact. The results are shown in Figure~\ref{fig:priorities}.

Even though constant priority function achieves remarkable performance, the new priority functions are
useful additions to the prover's repertoire: \NumberOK{37} additional
TPTP problems and \NumberOK{17} additional SH problems can be proved when some
nonconstant priority is used. The generally average-performing \verb|PEHO|
function can prove \NumberOK{9} problems not proved with any other priority function on TPTP
(and \NumberOK{1} on SH). Globally, \NumberOK{24} TPTP problems and \NumberOK{6} SH problems
can be proved exclusively using one particular priority function.


% It
% shows that the expensive priority functions \texttt{PHOS} and \texttt{BNF},
% which require inspecting the proof of clauses, hardly help. Simple functions
% such as \texttt{PL} are more effective: Compared with \textit{base}, \texttt{PL}
% loses \NumberNOK{one} problem overall but proves \NumberNOK{22} new problems.

% {\tt
%  \begin{itemize}

%   \item Another result which is hard to analyze without looking into differences.
%   \item On TPTP 32 solutions by using different priorities. On SH 20. Interestingly,
%   generally bad performing PEHO finds the largest number of solutions not found by CP : 15 (TPTP).
%   On SH the largest number of unique solutions is by PHOS and PEHO.
%   \item Another illustration of orthogonality: 25 problems can be proved by only one prio fun (TPTP), 9 on SH.

%  \end{itemize}
% }

Although it is necessary for refutational completeness, the \infname{FluidSup}
rule is disabled in \emph{base} because it is so explosive and so seldom useful.
To test whether increasing inference stream weights makes a difference on the
success rate, we tried enabling \infname{FluidSup} with different weight
increments~$\ParamPenaltyIncrease$ for \infname{FluidSup} inference queues. The
results are shown in Figure~\ref{fig:penalties}. As expected, using a low
increment with \infname{FluidSup} is detrimental on TPTP. On this
benchmark set, \NumberOK{16} additional problems can be proved when
\infname{FluidSup} is enabled. The penalty mostly affects only proving time: All
but \NumberOK{2} of these problems were proved by using at least three different
values of $\ParamPenaltyIncrease$. On SH problems, the best result is obtained
when the rule is disabled as well. Unexpectedly, the next best result is obtained when
$\ParamPenaltyIncrease=1$.
% However, as the column for
% $\ParamPenaltyIncrease=16$ shows, nor should we use too high an increment, since that
% delays useful \infname{FluidSup} inferences. Interestingly, even
% though the configuration with $\ParamPenaltyIncrease=1$ proves the least problems overall, it proves
% \NumberNOK{7} problems not proved by \emph{base}, which is more than any other
% configuration we tried.

% {\tt
% \begin{itemize}
%   \item Not a single solution found using fluidsup on SH
%   \item 11 solutions found on TPTP, almost all (9) with at least 3 configurations.
%         Configurations usually only influence the time necessary to prove.
%   \item Even though it proves the least problems p1 is necessary in portfolio: with p=1 it takes 0.13 s seconds to prove NUM636\^{}2
%   and 5.95s with p16 (provable by all ps but not base).
%   \end{itemize}
% }

%%% @PETAR: Too little, not informative. What's a "shallow clause"? Not defined yet. --JB
%In informal experiments, we also tested limiting the applicability
%of some rules to shallow clauses, but this had no clear impact.

\section{Controlling the Use of Backends}
\label{sec:satfol:ho-tech:backends}

\newcommand{\ParamNumClauses}{\ensuremath{K_\mathrm{size}}}
\newcommand{\ParamTime}{\ensuremath{K_\mathrm{time}}}

Cooperation with efficient off-the-shelf first-order theorem provers is an
essential feature of higher-order theorem provers such as Leo-III
\cite[Sect.~4.4]{as-18-phd} and Satallax \cite{cb-2013-satallax}.
% but also HOLyHammer \cite{ku-15-holyhammer} and Sledgehammer \cite{bn-10-sh}.
Those provers invoke first-order backends repeatedly
during a proof attempt and spend a substantial amount of time in backend
collaboration. Since \lsup{} generalizes a highly efficient
first-order calculus, we expect that future efficient \lsup{}
implementations will not benefit much from backends.
Nevertheless, experimental provers such
as Zipperposition can still gain a lot. We present some
techniques for controlling the use of backends.

In his thesis \cite[Sect.~6.1]{as-18-phd}, Steen extensively studies
the effects of using different first-order backends on the performance of
Leo-III. His results suggest that adding only one backend already substantially
improves the performance. To reduce the effort required for integrating multiple backends, we chose Ehoh \cite{ehoh-section} as our single
backend. Ehoh is an extension of the highly optimized superposition prover E 2.5
%%% PETAR: Check 2.5. Later we have 2.6 but that's for Ehoh++.
with support for higher-order features such as partial
application, applied variables, and interpreted Booleans.
%formulas occurring as arguments of function symbols.
On the one hand, Ehoh provides the efficiency of E while easing the translation from full
higher-order logic---the only missing syntactic feature is
$\lambda$-abstraction. On the other hand, Ehoh's higher-order reasoning
capabilities are limited. Its unification algorithm is essentially first-order,
and it cannot synthesize $\lambda$-abstractions.
% or instantiate predicate variables.

In a departure from Leo-III and other cooperative provers,
\confrep{}{instead of regularly invoking the backend, }%
we invoke \confrep{the backend}{it} at most once during a run of Zipperposition.
This is because most competitive higher-order provers, including Zipperposition, use a portfolio
mode in which many configurations are run for a short time, and we want to
leave enough time for native higher-order reasoning. Moreover, multiple
backend invocations tend to be wasteful, because currently each invocation
starts with no knowledge of the previous ones.

\looseness=-1
Only a carefully chosen subset of the available clauses are translated and sent
to Ehoh. Let $I$ be the set of \confrep{input clauses}{clauses representing the
input problem}. Given a proof state, let \confrep{$M = P \cup A$}{$M$ denote the
union of the current active and passive sets}, and let $M_\mathrm{ho}$ denote
the subset of $M$ that contains only clauses that were derived using at least
one \lsup{} rule not present in regular superposition. We order the clauses in
$M_\mathrm{ho}$ by increasing derivation depth, using syntactic weight to break
ties. Then we choose all clauses in $I$ and the first \ParamNumClauses~clauses from
$M_\mathrm{ho}$ for use with the backend reasoner.
%We include all the clauses in $I$ since they constitute the backbone of the
%initial problem. -- Tautology. --JB
We leave out clauses in $M \setminus (I \cup M_\mathrm{ho})$ because Ehoh can rederive them.
We also expect large clauses with deep derivations to be less useful.

The remaining step is the translation of $\lambda$-abstractions. We implemented two
translation methods:\ $\lambda$-lifting \cite{tj-1985-lambdalift} and
$\cst{SKBCI}$ combinators \cite{da-1979-combtrans}. For
$\cst{SKBCI}$, we omit the combinator definition axioms, because they are
very explosive \cite{br-20-full-sup-w-combs}. A third mode simply omits clauses
containing $\lambda$-abstractions.

\ourpara{Evaluation and Discussion}
  %
  % \begin{enumerate}
  %   \item Different points when E is called, for different time periods
  %   \item Number of clauses from $M_\mathrm{ho}$ that is translated
  %   \item Different kinds of liftings
  % \end{enumerate}

  % We evaluated the following parameters of backend collaboration: the moment of
  % backend invocation, size of $M_\mathrm{ho}$, and $\lambda$-abstraction
  % translation method.
  In Zipperposition, we can adjust the CPU time allotted to Ehoh, Ehoh's own
  parameters, the point when Ehoh is invoked, the number \ParamNumClauses~of selected clauses from
  $M_\mathrm{ho}$, and the $\lambda$ translation method. We fix the time
  limit to 3~s, use Ehoh in \emph{autoschedule} mode, and focus on the last three
  parameters. In \emph{base}, collaboration with Ehoh is
  disabled (labeled $-$Ehoh).
  %Parameters are evaluated by enabling the collaboration with E and
  %varying parameter values.  -- Uninformative. --JB

  \begin{figure}[t]
    \noindent\hbox{}\hfill
    \begin{minipage}[t]{.46\linewidth}
      \centering
      \def\arraystretch{1.1}%
      \relax{\begin{tabular}{@{}l@{\hskip 0.75em}c@{\hskip 0.75em}c@{\hskip 0.75em}c@{\hskip 0.75em}c@{\hskip 0.75em}c@{}} \toprule
            & $-$Ehoh                   & $0.1$              & $0.25$          & $0.5$               & $0.75$      \\ \midrule
      TPTP & 1635$^\star$               & {\bf 1981}         & 1980            & 1979                & 1972 \\[0.5\jot]
      SH   & \colalign452$^\star$       & \colalign606       & {\bf \colalign608}    & \colalign600        & \colalign592 \\ \bottomrule
      \end{tabular}}
      \caption{Impact of the backend invocation point $\ParamTime$}
      \label{fig:invocation}
    \end{minipage}\hfill\hfill
    \begin{minipage}[t]{.46\textwidth}
      \centering
      \def\arraystretch{1.1}%
      \relax{\begin{tabular}{@{}l@{\hskip 0.75em}c@{\hskip 0.75em}c@{\hskip 0.75em}c@{\hskip 0.75em}c@{}} \toprule
           & $-$Ehoh              & lifting              & \textsf{SKBCI}    & omitted \\ \midrule
      TPTP & 1635$^\star$         & {\bf 1980}           & 1877              & 1866 \\[0.5\jot]
      SH   & \colalign452$^\star$ & \colalign{\bf 608}   & \colalign577      & \colalign566 \\ \bottomrule
      \end{tabular}}
      \caption{Impact of the method used to translate $\lambda$-abstractions}
      \label{fig:translation}
    \end{minipage}%
    \hfill\hbox{}
  \end{figure}

  \begin{figure}[t]
    \centering
    \def\arraystretch{1.1}%
    \relax{\begin{tabular}{@{}l@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{}}\toprule
          & $-$Ehoh              & $16$               & $32$               & $64$           & $128$             & $256$         & $512$     \\ \midrule
     TPTP & 1635$^\star$         & {\bf 1985}         & 1980               & 1978           & 1968              & 1968          & 1919 \\[0.5\jot]
     SH   & \colalign452$^\star$ & \colalign606       & {\bf \colalign608} & \colalign600   & \colalign598      & \colalign596  & \colalign589 \\ \bottomrule
    \end{tabular}}
    \caption{Impact of the number of selected clauses~$\ParamNumClauses$}
    \label{fig:m-ho-size}
  \end{figure}

  \looseness=-1
  Ehoh is invoked after $ \ParamTime \cdot t $ CPU seconds, where $0 \le \ParamTime < 1$ and $t$
  is the total CPU time allotted to Zipperposition. Figure~\ref{fig:invocation}
  shows the effect of varying $\ParamTime$ when $\ParamNumClauses = 32$
  and $\lambda$-lifting is used. The evaluation confirms that using a highly
  optimized backend such as Ehoh greatly improves the performance of a
  less optimized prover such as Zipperposition.
  The figure indicates that it is preferable to invoke the backend early. We
  have indeed observed that if the backend is invoked late, small clauses with
  deep derivations tend to be present by then. These clauses might have been
  used to delete important shallow clauses already. But due to their derivation
  depth, they will not be translated. In such situations, it is better to
  invoke the backend before the important clauses are deleted.

  Figure~\ref{fig:translation} quantifies the effects of the three
  $\lambda$-abstraction translation methods. We fixed $\ParamTime = 0.25$ and
  $\ParamNumClauses=32$. The clear winner is $\lambda$-lifting.
  $\cst{SKBCI}$ combinators perform slightly better than omitting clauses
  containing $\lambda$-abstractions.

  \looseness=-1
  Figure~\ref{fig:m-ho-size} shows the effect of $\ParamNumClauses$ on
  performance, with $\ParamTime = 0.25$ and $\lambda$-lifting. Including a
  small number of higher-order clauses with the lowest weight performs better
  than including a large number of such clauses.

\section{Comparison with Other Provers}
\label{sec:satfol:ho-tech:comparison}

%The raw evaluation data for the previous experiments show that
Different choices of
parameters lead to noticeably different sets of proved problems. In an attempt
to use Zipperposition~2 to its full potential, we have created a portfolio mode
that runs up to 50 configurations in parallel during the allotted time. The portfolio
was designed to solve as many problems as possible from the TPTP benchmark set. To
provide some context, we compare Zipperposition~2 with the latest versions of
all higher-order provers that competed at CASC-J10:\ CVC4 1.9
\cite{cbetal-11-cvc4}, Leo-III 1.5.6 \cite{sb-21-leo3}, Satallax 3.5
\cite{cb-2013-satallax}, and Vampire 4.5.1 \cite{br-20-full-sup-w-combs}.
The provers were run using the same parameters as in CASC, but with updated
executables.
Note that
Vampire's higher-order schedule is optimized for running on a single core.
We also include E~2.7 (more precisely, Ehoh, its higher-order configuration), the first version of this prover to syntactically support
full higher-order logic, including $\lambda$-abstractions.
Semantically, E~2.7 is arguably the weakest among the listed provers:
It simply performs $o$-RW rewriting described in Sect.~\ref{sec:satfol:ho-tech:preprocessing} followed by
$\lambda$-lifting before it applies \relax{$\lambda$-free superposition} \cite{bbcw-21-lfho}
(a precursor of all three higher-order superposition calculi) on the preprocessed problem.

We use the same benchmark sets as elsewhere in this article. To imitate the
CASC-J10 setup, we use a 120~s wall-clock limit and a 960~s CPU limit.
We even carried out our evaluation on the 8-core CPU nodes that were used for
CASC-J10. We also ran Zipperposition in uncooperative mode, in which its
collaboration with a backend is disabled. Figure~\ref{fig:other-provers}
summarizes the results.

\begin{figure}[t]
  \centering
  \def\arraystretch{1.1}%
  \relax{\begin{tabular}{@{}l@{\hskip 2.5em}c@{\hskip 2em}c@{}} \toprule
                  & TPTP    & SH  \\ \midrule
   CVC4           & 1816    & 587   \\
   E              & 1980    & 676   \\
   Leo-III        & 2122    & 616  \\
   Satallax       & 2175    & 588  \\
   Vampire        & 2072    & 660   \\
   Zipperposition-uncoop & 2311 & 652 \\
   Zipperposition & {\bf 2412}    & {\bf 715}  \\ \bottomrule
  \end{tabular}}
  \caption{Comparison of competing higher-order theorem provers}
  \label{fig:other-provers}
\end{figure}

% Among the cooperative provers, Zipperposition is the one that depends the least
% on its backend, and its \emph{uncooperative} mode is only \NumberNOK{one}~problem
% behind Satallax's \emph{cooperative} mode. This confirms our hypothesis that
% \osup{} is a suitable basis for automatic higher-order reasoning.
% \confrep{This also}{The increase in performance due to the addition of an efficient backend}
% suggests that the implementation of this calculus in a modern first-order
% superposition prover such as E or Vampire would achieve markedly better results.
% Moreover, we believe that there are still techniques inspired by tableaux, SAT
% solving, and SMT solving that could be adapted and integrated in saturation
% provers.

The evaluation results corroborate the CASC results. They also
show that Zipperposition outperforms all other provers on SH benchmarks. This
confirms our hypothesis that \lsup{} is a suitable basis for automatic
higher-order reasoning. Further confirmation is provided by the success rate of
Zipperposition's uncooperative version: Even without backend,
Zipperposition is substantially better than all other provers on TPTP
benchmarks, and it matches the performance of the top contenders on SH.
On the other hand, the increase in performance due to the addition
of an efficient backend suggests that the implementation of this calculus in a
modern first-order superposition prover such as E or Vampire would achieve
even better results.

We believe that there are still techniques inspired by
tableaux, SAT solving, and SMT solving that could be adapted and integrated in
saturation provers. In particular, there are still \NumberOK{25} TPTP problems
and \NumberOK{17} SH problems that can be proved by other provers but not by
Zipperposition.

% For VSCode
% !TEX root = ./conf.tex

%%% TODO @JASMIN @PETAR: The definition of flat L-resolvent is broken because
%%% of the condition on the variables. --JB
%%% TODO @JASMIN @PETAR: The definition of "tautology" is probably wrong for
%%% BCE. Cf. Kiesl et al.

%%% TODO @PETAR: Check @PETAR below. --JB

%\def\orcid#1{} %{\href{http://orcid.org/#1}{\protect\raisebox{-1.25pt}{\protect\includegraphics{orcid.pdf}}}}}
%TODO: uncomment before submission:
% \def\orcid#1{\orcidID{#1}}

% \bibliographystyle{splncs04}
% \usepackage{amsthm}
% \usepackage{ntheorem}
\usepackage{flushend}
\usepackage{cite}
\usepackage{etoolbox}
\usepackage{dcolumn}
\usepackage{algorithmicx}
\usepackage{regexpatch}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage[noend]{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage[mathscr]{eucal}
\usepackage{prftree}
\usepackage[utf8]{inputenc}
% \usepackage[T1]{fontenc}
% \usepackage[english]{babel}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{tikz}
\usepackage{pgfplots} 
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{subcaption}
%\usepackage{caption}
\usepackage{booktabs}
\usepackage[nomessages]{fp}
\usepackage{enumerate}
%\captionsetup{justification=raggedright,singlelinecheck=false}
\xpatchcmd{\algorithmic}{\labelsep 0.5em}{\labelsep 3em}{\typeout{Success!}}{\typeout{Oh dear!}}

%% BEGIN Times font etc.
\usepackage{mathptmx}
\usepackage[scaled=.82]{beramono}
\usepackage[scaled=.865]{helvet}
%\usepackage{avant}
\DeclareSymbolFont{letters}{OML}{txmi}{m}{it}
%% END Times font etc.

% Used for displaying a sample figure. If possible, figure files should be
% included in EPS format.

%\usepackage[
%   a4paper,
%   pdftex,
%   pdftitle={SAT-Inspired Eliminations for Superposition},
%   pdfauthor={Petar Vukmirovi\'c, Jasmin Blanchette, and Marijn J.H. Heule},
%   pdfkeywords={},
%   pdfborder={0 0 0},
%   draft=false,
%   colorlinks,linkcolor=blue,citecolor=blue,urlcolor=blue,
%   bookmarksnumbered,
%   bookmarks,
%   bookmarksdepth=2,
%   bookmarksopenlevel=2,
%   bookmarksopen]{hyperref}
% If you use the hyperref package, please uncomment the following line to
% display URLs in blue roman font according to Springer's eBook style:
%\renewcommand\UrlFont{\color{blue}\rmfamily}

\allowdisplaybreaks

% \usepackage[mathlines]{lineno}
% \linenumbersep=30pt
% \linenumbers

% \newcommand*\patchAmsMathEnvironmentForLineno[1]{%
%   \expandafter\let\csname old#1\expandafter\endcsname\csname #1\endcsname
%   \expandafter\let\csname oldend#1\expandafter\endcsname\csname end#1\endcsname
%   \renewenvironment{#1}%
%%      {\linenomath\csname old#1\endcsname}%
%%      {\csname oldend#1\endcsname\endlinenomath}}% 
% \newcommand*\patchBothAmsMathEnvironmentsForLineno[1]{%
%   \patchAmsMathEnvironmentForLineno{#1}%
%   \patchAmsMathEnvironmentForLineno{#1*}}%
% \AtBeginDocument{%
% \patchBothAmsMathEnvironmentsForLineno{equation}%
% \patchBothAmsMathEnvironmentsForLineno{align}%
% \patchBothAmsMathEnvironmentsForLineno{flalign}%
% \patchBothAmsMathEnvironmentsForLineno{alignat}%
% \patchBothAmsMathEnvironmentsForLineno{gather}%
% \patchBothAmsMathEnvironmentsForLineno{multline}%
% }


% \usepackage{etoolbox} %% <- for \cspreto, \csappto, \patchcmd, \pretocmd, \apptocmd

% %% Patch 'normal' math environments:
%% \newcommand*\linenomathpatch[1]{%
%%   \cspreto{#1}{\linenomath}%
%%   \cspreto{#1*}{\linenomath}%
%%   \cspreto{end#1}{\endlinenomath}%
%%   \cspreto{end#1*}{\endlinenomath}%
% }
% %% Patch AMS math environments:
%% \newcommand*\linenomathpatchAMS[1]{%
%%   \cspreto{#1}{\linenomathAMS}%
%%   \cspreto{#1*}{\linenomathAMS}%
%%   \csappto{end#1}{\endlinenomath}%
%%   \csappto{end#1*}{\endlinenomath}%
% }

%% %% Definition of \linenomathAMS depends on whether the mathlines option is provided
%% \expandafter\ifx\linenomath\linenomathWithnumbers
%%   \let\linenomathAMS\linenomathWithnumbers
%   %% The following line gets rid of an extra line numbers at the bottom:
%%   \patchcmd\linenomathAMS{\advance\postdisplaypenalty\linenopenalty}{}{}{}
% \else
%%   \let\linenomathAMS\linenomathNonumbers
% \fi

%% \linenomathpatch{equation}
%% \linenomathpatchAMS{gather}
%% \linenomathpatchAMS{multline}
%% \linenomathpatchAMS{align}
%% \linenomathpatchAMS{alignat}
%% \linenomathpatchAMS{flalign}


\title{SAT-Inspired Eliminations for~Superposition}


\author{
\IEEEauthorblockN{Petar Vukmirovi\'c\textsuperscript{1}\orcid{0000-0001-7049-6847},
Jasmin Blanchette\textsuperscript{1,2}\orcid{0000-0002-8367-0936},
Marijn J.H. Heule\textsuperscript{3}\orcid{0000-0002-5587-8801}}\\
\IEEEauthorblockA{%
\textsuperscript{1}Vrije Universiteit Amsterdam, Amsterdam, the Netherlands\\ 
\textsuperscript{2}Universit\'e de Lorraine, CNRS, Inria, LORIA, Nancy, France \\
\textsuperscript{3}Carnegie Mellon University, Pittsburgh, Pennsylvania, United States}
}

% \author{
% \IEEEauthorblockN{Petar Vukmirovi\'c\orcid{0000-0001-7049-6847}}
% \IEEEauthorblockA{Vrije Universiteit Amsterdam}
% \and
% \IEEEauthorblockN{Jasmin Blanchette\orcid{0000-0002-8367-0936}}
% \IEEEauthorblockA{
% Vrije Universiteit Amsterdam, \\
% Max-Planck-Institut f\"ur Informatik, \\
% Universit\'e de Lorraine}
% \and
% \IEEEauthorblockN{Marijn J.H. Heule\orcid{0000-0002-5587-8801}}
% \IEEEauthorblockA{Carnegie Mellon University}
% }

% mathcal
\DeclareFontFamily{OT1}{pzc}{}
\DeclareFontShape{OT1}{pzc}{m}{it}{<-> s * [1.10] pzcmi7t}{}
\DeclareMathAlphabet{\mathcalx}{OT1}{pzc}{m}{it}
%\DeclareMathAlphabet{\mathcalx}{OMS}{zplm}{m}{n}

%\hyphenation{pre-process-ing in-process-ing}

\begin{document}

% \newcommand\ourpara[1]{\noindent{\textbf{\upshape#1.}}}
\newcommand\ourpara[1]{\subsection{#1}}

%% TYPESETTING: hacks
\newcommand\medrightarrow{\mathrel{{{\color{black}\relbar}\kern-0.9ex\rlap{\color{white}\ensuremath{\blacksquare}}\kern-0.9ex}\joinrel{\color{black}\rightarrow}}}
\newcommand\medleftarrow{\mathrel{{\color{black}\leftarrow}\kern-0.9ex\rlap{\color{white}\ensuremath{\blacksquare}}\kern-0.9ex\joinrel{{\color{black}\relbar}}}}
\newcommand\medleftrightarrow{\mathrel{\leftarrow\kern-1.685ex\rightarrow}}
\newcommand\Medrightarrow{\mathrel{{{\color{black}\Relbar}\kern-0.9ex\rlap{\color{white}\ensuremath{\blacksquare}}\kern-0.9ex}\joinrel{\color{black}\Rightarrow}}}
\newcommand\Medleftrightarrow{\mathrel{\Leftarrow\kern-1.685ex\Rightarrow}}

\undef\leftrightarrow %% use \medleftrightarrow instead for iff

\newcommand\scif{\quad\text{if~}}
\newcommand{\HL}{\ensuremath{\mathrm{HL}}}
\newcommand{\PHL}{\ensuremath{\mathrm{HL_\mathrm{p}}}}
\newcommand{\cst}[1]{{\mathsf{#1}}}
\newcommand{\eq}{\mathbin{\approx}}
\newcommand{\noteq}{\mathbin{\not\approx}}
\newcommand{\seq}[1]{\ensuremath{(#1)_{n=1}^{\infty}}}
\newcommand{\impk}[4]{\ensuremath{#1 \impl^{#3}_{#4} #2}}
\newcommand{\infname}[1]{\textsc{#1}}
\newcommand{\namedsimp}[3]{\prftree[d][r]{\relax{\infname{#1}}}{\strut#2}{\strut#3}}
\newcommand{\namedsimpsc}[4]{\prftree[d][r]{\relax{\infname{#1}\scif $#4$}}{\strut#2}{\strut#3}}

\newcommand{\impl}{\ensuremath{\medrightarrow}}
% \newcommand{\bigpimpl}[1]{\ensuremath{\overset{#1}{\rightharpoonup}}}
% \newcommand{\bigfimpl}[1]{\ensuremath{\overset{#1}{\hookrightarrow}}}
\newcommand{\bigpimpl}[1]{\hookrightarrow_\mathrm{p}}
\newcommand{\bigfimpl}[1]{\hookrightarrow}
\newcommand{\eqlit}[2]{\ensuremath{#1 \eq #2}}
\newcommand{\neqlit}[2]{\ensuremath{#1 \noteq #2}}
% \newcommand{\eqneqlit}[2]{\ensuremath{#1 \mathrel{\dot{\eq}} #2}}
\newcommand{\pospredlit}[1]{{#1}}
\newcommand{\negpredlit}[1]{\neglit{#1}}
\newcommand{\neglit}[1]{\neg{\kern.2ex #1}}
\newcommand{\arbpredlit}[1]{(\neg)\kern.2ex\pospredlit{#1}}
\newcommand{\cor}{\ensuremath{\mathrel{\lor}}}
\newcommand{\tuple}[2]{\ensuremath{\vec{#1}_{#2}}}
\newcommand{\tuplen}[1]{\tuple{#1}{n}}
\newcommand{\tupleempty}[1]{\tuple{#1}{}}
\newcommand{\flatres}{\mathbin{\smash{\hbox{\Large$\kern-.1ex\rtimes$}}}}  %% (slightly) asymmetric symbol
\newcommand{\flatresiter}{\mathbin\leadsto}
\newcommand{\withoutpred}[2]{\,\overline{\!#1}_{\!#2}}
\newcommand{\withpred}[2]{#1_{#2}}
\newcommand{\withpredpos}[2]{#1^+_{#2}}
\newcommand{\withpredneg}[2]{#1^-_{#2}}
\newcommand{\binset}[1]{#1{\downarrow}_2}
\newcommand\NumberOK[1]{#1}
\newcommand\NumberNOK[1]{\colorbox{red}{#1}}
\newcolumntype{d}{D{.}{.}{0}}
\newcommand{\confrep}[2]{\begin{conf}#1\end{conf}\begin{rep}#2\end{rep}}

\definecolor{light-gray}{gray}{0.925}
\newcommand\MAX[1]{\smash{\setlength{\fboxsep}{.3ex}\colorbox{light-gray}{\ensuremath{\vphantom{('q}{#1}}}}}


\newtheorem{theorem}{Theorem}{\bfseries}{\slshape}
\newtheorem{example}[theorem]{Example}{\bfseries}{\rmfamily}
\newtheorem{remark}[theorem]{Remark}{\bfseries}{\rmfamily}
\newtheorem{definition}[theorem]{Definition}{\bfseries}{\rmfamily}
\newtheorem{lemma}[theorem]{Lemma}{\bfseries}{\slshape}
\newtheorem{corollary}[theorem]{Corollary}{\bfseries}{\slshape}
% \newtheorem*{proof}{Proof}{\slshape}{\rmfamily}

\newlength{\myskip}
\setlength{\myskip}{0.6ex}
% TYPESETTING: when two environments are next to
% each other, we need to somehow squeeze them
\newcommand{}{\vspace{-\myskip}}

\newenvironment{theorem}% environment name
{% begin code
  \vspace{\myskip}
  \begin{theorem}
}%
{\end{theorem}
 \vspace{\myskip}}% end code

\newenvironment{exa}% environment name
{% begin code
  \vspace{\myskip}
  \begin{example}
}%
{\end{example}
 \vspace{\myskip}}% end code

\newenvironment{remarkx}% environment name
{% begin code
  \vspace{\myskip}
  \begin{remark}
}%
{\end{remark}
 \vspace{\myskip}}% end code

\newenvironment{defi}% environment name
 {% begin code
   \vspace{\myskip}
   \begin{definition}
 }%
 {\end{definition}
  \vspace{\myskip}}% end code

\newenvironment{lemma}% environment name
{% begin code
\vspace{\myskip}
\begin{lemma}
}%
{\end{lemma}
\vspace{\myskip}}% end code

\newenvironment{corollaryx}% environment name
{% begin code
\vspace{\myskip}
\begin{corollary}
}%
{\end{corollary}
\vspace{\myskip}}% end code
 


\newenvironment{proof}% environment name
{% begin code
  \vspace{\myskip}
  \textit{Proof:}%
}%
{\vspace{\myskip}}% end code

% Stolen from Jasmin's paper
\newcommand{\qed}{\hfill{\mbox{\ $\IEEEQED$}}}

\maketitle
\setcounter{footnote}{0}

\newcommand{\ourmodel}{\ensuremath{\mathscr{J}}}

\begin{abstract}
Optimized SAT solvers not only preprocess the clause set, they also transform it
during solving as inprocessing. Some preprocessing techniques have been
generalized to first-order logic with equality. In this \paper, we port
inprocessing techniques to work with superposition, a leading first-order proof
calculus, and we strengthen known preprocessing techniques. Specifically, we
look into elimination of hidden literals, variables (predicates), and blocked
clauses. Our evaluation using the Zipperposition prover confirms that the new
techniques usefully supplement the existing superposition machinery.
\end{abstract}

\section{Introduction}
\label{sec:satfol:introduction}

Automated reasoning tools have become much more powerful in the last few
decades thanks to procedures such as conflict-driven clause learning (CDCL)
\cite{MSLM09HBSAT} for propositional logic and superposition
\cite{bg-94-superposition} for first-order logic with equality. However,
the effectiveness of these procedures crucially depends on how the input
problem is represented as a clause set. The clause set
can be optimized beforehand (\emph{preprocessing}) or during the execution of
the procedure (\emph{inprocessing}). In this \paper, we lift several preprocessing and
inprocessing techniques from propositional logic to clausal first-order logic
and demonstrate their usefulness in a superposition prover.

For many years, SAT solvers have used inexpensive clause simplification
techniques such as hidden literal and hidden tautology elimination
\cite{hjb-2010-cl-elim,hjb-2011-big-simplification} and failed literal
detection \cite[Sect.~1.6]{jwf-1995-fld}. We generalize these techniques to
first-order logic with equality
(Sect.~\ref{sec:satfol:hidden-literal-based-elimination}). Since the generalization
involves reasoning about infinite sets of literals, we propose restrictions to
make them usable.

\emph{Variable elimination}, based on Davis--Putnam resolution \cite{dp-60-dp}, has
been studied in the context of both propositional logic
\cite{sp-04-niver,cs-00-zres} and quantified Boolean formulas (QBFs)
\cite{ab-2004-re}. The basic idea is to resolve all clauses with negative
occurrences of a propositional variable (i.e., a nullary predicate symbol) against
clauses with positive occurrences and delete the parent clauses. E\'en and
Biere \cite{eb-2005-satpreprocess} refined the technique to identify a subset
of clauses that effectively define a variable and use it to further optimize the
clause set. This latter technique, \emph{variable elimination by substitution},
has been an important preprocessor component in many SAT solvers since its
introduction in 2004.

Specializing second-order quantifier elimination
\cite{go-1992-so-pred-elim,hjo-1996-scan}, Khasidashvili and Korovin
\cite{kk-2016-pe-fol} adapted variable elimination to preprocess first-order
problems, yielding a technique we call \emph{singular predicate elimination}. We
extend their work along two axes (Sect.~\ref{sec:satfol:predicate-elimination}): We
generalize E\'en and Biere's refinement to first-order logic, resulting in
\emph{defined predicate elimination}, and explain how both types of predicate
elimination can be used during the proof search as inprocessing.

\begin{qle}
It is well known that clauses containing \emph{pure literals}---predicate
literals whose root symbol occurs only with one polarity in the problem---can
be eliminated. We propose a generalization of this technique, under the name
\emph{quasipure literal elimination}, to allow the elimination of some bipolar
literals as well (Sect.~\ref{sec:satfol:quasipure-literal-elimination}).
\end{qle}

The last technique we study is \emph{blocked clause elimination} (Sect.\
\ref{sec:satfol:satisfiability-by-clause-elimination}). It is used in both SAT
\cite{jbh-10-BCE} and QBF solvers \cite{bls-11-bloqqer}. Its generalization to first-order
logic has produced good results when used as a preprocessor, especially on
satisfiable problems \cite{ksstb-2017-blockedfol}. We explore more ways to use
blocked clause elimination on satisfiable problems, including using it to
establish equisatisfiability with an empty clause set or as an inprocessing
rule. Unfortunately, we find that its use as inprocessing can compromise the
refutational completeness of superposition.

% Preprocessing has been a crucial part of SAT solvers since 2004, when E\'en and
% Biere introduced bounded variable elimination by substitution~\cite{BVE}.
% They based it on Davis--Putnam resolution \cite{dp-60-dp}, which
% replaces all clauses containing a variable $x$ by all nontautological
% resolvents on $x$, but showed how to exploit definition information
% extracted form the clause set to add only a subset of the nontautological
% resolvents. In this way, many variables can be eliminated without
% growing the clause set. % (in terms of the number of clauses). ---that comes later --JB

% Preprocessing is generally effective, but it can be expensive. As a compromise,
% many solvers use inprocessing:\ interleaving the decision procedure with
% preprocessing steps~\cite{mhb-12-inprocessing}. Top-tier SAT solvers, such as CaDiCaL
% \cite{xxx} and Kissat \cite{xxx} alternate about every second between solving
% and inprocessing. In contrast to preprocessing, inprocessing can also be used
% to simplify the clause set after some important deductions have been made.

% Pre-~and inprocessing are successful beyond propositional logic. For example,
% lifting variable and clause elimination techniques to quantified Boolean
% formulas (QBF) helps solve many formulas that are too difficult in practice for
% decision procedures~\cite{bls-11-bloqqer}. This suggests that richer logics, such as
% first- and higher-order logic, can also benefit from elimination techniques.

% To improve the success rate of superposition provers, we extend several of
% preprocessing and inprocessing techniques from propositional to first-order logic,
% including failed literal detection, hidden literal elimination, hidden
% tautology elimination
% (Sect.~\ref{sec:satfol:hidden-literal-based-elimination}). These
% techniques are fairly inexpensive in propositional logic but need to be
% curtailed in first-order logic because they would otherwise involve reasoning
% about an infinite set of literals.

% Khasidashvili and Korovin \cite{kk-2016-pe-fol} generalized Davis--Putnam
% resolution to first-order logic with equality. We call their technique
% \emph{singular predicate elimination}. We also adapt bounded variable
% elimination by substitution, yielding \emph{defined predicate elimination}
% (Sect.~\ref{sec:satfol:predicate-elimination}). With some care, we can integrate these
% techniques with superposition without losing refutational completeness.

% Finally, we study a first-order generalization of blocked clause
% elimination~\cite{jbh-10-BCE} (Sect.~\ref{sec:satfol:satisfiability-by-clause-elimination}).
% %Unlike the other techniques, it weakens the clause set.
% If all clauses in the current clause set are blocked, this means that it is
% satisfiable.
% %It can be used to show
% %satisfiability by establishing equisatisfiability between the initial clause
% %set and the empty clause set.
% We show how this technique can help superposition provers detect
% satisfiability early instead of diverging. We also show, however, that
% interleaving the technique with superposition compromises refutational
% completeness.

All techniques are implemented in the Zipperposition prover
(Sect.~\ref{sec:satfol:implementation}), allowing us to ascertain their usefulness
(Sect.~\ref{sec:satfol:evaluation}). The best configuration solves \NumberOK{160}
additional problems on benchmarks consisting of all 13\,495 first-order TPTP theorems
\cite{gs-17-tptp}. The raw experimental data are publicly available.%
\footnote{\url{https://doi.org/10.5281/zenodo.4552499}}\confrep{
More details, including all the proofs, can be found in a
technical report \cite{our-report}.}{}

\begin{rep}
This \paper{} expands on our FMCAD 2021 paper \cite{fmcad}. It includes more
examples, lemmas, and proofs. It also corrects a mistake in the handling of
variables in Definition~\ref{def:equality-blocked}.
\end{rep}

\section{Preliminaries}
\label{sec:satfol:preliminaries}

\ourpara{Clausal First-Order Logic}
\looseness=-1
Our setting is many-sorted, or many-typed, first-order logic \cite{jg-1987-logic-textbook} with
interpreted equality and a distinguished type (or sort) $o$.
Each variable $x$ is assigned a non-Boolean
type, and each symbol $\cst{f}$ is assigned a tuple $(\tau_1, \ldots, \tau_n,
\tau)$ where $n \geq 0$, $\tau_i$ are non-Boolean types, and $\tau$ is
the \emph{result type}. We distinguish between
\emph{predicate symbols}, with $o$ as the result type, and \emph{function symbols}.
Nullary function symbols are called \emph{constants}. Terms are either
variables~$x$ or \confrep{well-typed }{}applications $\cst{f}(t_1, \ldots,
t_n)$\confrep{, or $\cst{f}$ if $n = 0.$}{, of type $\tau$ where $\cst{f}$ is assigned $(\tau_1, \ldots,
\tau_n, \tau)$, and each $t_i$ is a term of type $\tau_i$.
The parentheses are omitted if $n = 0$.} A term is
\emph{ground} if it contains no variables.
We assume standard definitions and
notations for positions, subterms, and contexts \cite{bn-98-tr-and-all-that}.
We abbreviate a vector $(a_1, \ldots, a_n)$ to $\tuplen{a}$ or $\tuple{a}{}$,
and write $\cst{f}^i(s)$ for the $i$-fold application of an unary symbol $\cst{f}$
(e.g., $\cst{f}^3(x) = \cst{f}(\cst{f}(\cst{f}(x)))$).

An atom is an equation $\eqlit{s}{t}$ corresponding to an unordered pair $\{s,
t\}$. A literal is an equation $\eqlit{s}{t}$ or a disequation $\neqlit{s}{t}$.
For every predicate symbol~$\cst{p}$, \confrep{}{the notation }$\cst{p}(\tuple{s}{})$
abbreviates $\eqlit{\cst{p}(\tuple{s}{})}{\top}$, and
$\neglit{\cst{p}(\tuple{s}{})}$ abbreviates
$\neqlit{\cst{p}(\tuple{s}{})}{\top}$, where $\top$ is a distinguished
constant of type $o$. We distinguish between \emph{predicate literals}
$\arbpredlit{\cst{p}(\tuple{s}{})}$ and \emph{functional literals} $\eqlit{s}{t}$,
where $s$ and $t$ are not of type $o$.
Given a literal $L$, we overload notation and write $\neglit{L}$ to
denote its complement. A clause $C$ is a multiset of literals, written as $L_1
\cor \cdots \cor L_n$ and interpreted disjunctively. Clauses are often defined
as sets of literals, but superposition needs multisets; with multisets,
an instance $C\sigma$ always has the same number of literals as $C$, a most
convenient property. Given a clause set $N$, $\binset{N}$ denotes the subset of
its binary clauses: $\binset{N} = \{ L_1 \cor L_2 \mid L_1 \cor L_2 \in N \}$.

\begin{rep}
A substitution $\sigma$ is a well-typed mapping from
variables to terms such that the set $\{x \mid \sigma(x) \not= x \}$ is finite.
%Assuming $x_i \not= t_i$,
Substitutions are written as $\{x_1 \mapsto t_1,
\ldots,\allowbreak x_n \mapsto t_n\}$ or $\{ \tupleempty{x} \mapsto \tupleempty{t} \}$.%
\confrep{}{ A substitution is a \emph{variable renaming} if it is a
bijection from a set of variables to a set of variables.}

We assume the natural extensions of domain, valuation, interpretation and model
(as defined by Fitting \cite{mf-1996-fol}) from unsorted to many-sorted
logic.
Models we consider are \emph{normal}, i.e., they interpret $\eq$ as an
equality relation. Usual notions of (un)satisfiability and (in)validity are
assumed. We write $\ourmodel~\models_\xi~N$ to denote that a model $\ourmodel$
satisfies a clause set $N$, for a variable assignment $\xi$. If $\ourmodel$
is a model of $N$ (i.e., it satisfies it under every variable assignment), 
we simply write $\ourmodel \models N.$  Abusing notation,
we write $M \models N$ to denote that $M$ \emph{entails} $N$, i.e., that every
model of $M$ is a model of $N.$

A canonical model $\ourmodel$ is a normal model such that for every element $d$
in one of $\ourmodel$'s domains there exists a ground term $t$ such that
$\ourmodel$ interprets $t$ as $d$. Canonical models generalize Herbrand models
to first-order logic with equality: If a clause set is satisfiable, then it
is satisfiable in a canonical model.
\end{rep}

\ourpara{Superposition Provers}
%
Superposition \cite{bg-94-superposition} is a calculus for clausal
first-order logic that extends ordered resolution
\cite{bg-01-resolution} with equality reasoning. It is
refutationally complete: Given a finite, unsatisfiable clause set, it will
eventually derive the empty clause. It is parameterized by a \emph{selection
function} that influences which of a clause's literals are eligible as the target
of inferences. Moreover, it is compatible with the \emph{standard redundancy
criterion}, which can be used to delete a clause $C$ while preserving completeness of the calculus.

\looseness=-1
The redundancy criterion relies on an order $\succ$ that
compares terms, literals, or clauses.
The order is used to determine whether clauses can be deleted.
If $N$ is ground, $C$ can be deleted
if it is entailed by $\prec$-smaller clauses in $N.$ This definition is lifted
to nonground sets $N$. The criterion can be used to delete a
clause that is \emph{subsumed} by another clause (e.g.,
$\cst{\cst{p}(\cst{a})} \cor \cst{q}$ by $\cst{p}(x)$) or
to \emph{simplify} a clause $C$ into $C'$, which amounts to adding $C'$ and
then deleting $C$ as redundant with respect to $N \cup \{C'\}$.
Subsumption and simplification are the main inprocessing mechanisms available
to superposition provers. Some provers also implement clause splitting
\cite{riazanov-voronkov-2001,fietzke-weidenbach-2009,av-2014-avatar}.

Superposition provers saturate the input problem with respect to the calculus's
inference rules using the \emph{given clause procedure}
\cite{mcw-1997-otter,adf-1995-discount}. It partitions the proof state into a
passive set $\mathcalx{P}$ and an active set $\mathcalx{A}$. All clauses start in
$\mathcalx{P}$. At each iteration of the procedure's main loop, the prover
chooses a clause $C$ from $\mathcalx{P}$, simplifies it, and moves it to
$\mathcalx{A}$. Then all inferences between $C$ and active clauses are performed.
The resulting clauses are again simplified and put in $\mathcalx{P}$.\confrep{}{
The provers differ in which clauses are used for simplification: Otter-loop
\cite{mcw-1997-otter} provers use both active and passive clauses whereas
DISCOUNT-loop \cite{adf-1995-discount} provers use only active clauses.}

\section{Hidden-Literal-Based Elimination}
\label{sec:satfol:hidden-literal-based-elimination}

In propositional logic, binary clauses from a clause set $N$ can be used to
efficiently discover literals $L, L'$ for which the implication $L' \impl L$
is entailed by $N$'s binary clauses---i.e., $\binset{N} \models L' \impl L$.
Heule et al.\ \cite{hjb-2011-big-simplification} introduced the concept of \emph{hidden
literals} to capture such implications.

\begin{defi}
   Given a propositional literal $L$ and a propositional clause set~$N$, the
   set of \emph{propositional hidden literals} for $L$ and~$N$ is
   $\PHL(L, N) = \{ L' \mid L' \bigpimpl{N}^\ast L \} \setminus \{ L \}$, where $\bigpimpl{N}$ is
   defined such that $\neglit{L_1} \bigpimpl{N} L_2$ whenever $L_1 \cor L_2 \in N.$
   Moreover, $\PHL(L_1 \cor \cdots \cor L_n, N) = \bigcup_{i=1}^n
   \PHL(L_i, N)$.
\end{defi}

Heule et al.\ used a fixpoint computation, but our definition based on the
reflexive transitive closure is equivalent. Intuitively, a hidden literal can be added to
or removed from a clause without affecting its semantics in models of~$N.$
By eliminating hidden literals from $C$, we simplify it. By adding hidden literals to $C$, we might get a tautology $C'$ (i.e., a valid clause:\ $\models
C'$), meaning that $\binset{N} \models C$, thereby enabling us to delete $C$.
Note that $\PHL(L,N)$ is finite for a finite~$N.$

\begin{defi}
   \label{def:hle}\label{def:hte}
   Given $L' \cor L \cor C \in N,$ if $L' \in
   \PHL(L, N)$, \emph{hidden literal elimination} (HLE) replaces $N$ by $(N
   \setminus \{L' \cor L \cor C\}) \cup \{L \cor C\}$.
   Given $C \in N$, $\{L_1,\dots,L_n\} = \PHL(C, N)$, and $C' = C \cor L_1 \cor
   \cdots \cor L_n$, if $C'$ is a tautology, \emph{hidden
   tautology elimination} (HTE) replaces $N$ by $N \setminus \{C\}$.
\end{defi}

\begin{theorem}
   The result of applying HLE or HTE to a clause set $N$ is
   equivalent to $N$.
\end{theorem}

\begin{proof}
For HLE, if $L' \in \PHL(L, N)$,  $\binset{N} \models \neglit{L'} \cor L$.
Then, subsumption resolution yields shortened clause $L \cor C'$ from
Definition~\ref{def:hle}. For HTE, it can be shown that $N' \models C$ if and
only if $C \cor L'$, where $L' \in \PHL(C, N)$. By transitivity of equivalence,
we get the desired result.
\qed
\end{proof}

We generalize hidden literals to first-order logic with equality by
considering substitutivity of variables as well as congruence of equality.

\begin{defi}
\label{def:hl-fo}
Given a literal $L$ and a clause set $N$,
the set of \emph{hidden literals} for $L$ and $N$
is $\HL(L,N) = \{ L' \mid L' \bigfimpl{N}^\ast L \} \setminus \{ L \}$, where
$\bigfimpl{N}$ is defined so that
(1)~$\neglit{L'\sigma} \bigfimpl{N} L\sigma$ if
$L' \cor L \in N$ and $\sigma$ is a substitution;
(2)~$\eqlit{s}{t} \bigfimpl{N} \eqlit{u[s]}{u[t]}$ for all terms $s, t$ and
contexts $u[\phantom{\cdot}]$; and
(3)~$\neqlit{u[s]}{u[t]} \bigfimpl{N} \neqlit{s}{t}$ for all terms $s, t$ and
contexts $u[\phantom{\cdot}]$.
Moreover, $\HL(L_1 \cor \cdots \cor L_n, N) = \bigcup_{i=1}^n \HL(L_i, N)$.
\end{defi}

\looseness=-1
The generalized definition also enjoys the key property that $L'
\in \HL(L, N)$ implies $\binset{N} \models L' \impl L$. However, $\HL(L, N)$
may be infinite even for predicate literals; for example,
$\pospredlit{\cst{p}(\cst{f}^i(x))} \in \HL(\pospredlit{\cst{p}(x)}, \{\pospredlit{\cst{p}(x)} \cor
\negpredlit{\cst{p}(\cst{f}(x))} \})$ for every $i$.

Based on Definition \ref{def:hl-fo}, we can generalize hidden literal elimination
and support a related technique:
%
\begin{align*}
   \vcenter{\namedsimp{HLE}{L' \cor L \cor C}{L \cor C}}
   & \scif {L' \in \HL(L,N)} \\[.5\jot]
   \vcenter{\namedsimp{\rlap{FLE}\phantom{HLE}}{L \cor C}{C}}
   & \scif {L',\neglit{L'} \in \HL(\neglit{L}, N)}
\end{align*}
%
Double lines denote \emph{simplification rules}: When the premises appear in
the clause set, the prover can use the redundancy criterion to replace
them by the conclusions. The second rule is called \textit{failed literal
elimination},
inspired by the SAT technique of asserting $\neglit{L}$ if $L$ is a \textit{failed literal} \cite{jwf-1995-fld}.
It is easy to see that rule \infname{HLE} is sound.
From $L' \in \HL(L,N)$ we have $N \models L' \impl L$ (i.e., $\neglit{L'} \cor L$).
Performing subsumption resolution \cite{bg-01-resolution} between $L'
\cor L \cor C$ and $\neglit{L'} \cor L$ yields the conclusion, which is
therefore entailed by $N.$ For \infname{FLE}, the condition $L',
\neglit{L'} \in \HL(\neglit{L},N)$ means that $\binset{N} \models \{\neglit{L'} \cor
\neglit{L}{,}\; L' \cor \neglit{L}\} \models \neglit{L}$.


\begin{exa}
   \label{example:hle}
   Consider the clause set $N = \{ \cst{p}(x) \cor \neglit{\cst{p}(\cst{f}(x))}{,}\;
   \cst{p}(\cst{f}(\cst{f}(x))) \cor \eqlit{\cst{a}}{\cst{b}} \}$ and the clause
   $C = \neqlit{\cst{f}(\cst{a})}{\cst{f}(\cst{b})} \cor \cst{p}(x)$. The first
   clause in $N$ induces $\cst{p}(\cst{f}(x)) \bigfimpl{N} {\cst{p}(x)}$,
   $\cst{p}(\cst{f}(\cst{f}(x))) \bigfimpl{N} \cst{p}(\cst{f}(x))$, and
   hence $\cst{p}(\cst{f}(\cst{f}(x))) \bigfimpl{N}^\ast  \cst{p}(x)$.
   Together with the second clause in $N$, it can be used to derive $\neqlit{\cst{a}}{\cst{b}}
   \bigfimpl{N}^\ast  \cst{p}(x)$. Finally, using rule~(3) of Definition
   \ref{def:hl-fo}, we derive $\neqlit{\cst{f}(\cst{a})}{\cst{f}(\cst{b})}
   \bigfimpl{N}^\ast  \cst{p}(x)$---that is,
   $\neqlit{\cst{f}(\cst{a})}{\cst{f}(\cst{b})} \in \HL(\cst{p}(x), N)$. This
   allows us to remove $C$'s first literal using \infname{HLE}.
\end{exa}

Two special cases of \infname{HLE} exploit equality congruence as embodied by
conditions (2)~and~(3) of Definition~\ref{def:hl-fo} without requiring to
compute the $\HL$~set:
\begin{align*}
  %% TYPESETTING: \,'s below
  \namedsimp{CongHLE$^+$}{\eqlit{s}{t} \cor \eqlit{u[s]}{u[t]} \cor C}{\eqlit{u[s]}{u[t]} \cor C}\,\, \\[.5\jot]
  \namedsimp{CongHLE$^-$}{\neqlit{s}{t} \cor \neqlit{u[s]}{u[t]} \cor C}{\neqlit{s}{t} \cor C}
\end{align*}%

Hidden literals can be combined with unit clauses~$L'$ to remove more literals:
\[\namedsimpsc{UnitHLE}{L' \quad L \cor C}{L' \quad C}{L'\sigma \in \HL(\neglit{L},N)}
%\text{~for some~}\sigma -- %%% Not necessary in a rule -- all variables are universally quantified. --JB
\]
%
Given a unit clause $L' \in N$, the rule uses it to discharge $L'\sigma$ in
$N \models L'\sigma \impl \neglit{L}$. As a result, we have $N \models
\neglit{L}$, making it possible to remove $L$ from $L \cor C$.

% Kiesl and Suda described first-order generalization of {\it asymmetric literals}
% \cite{ks-2017-unif-principle}: literal $L\sigma$ is asymmetric for a clause $C$
% and a clause set $N$ if there exists a clause $D \cor \neglit{L} \in N \setminus
% \{C \}$ such that $D\sigma \subseteq C$ for some substitution $\sigma$. 

\begin{exa}
   \label{example:unithle}
   \looseness=-1
   Consider the clause set $N = \{\cst{p}(x) \cor \cst{q}(\cst{f}(x)){,}\;
   \neglit{\cst{q}(\cst{f}(\cst{a}))} \cor \eqlit{\cst{f}(\cst{b})}{\cst{g}(\cst{c})}{,}\;
   \neqlit{\cst{f}(x)}{\cst{g}(y)}\}$ and the clause $C = \neglit{\cst{p}(\cst{a})} \cor
   \neglit{\cst{q}(\cst{b})}$. The first clause in $N$ induces
   $\neglit{\cst{q}(\cst{f}(\cst{a}))} \bigfimpl{N} \cst{p}(\cst{a})$, whereas
   the second one induces $\neqlit{\cst{f}(\cst{b})}{\cst{g}(\cst{c})} \bigfimpl{N}
   \neglit{\cst{q}(\cst{f}(\cst{a}))}$. Thus, we have $\neqlit{\cst{f}(\cst{b})}{\cst{g}(\cst{c})}
   \bigfimpl{N}^\ast \cst{p}(\cst{a})$---that is, $\neqlit{\cst{f}(\cst{b})}{\cst{f}(\cst{c})} \in
   \HL(\cst{p}(\cst{a}), N)$. By applying the substitution $\{x \mapsto \cst{b}{,}\allowbreak\; y
   \mapsto \cst{c}\}$ to the third clause in $N$, we can fulfill the conditions of
   \infname{UnitHLE} and remove $C$'s first literal.
\end{exa}

Next, we generalize hidden tautologies to first-order logic.

\begin{defi}
   A clause $C$ is a \emph{hidden tautology} for a clause set $N$ if there
   exists a finite set $\{L_1,\dots,L_n\} \subseteq \HL(C,N)$ such that
   $C \cor L_1 \cor \cdots \cor L_n$ is a tautology.
\end{defi}

\begin{exa}
   \label{example:hidden-tautologies-completeness}
   \looseness=-1
   In general, hidden tautologies are not redundant and cannot be deleted during saturation.
   Consider the unsatisfiable set $N = \{ \negpredlit{\cst{a}}{,}\;
   \negpredlit{\cst{b}}{,}\; \pospredlit{\cst{a}} \cor
   \pospredlit{\cst{c}}{,}\allowbreak\; \pospredlit{\cst{b}} \cor \negpredlit{\cst{c}}
   \}$, the order $\cst{a} \prec
   \cst{b} \prec \cst{c}$, and the empty selection function. The
   only possible superposition inference from $N$ is between the last two clauses,
   yielding the hidden tautology $\pospredlit{\cst{a}} \cor \pospredlit{\cst{b}}$
   (after simplifying away $\neqlit{\top}{\top}$), which is entailed by the larger
   clauses $\pospredlit{\cst{a}} \cor \pospredlit{\cst{c}}$ and
   $\pospredlit{\cst{b}} \cor \negpredlit{\cst{c}}$. If this clause is removed,
   the prover could enter an infinite loop, forever generating and deleting the
   hidden tautology\confrep{}{ and never getting the opportunity to derive the
   empty clause}.
\end{exa}

\begin{rep}
   \newcommand{\aset}{\mathcalx{A}}
   \newcommand{\pset}{\mathcalx{P}}
   
   In practice, most provers use a variant of the given clause procedure.
   Removing hidden tautologies breaks the invariant of the procedure that all
   inferences between clauses in $\aset$ are redundant. The end result is not
   that the prover diverges, but that it terminates without deriving empty
   clause.

   To observe this, assume the setting as in Example
   \ref{example:hidden-tautologies-completeness}, and let $\pset=N$ and
   $\aset=\emptyset$. After moving the first three clauses from $\pset$ to
   $\aset$ ($\aset = \{ \negpredlit{\cst{a}}, \negpredlit{\cst{b}}, \pospredlit{\cst{a}} \cor
   \pospredlit{\cst{c}} \}$, $\pset = \{\pospredlit{\cst{b}} \cor \negpredlit{\cst{c}}\}$), 
   no inferences are possible, and no new clauses are added to $\pset$. After the last clause
   is moved to $\aset$, the hidden tautology $\pospredlit{\cst{a}} \cor \pospredlit{\cst{b}}$
   is produced. If it is deleted, the prover terminates with the unsatisfiable set $\aset$,
   but does not derive the empty clause. 
\end{rep}

To delete hidden tautologies during saturation, the prover could check that all
the relevant clause instances encountered along the computation of $\HL$ are
$\prec$-smaller than a given hidden tautology. However, this would be expensive
and seldom succeed, given that superposition creates lots of nonredundant
hidden tautologies. Instead, we propose to simplify hidden tautologies using the
following rules:
%
%\begin{linenomath*}
% \begin{align*}
%    %% TYPESETTING: \,'s below
%    \namedsimpsc{HTR}{L \cor L' \cor C}{L \cor L'}{\neglit{L'} \in \HL(L, N)} &\\
%    \namedsimpsc{FLR}{L \cor C}{L}{L',\neglit{L'} \in \HL(L, N)} &
% \end{align*}
%
\[\begin{aligned}[t]
   \vcenter{\namedsimp{HTR}{L \cor L' \cor C}{L \cor L'}}
   & \scif {\neglit{L'} \in \HL(L, N) \text{~and~} C \not= \bot} \\[.5\jot]
   \vcenter{\namedsimp{\rlap{FLR}\phantom{HTR}}{L \cor C}{L}}
   & \scif {L',\neglit{L'} \in \HL(L, N) \text{~and~} C \not= \bot}
\end{aligned}\]
%

%\end{linenomath*}
%
We call these techniques \emph{hidden tautology reduction} and \emph{failed literal
reduction}, respectively.
% \infname{FLD} is inspired by the \confrep{}{eponymous }SAT
% approach of asserting $L$ if $\neglit{L}$ is determined to be a failed literal
% \cite{jwf-1995-fld}.
%
Both rules are sound.
%
As with hidden literals, unit clauses~$L'$ can be exploited:
\[\namedsimpsc{UnitHTR}{L' \quad L \cor C}{L' \quad L}{L'\sigma \in \HL(L,N) \text{~and~} C \not= \bot}
%\text{~for some~}\sigma
\]

We give the simplification rules above\confrep{}{ (for hidden literal
elimination, hidden tautology reduction,
failed literal detection, and their variants)} the collective name of
\emph{hidden-literal-based elimination} (HLBE).
Yet another use of hidden literals is for \emph{equivalent literal
substitution} \cite{hjb-2010-cl-elim}: If both $L' \in \HL(L, N)$ and $L
\in \HL(L', N)$, we can often simplify $L'\sigma$ to $L\sigma$ in $N$ if
$L'\sigma \succ L\sigma$. We want to investigate this~further.

% \looseness=-1
% Provers based on the DISCOUNT loop use only active clauses for
% simplification. If we restrict our attention to a finite subset of hidden
% literals, we can organize them into data structures that support efficient
% lookup of a hidden literal. Efficient implementation of these data structures
% allows to use all passive clauses for simplification, strengthening DISCOUNT
% loop simplification machinery.

\begin{theorem}
The rules \infname{HLE}, \infname{FLE}, \infname{CongHLE$^+$},
\infname{Cong\allowbreak HLE$^-$}, \infname{UnitHLE}, \infname{HTR},
\infname{FLR}, and \infname{UnitHTR} are sound simplification rules.
\end{theorem}

\begin{rep}
\begin{proof}
It is easy to see that the deleted premises are entailed by the conclusions that
replace them and that the conclusions' instances are $\prec$-smaller than the
premises' instances, as required by the redundancy criterion. It remains to
check soundness.

\medskip

\noindent
\textsc{Case} \infname{HLE}:\enskip
We have $N{,}\> L' \models L$ by the side condition and must show
$N{,}\> L' \lor L \lor C \models L \lor C$. Let
$\ourmodel \models N{,}\> L' \lor L \lor C$. If $\ourmodel \models L'$, then
we also have $\ourmodel \models L$ thanks to the side condition and hence
$\ourmodel \models L \lor C$. Otherwise, we have $\ourmodel \models L \lor C$,
which is exactly what we need to show.

\medskip

\noindent
\textsc{Case} \infname{FLE}:\enskip
We have $N{,}\> L \models L'$ and $N, L \models \lnot L'$ by the side condition.
If $\ourmodel \models N{,}\> L$, then both $\ourmodel \models L'$ and
$\ourmodel \models \lnot L'$, an absurdity.
Otherwise, we have $\ourmodel \models C$, as desired.

\medskip

\noindent
\textsc{Case} \infname{CongHLE$^+$}, \infname{CongHLE$^-$}:\enskip
Obvious by congruence of equality.

\medskip

\noindent
\textsc{Case} \infname{UnitHLE}:\enskip
We have $N{,}\> L \models \lnot L'\sigma$ by the side condition. If $\ourmodel
\models N{,}\> L$, then $N \models \lnot L'\sigma$. But since $L' \in N$, this is
an absurdity. Otherwise, we have $\ourmodel \models C$, as desired.

\medskip

\noindent
\textsc{Case} \infname{HTR}:\enskip
We have $N{,}\> \lnot L' \models L$ by the side condition. If either $\ourmodel
\models L$ or $\ourmodel \models L'$, the desired result follows directly.
Otherwise, from $\ourmodel \models \lnot L'$ we also have $\ourmodel \models L$
thanks to the side condition, contradicting $\ourmodel \models \lnot L$.

\medskip

\noindent
\textsc{Case} \infname{FLR}:\enskip
We have $N{,}\> L' \models L$ and $N{,}\> \lnot L' \models L$ by the side condition.
Hence $N \models L$, as desired.

\medskip

\noindent
\textsc{Case} \infname{UnitHTR}:\enskip
We have $N{,}\> L'\sigma \models L$. Since $L' \in N$, we have $N \models L$,
as desired.
\qed
\end{proof}
\end{rep}

\section{Predicate Elimination}
\label{sec:satfol:predicate-elimination}

For propositional logic, variable elimination \cite{eb-2005-satpreprocess} is
one of the main preprocessing and inprocessing techniques. Following Gabbay and Ohlbach's ideas
\cite{go-1992-so-pred-elim}, Khasidashvili and Korovin \cite{kk-2016-pe-fol}
generalized variable elimination to first-order logic with equality and
demonstrated that it is effective as a preprocessor. We propose an improvement
that makes this applicable in more cases and show that, with a minor
restriction, it can be integrated in a superposition prover
without compromising its refutational completeness.

\subsection{Singular Predicates}

Khasidashvili and Korovin's preprocessing technique removes singular predicates
(which they call ``non-self-referential predicates'') from the problem using
so-called flat resolution.

\begin{defi}
   A predicate symbol is called \emph{singular}\confrep{ (or ``non-self-referential'')}{}
   for a clause set $N$ if it occurs at most once in every clause contained in
   $N.$
\end{defi}

\begin{defi}
   \label{def:flat-res}
   Let $C = \pospredlit{\cst{p}(\tuplen{s})} \cor C'$ and $D =
   \negpredlit{\cst{p}(\tuplen{t})} \cor D'$ be clauses with no variables in
   common. The clause $\neqlit{s_1}{t_1} \cor \cdots \cor \neqlit{s_n}{t_n}
   \cor C' \cor D'$ is a \emph{flat resolvent} of $C$ and $D$ on $\cst{p}$.
\end{defi}

% Predicate elimination saturates with respect to flat resolvents, removing all
% clauses containing $\cst{p}$. Given two (possibly identical) clause sets $M, N$,
% the resolved set $M \flatres_{\!\cst{p}} N$, defined below, repeatedly draws
% one inference partner from $M$ and the other one from $N$.

Given two (possibly identical) clause sets $M, N$, predicate elimination
iteratively replaces clauses from $N$ containing the symbol $\cst{p}$ with all flat
resolvents against clauses in $M$. Eventually, it yields a
set with no occurrences of $\cst{p}$.


\begin{defi}
\label{def:flat-res-set}
   Let $M, N$ be clause sets and $\cst{p}$ be a singular predicate for
   $M$. Let $\flatresiter$ be the following relation on clause set pairs and clause sets:
   \begin{enumerate}
   \item $(M{,}\> \{\arbpredlit{\cst{p}(\tupleempty{s})} \cor C'\} \uplus N) \flatresiter (M{,}\; N' \cup N)$ if
   $N'$ is the set that consists of all clauses (up to variable renaming) that are flat resolvents
   with $\arbpredlit{\cst{p}(\tupleempty{s})} \cor C'$ on $\cst{p}$ and a clause from $M$ as premises.
   The premises' variables are renamed apart.

   \smallskip
   \item $(M, N) \flatresiter N$ if $N$ has no occurrences of $\cst{p}$.
   \end{enumerate}
   \noindent  The \emph{resolved set} $M \flatres_{\!\cst{p}} N$ is the clause set $N'$ such that $(M,N) \flatresiter^* N'$.
   % obtained by the following
   % procedure: Set $N' = N$. Then repeatedly (1)~choose a clause $D \in N'$
   % containing $\cst{p}$, (2)~compute all flat resolvents involving $D$ and
   % partners from $M$, renaming variables apart, and (3)~replace $D$ with these
   % flat resolvents in $N'$. Stop when $N'$ contains no more occurrence of
   % $\cst{p}$.
\end{defi}

\pagebreak[2]

\begin{rep}
\begin{lemma}
   \label{lem:flat-res-set-termination-confluence}
   Let $M, N$ be clause sets and $\cst{p}$ be a singular predicate for
   $M$. The resolved set $N'$ is reached in a finite number of $\flatresiter$ steps,
   and it is unique up to variable renaming.
\end{lemma}
\begin{proof}
   To show $\flatresiter$ is terminating we use the following ordinal measure on
   clause sets: $\nu(\{D_1,\dotsc,\allowbreak D_n\}) = \omega^{\nu(D_1)} \oplus
   \cdots \oplus \omega^{\nu(D_n)}$, where $\nu(D)$ is the number of
   occurrences of $\cst{p}$ in $D$, $\omega$ is the first infinite ordinal, and
   $\oplus$ is the Hessenberg, or natural, sum, which is commutative.
   For every transition
   $(M, \{C\} \cup N) \flatresiter (M, N' \cup N)$, we have $\nu(N) > \nu(N')$
   because $\omega^{\nu(C)} > \omega^{\nu(C)-1} \cdot |N'|$.
   Eventually, a state $(M,N')$ with $\nu(N')=\omega^0\cdot n$ is reached.
   Then, we apply the second rule of Definition \ref{def:flat-res-set} to obtain the resolved set $N'$.

   To show that $N'$ is unique, i.e., $\flatresiter$ is confluent, it suffices
   to show (since $\flatresiter$ is terminating and Newmann's lemma applies
   \cite{bn-98-tr-and-all-that}) that $\flatresiter$ is locally confluent. In
   other words, whenever $(M,N) \flatresiter (M, N_1)$ and $(M,N) \flatresiter
   (M, N_2)$ there exists $N'$ such that $(M, N_1) \flatresiter (M, N')$ and
   $(M, N_2) \flatresiter (M,N')$. 
   
   There are two main sources of nondeterminism of $\flatresiter$: The choice of $C
   \in N$ and the choice of the literal in $C$ to act on. Let us focus on the choice of
   $C$ in $N$; the same discussion applies for the choice of literal in $C$.

   Let $N = \{C_1\} \uplus \{C_2\} \uplus N'$, where $C_1$ and $C_2$ are
   clauses with occurrences of $\cst{p}$. Then, $(M, \{C_1\} \uplus \{ C_2 \cup
   N' \}) \flatresiter (M, N'_1 \cup \{ C_2 \cup N'\})$ and $(M, \{C_2\} \uplus
   \{C_1 \cup N' \}) \flatresiter (M, N'_2 \cup \{ C_1 \cup N' \})$ where $N'_1$ and
   $N'_2$ are sets of corresponding resolvents. Both of $\flatresiter$ steps can
   be joined (up to variable renaming) to $(M, N'_1 \cup N'_2 \cup N')$, showing
   that $\flatresiter$ is locally confluent.\qed
\end{proof}
\end{rep}

\begin{conf}
The relation $\flatresiter$ is confluent up to variable renaming. Thanks to
the singularity constraint on $M$, it
also terminates on finite sets because the following ordinal measure decreases:
$\nu(\{D_1,\allowbreak\dotsc,\allowbreak D_n\}) = \omega^{\nu(D_1)} \oplus \cdots \oplus
\omega^{\nu(D_n)}$, where $\nu(D)$ counts the occurrences of $\cst{p}$ in
$D$, $\omega$ is the first infinite ordinal,
and $\oplus$ is the Hessenberg, or natural, sum, which is commutative.
For every transition
$(M, \{C\} \cup N) \flatresiter (M, N' \cup N)$, we have
$\nu(\{C\}) = \omega^{\nu(C)} > \omega^{\nu(C)-1} \cdot |N'| = \nu(N')$.
\end{conf}

Next, it is useful to partition
clause sets into subsets based on the presence and polarity of a singular
predicate.

\begin{defi}
   Let $N$ be a clause set and $\cst{p}$ be a singular predicate for
   $N.$ Let $\withpredpos{N}{\cst{p}}$ consist of all clauses of the form $\pospredlit{\cst{p}(\tuple{s}{})} \cor C' \in N$,
   let $\withpredneg{N}{\cst{p}}$ consist of all clauses of the form $\negpredlit{\cst{p}(\tuple{s}{})} \cor C' \in N$,
   let $\withpred{N}{\cst{p}} = \withpredpos{N}{\cst{p}} \cup \withpredneg{N}{\cst{p}}$,
   and let $\withoutpred{N}{\cst{p}} = N \setminus \withpred{N}{\cst{p}}$.
\end{defi}

\begin{defi}
   \label{def:pred-elim}
   Let $N$ be a clause set and $\cst{p}$ be a singular predicate for
   $N.$
   \emph{Singular predicate elimination} (SPE) of $\cst{p}$ in $N$ replaces $N$ by
   $\withoutpred{N}{\cst{p}} \cup (\withpredpos{N}{\cst{p}} \flatres_{\!\cst{p}} \withpredneg{N}{\cst{p}})$.
\end{defi}

The result of SPE is satisfiable if and only if $N$ is satisfiable
\cite[Theorem~1]{kk-2016-pe-fol}, justifying SPE's use in a preprocessor.
However, eliminating singular predicates aggressively can dramatically increase
the number of clauses. To prevent this, Khasidashvili and Korovin suggested to
replace $N$ by $N'$ only if $\lambda(N') \leq \lambda(N)$
and $\mu(N') \leq \mu(N)$, where
$\lambda(N)$ is the number of literals in~$N$ and
$\mu(N)$ is the sum for all clauses $C \in
N$ of the square of the number of distinct variables in $C$.

Compared with what modern SAT solvers use, this
criterion is fairly restrictive. We relax it to make it possible to
eliminate more predicates, within reason. Let $K_\mathrm{tol} \in \mathbb{N}$ be a tolerance
parameter. A predicate elimination
step from $N$ to $N'$ is allowed if
$\lambda(N') < \lambda(N) + K_\mathrm{tol}$ or
$\mu(N') < \mu(N)$ or
$|N'| < |N| + K_\mathrm{tol}$.
\confrep{}{(A refinement, which we want to try out in future work, would be to gradually
increment the tolerance $K_\mathrm{tol}$, as is done in some SAT solvers.)}

\subsection{Defined Predicates}

SPE is effective, but an important refinement has not yet been adapted to
first-order logic:\ variable elimination by substitution. E{\'{e}}n and Biere
\cite{eb-2005-satpreprocess} discovered that a propositional variable~$\cst{x}$ can be
eliminated without computing all resolvents if it is expressible as an
equivalence $\cst{x} \medleftrightarrow \varphi$, where $\varphi$, the ``gate,''
is an arbitrary formula that does not reference~$\cst{x}$.
They \confrep{partition}{extract from} a set $N$
\confrep{}{of propositional clauses }into a definition set $G$, essentially the
clausification of $\cst{x} \medleftrightarrow \varphi$, and
$R = \withpred{N}{\cst{p}} \setminus G$, the remaining
clauses containing~$\cst{p}$. To eliminate $\cst{x}$ from $N$ while
preserving satisfiability, it suffices to resolve clauses from $G$ against
clauses from $R$, effectively substituting $\varphi$ for $\cst{x}$ in $R$.
Crucially, we do not need to resolve pairs of clauses from $G$
or pairs of clauses from $R$.
We generalize this idea to first-order logic.

\begin{defi}
   \label{def:definition}
   \looseness=-1
   Let $G$ be a clause set, $\cst{p}$ be a predicate symbol, and
   $\tupleempty{x}$ be distinct variables.
%
   The set $G$ is a \emph{definition set} for $\cst{p}$ if (1)~$\cst{p}$
   is singular for $G$, (2)~$G$ consists of clauses of the form
   $\arbpredlit{\cst{p}(\tupleempty{x})} \cor C'$ (up to variable renaming), (3)~the
   variables in $C'$ are all among $\tupleempty{x}$, (4)~all clauses in $\withpredpos{G}{\cst{p}}
   \flatres_{\!\cst{p}} \withpredneg{G}{\cst{p}}$ are tautologies, and
   (5)~$E(\tupleempty{\cst{c}})$ is unsatisfiable, where
   the \emph{environment} $E(\tupleempty{x})$ consists of all subclauses $C'$ of any
   $\arbpredlit{\cst{p}(\tupleempty{x})} \mathbin{\cor} C' \in G$ and
   $\tupleempty{\cst{c}}$ is a tuple of distinct fresh constants substituted
   in for $\tupleempty{x}$.
\end{defi}

A definition set $G$ corresponds intuitively to a definition by
cases in mathematics---e.g.,
\[\cst{p}(\tuple{x}{}) =
\begin{cases} \top & \mathrm{if}~\varphi(\tuple{x}{}{}) \\[-\jot]
\bot & \mathrm{if}~\psi(\tuple{x}{})\end{cases}\]
Part~(4) states that the case conditions are mutually exclusive
(e.g., $\neglit{\varphi(\tuple{x}{})} \lor \neglit{\psi(\tuple{x}{})}$),
and part~(5) states that they are exhaustive
(e.g., $\nexists\tuple{\cst{c}}.\; \neglit{\varphi(\tuple{\cst{c}}{})} \land \neglit{\psi(\tuple{\cst{c}}{})}$).
%
Given a quantifier-free formula $\cst{p}(\tuple{x}{}) \medleftrightarrow
\varphi(\tuple{x}{})$ with distinct variables $\tuple{x}{}$ such that
$\varphi(\tuple{x}{})$ does not contain $\cst{p}$, any reasonable
clausification algorithm would produce a definition set for $\cst{p}$.

\begin{exa}
   \label{example:tautologies}
   Given the formula $\cst{p}(x) \medleftrightarrow \cst{q}(x) \mathrel\land
   (\cst{r}(x) \cor \cst{s}(x))$, a standard clausification algorithm
   \cite{nw-01-small-cnf} produces $\{ \negpredlit{\cst{p}(x)} \cor
   \pospredlit{\cst{q}(x)}{,}\; \negpredlit{\cst{p}(x)} \cor
   \pospredlit{\cst{r}(x)} \cor \pospredlit{\cst{s}(x)}{,}\;
   \pospredlit{\cst{p}(x)} \cor \negpredlit{\cst{q}(x)} \cor
   \negpredlit{\cst{r}(x)}{,}\; \pospredlit{\cst{p}(x)} \cor
   \negpredlit{\cst{q}(x)} \cor \negpredlit{\cst{s}(x)}\}$, which qualifies
   as a definition set for $\cst{p}$.
\end{exa}

Definition sets generalize E{\'{e}}n and Biere's
gates. They can be recognized syntactically for formulas such as
$\cst{p}(\tuple{x}{}) \medleftrightarrow \bigvee_{\!i} \cst{q}_i(\tuple{s_i}{})$ or
$\cst{p}(\tuple{x}{}) \medleftrightarrow \bigwedge_i \cst{q}_i(\tuple{s_i}{})$,
or semantically: Condition~(4) can be checked
using the congruence closure algorithm, and condition~(5) amounts to
a propositional unsatisfiability check.

The key result about propositional gates carries over to definition sets.

\begin{defi}
   \looseness=-1
   Let $N$ be a clause set, $\cst{p}$ be a predicate symbol,
   $G \subseteq N$ be a definition set
   for $\cst{p}$, and $R = \withpred{N}{\cst{p}}
   \setminus G$. \emph{Defined predicate elimination} (DPE) of $\cst{p}$ in $N$ replaces $N$ by
   $\withoutpred{N}{\cst{p}} \cup
%%% @PETAR: Double-check the supression of _p below. --JB
   (\confrep{\withpred{G}{\cst{p}}}{G} \flatres_{\!\cst{p}} \confrep{\withpred{R}{\cst{p}}}{R})$.
\end{defi}

\newcommand{\mm}{\ourmodel}
\begin{rep}
   \begin{lemma}
   \label{lem:unsat-fresh-consts}
   Let $N(\tupleempty{x})$ be a clause set such that the variables of all
   clauses in it are among the argument $n$-tuple $\tupleempty{x}$, and let
   $\tupleempty{\cst{c}}$ be an $n$-tuple of distinct fresh constants. If
   $N(\tupleempty{\cst{c}})$ {\upshape(}i.e.,
   $N(\tupleempty{x})\{\tupleempty{x}\mapsto\tupleempty{\cst{c}}\}${\upshape)} is
   unsatisfiable, then for every interpretation $\mm$ and valuation $\xi$,
   $\mm \notmodels_\xi N$.
   \end{lemma}
   \begin{proof}
      We show the contrapositive. Assume that for some $\mm$ and $\xi$,  $\mm \models_\xi N(\tupleempty{x})$.
      Then let $\ourmodel'$
      be a model that assigns each $\cst{c}_i$ the interpretation of $x_i$ under $\mm$ and $\xi$, and otherwise
      coincides with $\ourmodel$. We obtain $\ourmodel' \models N(\tupleempty{\cst{c}})$.\qed
   \end{proof}

   \begin{lemma}
      \label{lem:flat-res-set-step-satisfiability}
      Let $G$ be a definition set for $\cst{p}$\confrep{,}{} and $N$ \confrep{}{be }an arbitrary clause
      set. If $(G, N) \flatresiter (G, N')$ then $G \cup N$ and $G \cup N'$ are
      equivalent.
   \end{lemma}
   \begin{proof}
      Since flat resolution is sound, the nontrivial direction is to show that a
      model $\mm$ of the set $G \cup N'$ is also a model of $G \cup N$. As the only clause in $N
      \setminus N'$ is $C = \arbpredlit{\cst{p}(\tuple{s}{n})} \cor C'$ on which
      the $\flatresiter$ step is performed, we must show $\mm \models C$.
      % Without 

      % Let $\mm$ be canonical model of $G \cup N'$ \cite[Theorem~9.5.2]{mf-1996-fol} and let $C
      % \in N \setminus N'$. We need to show that $\mm \models C$---which is to say,
      % for all variable interpretation $\xi$, we have $\mm \models_\xi C$. To
      % lighten notations, we will assume $C$ is ground and ignore $\xi$. Canonical
      % models make this step possible.

      % First, $C$ must have the form $\arbpredlit{\cst{p}(\tuple{s}{})} \cor C'$,
      % where $\tuple{s}{}$ is a tuple of ground terms. 
      Without loss of generality,
      we assume that the leading literal of $C$ is positive.
      Towards a contradiction, assume $\xi$ is a valuation such that $\mm \notmodels_\xi C$. Then, $\mm \notmodels_\xi
      \pospredlit{\cst{p}(\tuple{s}{n})}$. 
      % Let $\xi'$ be a valuation that
      % assigns every instance $D = \pospredlit{\cst{p}(\tupleempty{s})} \cor D'$ of a clause in
      % $G$, since $\mm \notmodels \pospredlit{\cst{p}(\tuple{s}{})}$, we have $\mm
      % \models D'$.
      Consider an arbitrary clause $D = \pospredlit{\cst{p}}(\tuplen{x}) \cor D'
      \in \withpredpos{G}{\cst{p}}$ and a valuation $\xi'$, which assigns each $x_i$ the
      interpretation of $s_i$ under $\mm$ and $\xi$. As $\mm \notmodels_{\xi'}
      \pospredlit{\cst{p}}(\tuple{x}{n})$ and $\mm \models G$, then $\mm \models_{\xi'} D'$ for every
      such clause $D$.
   %
      However, by part (5) of Definition~\ref{def:definition} and by Lemma
      \ref{lem:unsat-fresh-consts}, $\mm \notmodels_{\xi'} E(\tuplen{x})$, where $E(\tuplen{x})$
      is the environment associated with the definition set $G$.
      Therefore, there must exist a clause $D =
      \negpredlit{\relax{\cst{p}(\tuplen{x})}} \cor D'$ in $\withpredneg{G}{\cst{p}}$ such
      that $\mm \notmodels_\xi D'$.


      Now consider the flat resolvent of $C$ and $D$ on~$\cst{p}$: $R =
      \neqlit{x_1}{s_1} \cor \cdots \cor \neqlit{x_n}{s_n} \cor C' \cor D'$.
      Let $\zeta$ be a valuation coinciding with $\xi$ on variables of $C$
      and with $\xi'$ on $\tuplen{x}$.
      Clearly, $\mm \notmodels_\zeta R$. Yet, $R \in N'$, and as $\mm \models N'$, we reach a contradiction.\qed
   \end{proof}

   \begin{lemma}
      \label{lem:flat-res-set-last-step-satisfiability}
      Let $G$ be a definition set for $\cst{p}$\confrep{,}{} and $N$ \confrep{}{be }a clause set with no occurrences of $\cst{p}$.
      Then $G \cup N$ is satisfiable if and only if $N$ is satisfiable.
   \end{lemma}
   \begin{proof}
      The nontrivial direction is to show that if $N$ is satisfiable, $G \cup N$ is as well.
      %
      Let $\mm$ be a model of $N$. We construct a model $\mm'$ of $G$ over
      the same universe as $\mm$. For any atom $A$ such that $\cst{p}$ does not
      occur in $A$ and for every $\xi$, we set $\mm' \models_\xi A$ if and only if
      $\mm \models_\xi A$. For any clause $\pospredlit{\cst{p}(\tuplen{x})} \cor
      C' \in G$ and any assignment $\xi$ such that $\mm \notmodels_\xi C'$, we
      define $\mm'$ so that $\mm'\models_\xi \cst{p}(\tuplen{x})$. By
      construction, $\mm' \models \withpredpos{G}{\cst{p}} \cup N$. It remains to show that $\mm' \models
      \withpredneg{G}{}$.

      Let $C = \negpredlit{\cst{p}(\tuplen{x})} \cor C' \in G$ and let $\xi$ be
      an arbitrary assignment. Towards a contradiction, assume
      $\mm'\notmodels_\xi C$, and consequently $\mm' \models_\xi
      \cst{p}(\tuplen{x})$. By construction of $\mm'$, there exists a clause
      $\pospredlit{\cst{p}(\tuplen{y})} \cor D' \in G$ and an assignment $\xi'$
      which assigns each $y_i$ value of $\xi(x_i)$ such that $\mm
      \notmodels_{\xi'} D'$. The resolvent $R = \neqlit{x_1}{y_1} \lor \cdots \lor
      \neqlit{x_n}{y_n} \lor C' \lor D'$ is a tautology, according to condition
      (4) of Definition \ref{def:definition}. However, for a valuation that
      behaves like $\xi$ on $\tupleempty{x}$ and $\xi'$ on $\tupleempty{y}$,
      $\mm'$ does not satisfy $R \in N$, contradicting our assumption.
      % @Jasmin: Your trick with var renaming does not work as flat resolvent is
      % defined on
      % variable-disjoint clauses.
      
   \end{proof}
\end{rep}
\begin{conf}

\end{conf}
\begin{theorem}
\label{thm:pes-sat-equiv}
   The result of applying DPE to a clause set $N$ is
   satisfiable if and only if $N$ is satisfiable.
\end{theorem}
\begin{rep}
\begin{proof}
   Let $\cst{p}$ be a predicate symbol and $G \subseteq N$ be the
   definition set used by DPE, and let $R = \withpred{N}{\cst{p}} \setminus G$.
%
   % First, using Lemmas \ref{lem:flat-res-set-termination-confluence} and \ref{lem:flat-res-set-step-satisfiability}
   % we show that we will eventually obtain a set $R'$
   % that contains no occurrences of $\cst{p}$ and that preserves satisfiability
   % of the original $R$. Finally, we will show that we can
   % omit $G$ in the result without affecting satisfiability.

   Using Lemma
   \ref{lem:flat-res-set-termination-confluence}, we get that there is a
   derivation $(G,R) \flatresiter^n (G,R') \flatresiter R'$. Applying Lemma
   \ref{lem:flat-res-set-step-satisfiability} $n$ times, we get that $G \cup R$
   is equivalent to $G \cup R'$. Finally, Lemma \ref{lem:flat-res-set-last-step-satisfiability}
   gives us the desired result.
   \qed
\end{proof}
\end{rep}

Since there will typically be at most only a few defined predicates in the
problem, it makes sense to fall back on SPE when no definition is found.

\begin{defi}
   \looseness=-1
   Let $N$ be a clause set and $\cst{p}$ be a predicate symbol. If there exists a
   definition set $G \subseteq N$ for $\cst{p}$, \emph{portfolio predicate
   elimination} (PPE) on $\cst{p}$ in $N$ replaces $N$ with
%%% @PETAR: Double-check the supression of _p below. --JB
   $\withoutpred{N}{\cst{p}} \cup (\confrep{\withpred{G}{\cst{p}}}{G} \flatres_{\!\cst{p}}
   \confrep{\withpred{R}{\cst{p}}}{R})$, where $R = \withpred{N}{\cst{p}} \setminus G$.
   Otherwise, if $\cst{p}$ is singular in $N$, it results in $\withoutpred{N}{\cst{p}}
   \cup (\withpredpos{N}{\cst{p}} \flatres_{\!\cst{p}} \withpredneg{N}{\cst{p}})$.
   In all other cases, it is not applicable.
\end{defi}


\subsection{Refutational Completeness}
\label{ssec:predicate-elimination-refutational-completeness}

\newcommand\FInf{\mathit{FInf}}
\newcommand\FLInf{\mathit{FLInf}}
\newcommand\Red{\mathit{Red}}
\newcommand\RedI{\Red_\mathrm{I}}
\newcommand\RedF{\Red_\mathrm{F}}
\newcommand\LRed{\mathit{LRed}}
\newcommand\LRedI{\LRed_\mathrm{I}}
\newcommand\LRedF{\LRed_\mathrm{F}}
\newcommand\gnd{\mathcalx{G}}
\newcommand\pow[1]{\mathcalx{pe}(#1)}
\newcommand\Lab{\mathbf{L}}
\newcommand\FFLab{{\mathbf{FL}}}
\newcommand\Inv{\mathit{Inv}}

\newcommand\RedWk{\smash{\Red\Wksup}}
\newcommand\RedIWk{\smash{\Red_\mathrm{I}\Wksup}}
\newcommand\RedFWk{\smash{\Red_\mathrm{F}\Wksup}}

\newcommand\Wksym{\flat}
\newcommand\Wksup{^\Wksym}
\newcommand\modelsWk{\models\Wksup}
%\newcommand\modelsWkcapgnd{\models^{\Wksym{}\gnd}_\cap}
\newcommand\modelsWkcapgnd{\modelsWk}
\newcommand\LRedWk{\LRed^{\Wksym}}
\newcommand\LRedWksub{\LRed^{\Wksym,\sqsupset}}
\newcommand\LRedFWksub{\LRedF^{\Wksym,\sqsupset}}
\newcommand\LRedIWk{\LRedI^{\Wksym}}
\newcommand\Wk[1]{#1^\Wksym}
\newcommand\prop{o}
\newcommand\true{\top}
\newcommand\false{\bot}

\looseness=-1
Hidden-literal-based techniques fit within the traditional framework of
saturation, because they delete or reduce a clause based on the \emph{presence}
of other clauses. In contrast, predicate elimination relies on the
\emph{absence} of clauses from the proof state. We can still integrate it with
superposition as follows: At every $k$th iteration of the given clause
procedure, perform predicate elimination on $\mathcalx{A}
\cup \mathcalx{P}$, and add all new clauses to $\mathcalx{P}$.

One may wonder whether such an approach preserves the refutational
completeness of the calculus. The answer is no.
%
\begin{conf}
To see why, consider the following \emph{binary splitting} rule
based on Riazanov and Voronkov \cite{riazanov-voronkov-2001}:
%
\[\namedsimp{BS}{C \cor D}{\cst{p} \cor C \quad D \cor \neglit{\cst{p}}}\]
%
Provisos: $C$ and $D$ have no free variables in common, $\cst{p}$ is fresh, and
$\cst{p}$ is $\prec$-smaller than $C$ and $D$. Since the conclusions are
smaller than the premise, the rule can be applied aggressively as a
simplification. But notice that the effect of splitting can be undone by
singular predicate elimination, possibly giving rise to loops
$\infname{BS}, \infname{SPE}, \infname{BS}, \infname{SPE}, \dotsc$.
This breaks completeness.

Our solution is to curtail the entailment relation used by the redundancy
criterion to disallow splitting-like simplifications. Weak entailment $\modelsWk$
is defined via an ad hoc nonclassical logic so that $\{\cst{p} \cor C{,}\;
\neglit{\cst{p}} \cor C\} \notmodelsWk \{C\}$ and yet $\modelsWk \{\cst{p} \cor
\neglit{\cst{p}}\}$.
%
More precisely, this logic is defined via an
encoding: $M \modelsWk N$ if and only if $\Wk{M} \models \Wk{N}$, where
%
$\Wk{\cst{p}(\tuple{t}{})} = \cst{p}(\tuple{t}{}) \noteq \false$,
$\Wk{\lnot\,\cst{p}(\tuple{t}{})} = \cst{p}(\tuple{t}{}) \noteq \true$, and
$\Wk{L} = L$ otherwise.
%
Moreover, the type $\prop$ may be interpreted as any set of cardinality at least
2, and $\false$ must be a distinguished symbol interpreted differently
from~$\top$.

The standard redundancy criterion $\RedWk$ based on $\modelsWk$ supports all the
familiar deletion and simplification techniques except splitting. Using $\RedWk$
not only prevents looping, but it also enables the use of
the given clause procedure, because any redundant inference
according to $\RedWk$ remains redundant after SPE or DPE. As usual, the devil is
in the details, and the details are in the report \cite{our-report}.
\end{conf}

\begin{rep}
To see why, consider the following \emph{binary splitting} rule
based on Riazanov and Voronkov \cite{riazanov-voronkov-2001}:
%
\[\namedsimp{BS}{C \cor D}{\cst{p} \cor C \quad D \cor \neglit{\cst{p}}}\]
%
Provisos: $C$ and $D$ have no free variables in common, $\cst{p}$ is fresh, and
$\cst{p}$ is $\prec$-smaller than $C$ and $D$. Since the conclusions are
smaller than the premise, the rule can be applied aggressively as a
simplification. But notice that the effect of splitting can be undone by
singular predicate elimination, possibly giving rise to loops
$\infname{BS}, \infname{SPE}, \infname{BS}, \infname{SPE}, \dotsc$.
Clearly, we need to curtail predicate elimination.

Under which conditions is predicate elimination refutationally complete? To
answer this question, we employ the saturation framework of Waldmann, Tourret,
Robillard, and Blanchette \cite{wtrb-20-sat-framework}. Let
$(\FInf, \Red)$ be the base calculus without predicate elimination---e.g.,
resolution or superposition inferences together with the standard
redundancy criterion \cite[Sect.~4.2]{bg-01-resolution}. The
inference system $\FInf$ is a set of inferences $(C_n,\ldots,C_1,C_0)$, for $n
\ge 1$, where $C_n,\ldots,C_1$ are the premises and $C_0$ is the conclusion.
$C_1$ is called the main premise. %; $C_n,\ldots,C_2$ are the side premises.
The redundancy criterion is a pair $\Red = (\RedI, \RedF)$ where $\RedI$
determines which inferences can be omitted and $\RedF$ is used to remove
clauses.

Next, consider an abstract proving process working on a single clause set.
Let $\rhd_{\Red}$ denote the transition relation that supports (1)~adding
arbitrary clauses and (2)~removing clauses deemed useless by $\RedF$.
Typically, the added clauses are the result of performing inferences and are
entailed by the premises, but other clauses can be added as well. A
\emph{$\rhd_{\Red}$-derivation} is a finite or infinite sequence of clause sets
$N_0 \rhd_{\Red} N_1 \rhd_{\Red} \cdots$.

We fix a finite set $\mathbf{P}$ of predicate symbols that may be subjected to
predicate elimination. These might include all the predicate symbols occurring
in the input problem, but exclude any symbols introduced by splitting or other
rules. Given a clause or clause set $N$, we write $\mathbf{P}(N)$ to denote the
set of all predicate symbols from $\mathbf{P}$ occurring in $N.$
%
Let $\rhd_\mathbf{P}$ denote the elimination of a
singular or defined predicate symbol from $\mathbf{P}$. A \emph{mixed
derivation} consists of transitions either of the form $N
\rhd_\mathbf{P} N'$ or of the form $N \rhd_{\Red} N'$ where
$\mathbf{P}(N) \supseteq \mathbf{P}(N')$.
Because $\mathbf{P}$ is finite, any mixed derivation consists of at most
finitely many $\rhd_\mathbf{P}$ transitions. Hence, in any derivation,
there exists an index~$k$ from which all transitions are standard
${\rhd}_{\Red}$-transitions.

This suggests the following path to completeness: Pretend that the transitions
between $N_0$ and $N_k$ are merely preprocessing and start the actual
derivation at $N_k$. This works at the abstract level of derivations on single
clause sets. It fails, however, for an actual saturation prover that
distinguishes between passive and active clauses.

\begin{exa}
\label{ex:cex-predelim-sat}
The counterexample below is based on the given clause prover \textsf{GC}
from the saturation framework. It shows how predicate elimination can break
\textsf{GC}'s key invariant, which states that all inferences between active
clauses are redundant. Breaking the invariant means that the limit might be
unsaturated, breaking the refutational completeness proof.

We use superposition with the order $\cst{a} \prec
\cst{b} \prec \cst{c} \prec \cst{d}$ and without selection.
Assume $\cst{a} \in \mathbf{P}$ and
suppose we start with the satisfiable clause set %consisting of
%
%\begin{linenomath*}
\begin{align*}
  & \neglit{\cst{a}} \cor \MAX{\cst{d}}
  && \neglit{\cst{a}} \cor \MAX{\neglit{\cst{d}}}
  && \cst{a} \cor \cst{b} \cor \MAX{\cst{c}}
  && \cst{c} \cor \MAX{\cst{d}}
  && \cst{b} \cor \MAX{\neglit{\cst{d}}}
\end{align*}
%\end{linenomath*}
%
where gray boxes mark maximal literals. Suppose the prover makes
$\cst{c} \cor \cst{d}$ and $\cst{b} \cor \neglit{\cst{d}}$ active. From these
two clauses, a superposition inference~$\iota$ could derive the conclusion
$\cst{b} \cor \cst{c}$. However, the three passive clauses are
$\prec$-smaller than $\iota$'s main premise $\cst{b} \cor \neglit{\cst{d}}$ and
collectively entail $\iota$'s conclusion. This means that $\iota$ is
redundant and can be ignored.

If the prover now eliminates the predicate $\cst{a}$ using \infname{SPE}, the
passive set is reduced to $\{\cst{b} \cor \cst{c} \cor \cst{d}{,}\;
\cst{b} \cor \cst{c} \cor \neglit{\cst{d}}\}$.
%
Either clause is subsumed by an active clause, so the prover deletes it.
It stops with the active set
$\{\cst{c} \cor \cst{d}{,}\; \cst{b} \cor \neglit{\cst{d}}\}$, which is
unsaturated because $\iota$ is no longer redundant. The invariant is broken.
\end{exa}

\begin{exa}
\label{ex:cex-predelim-unsat}
In Example~\ref{ex:cex-predelim-sat}, the initial clause set was satisfiable.
If it is unsatisfiable, we can even lose refutational
completeness. To see why, we add the unit clauses
$\neglit{\cst{b}}$ and $\neglit{\cst{c}}$ to the initial clause set of
Example~\ref{ex:cex-predelim-sat} to make it unsatisfiable. We repeat the same
steps as above, including the subsumptions at the end,
yielding the passive set $\{\MAX{\neglit{\cst{b}}}, \MAX{\neglit{\cst{c}}}\}$ and
the active set $\{\cst{c} \cor \MAX{\cst{d}}{,}\; \cst{b} \cor \MAX{\neglit{\cst{d}}}\}$. Then,
making $\neglit{\cst{b}}$ and $\neglit{\cst{c}}$ active triggers no
inferences. The prover stops with an
unsatisfiable four-clause active set that does not contain the empty clause.
\end{exa}

A solution could be to move all active clauses to the passive set at step~$k$
or later, but this would be costly, since it would force the prover to redo
inferences whose conclusions might then have to be simplified or subsumed again.
Instead, we salvage the existing completeness proof for $\mathsf{GC}$, by
resolving the issues concerning splitting and the $\mathsf{GC}$ invariant. Our
approach is to weaken the redundancy criterion slightly, enough both to disable
splitting on $\mathbf{P}$-predicates and to ensure that
inferences such as $\iota$ in Examples
\ref{ex:cex-predelim-sat}~and~\ref{ex:cex-predelim-unsat} are performed. The
required weakening is so mild that it does not invalidate any practical
simplification or subsumption techniques we are aware of, except of course
splitting.

In accordance with the saturation framework, let $\mathbf{F}$ be the set of
first-order $\mathrm{\Sigma}$-clauses, let $\mathbf{G}$ be its ground subset,
and let $\gnd$ be a function that maps an $\mathbf{F}$-clause to the set of its
$\mathbf{G}$-clause instances and analogously for $\mathbf{F}$-inferences. We
define an extension $\mathbf{G}\Wksup$ of $\mathbf{G}$ for
$\mathrm{\Sigma}\Wksup$-clauses in an ad hoc nonclassical logic reminiscent
of paraconsistent logics \cite{carnielli-et-al-2007}. The objective is to
disallow the entailment that makes splitting and Examples
\ref{ex:cex-predelim-sat}~and~\ref{ex:cex-predelim-unsat} possible.
%
The signature~$\mathrm{\Sigma}\Wksup$ extends $\mathrm{\Sigma}$ with a
distinguished predicate symbol $\false$ that is interpreted differently
from~$\top$. For $\mathrm{\Sigma}\Wksup$, the Boolean type $\prop$ may be
interpreted as any set of cardinality at least 2.

\begin{defi}
The operator $\Wk{}$ translates $\mathrm{\Sigma}$-literals to
$\mathrm{\Sigma}\Wksup$-literals as follows, where $\cst{p} \in \mathbf{P}$,
$\cst{q} \notin \mathbf{P}$, and $s, t$ are non-Boolean terms:
%
%\begin{linenomath*}
\begin{align*}
  \Wk{\cst{p}(\tuple{t}{})} & = \cst{p}(\tuple{t}{}) \noteq \false
& \Wk{\cst{q}(\tuple{t}{})} & = \cst{q}(\tuple{t}{}) \eq \true
& \Wk{(s \eq t)} & = s \eq t \\[-\jot]
  \Wk{\lnot\,\cst{p}(\tuple{t}{})} & = \cst{p}(\tuple{t}{}) \noteq \true
& \Wk{\lnot\,\cst{q}(\tuple{t}{})} & = \cst{q}(\tuple{t}{}) \noteq \true
& \Wk{(s \noteq t)} & = s \noteq t
\end{align*}
%\end{linenomath*}
%
The operator is lifted elementwise to $\mathbf{G}$-clauses and
$\mathbf{G}$-clause sets.
The \emph{weak entailment} $\modelsWk$ over $\mathbf{G}$-clause sets
is defined via an encoding into $\mathrm{\Sigma}\Wksup$-clauses:
$M \modelsWk N$ if and only if $\Wk{M} \models \Wk{N}$.
The lifting to $\mathbf{F}$-clauses and $\mathbf{F}$-clause set is achieved
in the standard way via grounding.
\end{defi}

The following property of weak entailment will allow us to eliminate
$\mathbf{P}$-predicates without losing completeness:

\begin{lemma}
\label{lem:p-Wk-entail}
Let $C$ be a clause that contains the predicate symbol $\cst{p} \in
\mathbf{P}$ and $D$ be a clause that does not contain $\cst{p}$. If $N
\cup \{C\} \modelsWk \{D\}$, then $N \modelsWk \{D\}$.
\end{lemma}

\begin{proof}
Suppose $\mathscr{J} \models \Wk{N}$. We will define $\mathscr{J}'$ so that
$\mathscr{J}' \models \Wk{N} \cup \{\Wk{C}\}$, retrieve $\mathscr{J}'
\models \Wk{D}$, and then argue that $\mathscr{J} \models \Wk{D}$. We
take $\mathscr{J}'$ to coincide with $\mathscr{J}$ except that we extend the
domain for $\prop$ with one fresh value and use this value as the
interpretation of $\cst{p}(\tuple{t}{})$ for all argument tuples $\tuple{t}{}$.
This modification makes any $\cst{p}$ literal of $\Wk{C}$ true, and it
preserves the truth of $\Wk{N}$. By the hypothesis, $\mathscr{J}' \models
\Wk{D}$. And since $\cst{p}$ does not occur in $D$, we have $\mathscr{J}
\models \Wk{D}$.\qed
\end{proof}

Note that the above lemma does not hold for classical entailment $\models$;
indeed, $\{\cst{p} \cor \cst{q}{,}\; \neglit{\cst{p}} \cor \cst{q}\} \models
\{\cst{q}\}$. On the other hand, the law of excluded middle does hold for weak
entailment: $\modelsWk \cst{p} \cor \lnot\,\cst{p}$. In fact, all classical
clausal tautologies are tautologies for $\modelsWk$. 

The standard redundancy criterion $\Red$ is obtained by lifting a criterion on
$\mathbf{G}$-clauses to $\mathbf{F}$-clauses. The same construction can be
replicated using $\modelsWk$ instead of $\models$, yielding the \emph{weak
redundancy criterion} $\RedWk$.
It is easy to check that the usual simplification techniques implemented in
superposition provers can be justified using $\RedWk$. Specifically,
this concerns the following rules described by Schulz \cite[Sects.\
2.3.1~and~2.3.2]{ss-02-brainiac}:\
deletion of duplicated literals, 
deletion of resolved literals, 
syntactic tautology deletion,
semantic tautology deletion,
rewriting of negative literals,
positive simplify-reflect,
negative simplify-reflect,
clause subsumption, and
equality subsumption.
Moreover, rewriting of positive literals is possible if the
rewriting clause is smaller than the rewritten
clause (a condition that is also needed with $\models$ but omitted by Schulz).
Finally, destructive equality resolution cannot be justified with $\models$,
let alone $\modelsWk$.

We instantiate the saturation framework with $(\FInf,\RedWk)$ to obtain a
given clause prover \textsf{GC}. The prover operates on sets of labeled clauses
$(C, l)$, where $C$ is a standard clause and $l \in \Lab$ is a label.
The $\mathsf{active}$ label identifies active clauses; all other clauses are
passive. The prover takes the form of two rules,
\textsc{Process} and \textsc{Infer}, restricted to prevent the introduction of
$\mathbf{P}$-predicates. We extend it with a third rule,
\textsc{PredElim}, for predicate elimination, and call the extended prover
\textsf{GCP}. The rules are as follows, using again the framework notations:

\begin{enumerate}[\rm\textsc{PredElim}\,]
\itemsep1\jot
\item[\rm\textsc{Process}\,]
  $\mathcalx{N} \cup \mathcalx{M}
   \,\Longrightarrow_\mathsf{GCP}\,
   \mathcalx{N} \cup \mathcalx{M}'$
\\
  where $\mathcalx{M} \subseteq \LRedFWksub(\mathcalx{N} \cup \mathcalx{M}')$,
  $\mathcalx{M}'{\downarrow}_\mathsf{active} = \emptyset$,
  and \\
  $\mathbf{P}(\lfloor\mathcalx{M}'\rfloor) \subseteq \mathbf{P}(\lfloor\mathcalx{N} \cup \mathcalx{M}\rfloor)$

\item[\rm\textsc{Infer}\,]
  $\mathcalx{N} \cup \{(C, l)\}
   \,\Longrightarrow_\mathsf{GCP}\,
   \mathcalx{N} \cup \{(C, \ensuremath{\mathsf{active}})\} \cup \mathcalx{M}$
\\
  where $l \not= \ensuremath{\mathsf{active}}$,
  $\mathcalx{M}{\downarrow}_\mathsf{active} = \emptyset$, \\
  $\FInf(\lfloor\mathcalx{N}{\downarrow}_\mathsf{active}\rfloor, \{C\}) \subseteq
    \RedIWk^{{\cap}\gnd}(\lfloor\mathcalx{N}\rfloor \cup \{C\} \cup \lfloor\mathcalx{M}\rfloor)$,
  and \\
  $\mathbf{P}(\lfloor\mathcalx{M}\rfloor) \subseteq \mathbf{P}(\lfloor\mathcalx{N}\rfloor \cup \{C\})$

\item[\rm\textsc{PredElim}\,]
  $\mathcalx{N} \cup \mathcalx{M}
   \,\Longrightarrow_\mathsf{GCP}\,
   \mathcalx{N} \cup \mathcalx{M}'$
\\
  where $\mathcalx{N} \cup \mathcalx{M} \rhd_\mathbf{P} \mathcalx{N} \cup \mathcalx{M}'$
  and $\mathcalx{M}'{\downarrow}_\mathsf{active} = \emptyset$
\end{enumerate}

Here is a summary of the main framework notations:
%
\begin{itemize}
\item The letters $\mathcalx{M}, \mathcalx{N}$ range over sets of labeled clauses.
  $\mathcalx{M}{\downarrow}_l$ denotes the subset of clauses in $\mathcalx{M}$
  labeled with $l$. The operator $\lfloor\phantom{\cdot}\rfloor$ erases all
  labels in a labeled clause or clause set.

\smallskip

\item $\FInf(N)$ denotes the set of all base calculus inferences with premises
  in $N$, and $\FInf(N, M) = \FInf(N \cup M) \setminus \FInf(N \setminus M)$.
  The same notations are also available for the straightforward extension $\FLInf$
  of $\FInf$ with labels.

\smallskip

\item $\LRedWksub$ is the extension of the standard redundancy
  criterion $\RedWk$ defined using $\modelsWk$ to nonground labeled clauses
  with subsumption ($\sqsubset$).

\smallskip

\item Given a sequence $(\mathcalx{N}{}_i)_i$, its \emph{limit} (\emph{inferior})
  is $\mathcalx{N}{}_\infty = \bigcup_i \bigcap_{j\ge i} \mathcalx{N}{}_j$.
\end{itemize}

The completeness proof follows the invariance-based argument found the
forthcoming journal submission \cite{waldmann-et-al-202x-article}
by Waldmann et al.

\begin{lemma}
\label{lem:gcp-derivations-are-red-black-derivations}
Every $\Longrightarrow_\mathsf{GCP}$-derivation is a mixed derivation.
\end{lemma}

\begin{proof}
The cases for \textsc{Process} and \textsc{Infer} are almost as in Waldmann et
al., with adjustments to show that $\cst{P}$-predicates cannot reappear once
they have disappeared. The case for \textsc{ElimPred} is trivial.\qed
\end{proof}

Let $\Inv\Wksup_\mathcalx{N}(k)$ denote the condition
$\textstyle\FLInf(\mathcalx{N}{}_k{\downarrow}_\mathsf{active}) \subseteq
\LRedIWk(\mathcalx{N}{}_k)$.
Notice the difference with the definition of the key invariant
$\Inv_\mathcalx{N}$ in the saturation framework, whose right-hand side is
$\bigcup_{i=0}^k \RedI^{\Lab}(\mathcalx{N}{}_i)$. We cannot use the
big union ${\bigcup}$ starting at $i = 0$ because we will need to truncate a
sequence prefix of an a priori unknown length. The argument will still work
thanks to monotonicity properties of redundancy criteria.

\begin{lemma}
\label{lem:gcp-invar-gcp}
Let $(\mathcalx{N}{}_i)_i$ be a $\Longrightarrow_\mathsf{GCP}$-derivation.
If
$\mathcalx{N}{}_0{\downarrow}_\mathsf{active} = \emptyset$,
then
$\Inv\Wksup_\mathcalx{N}(k)$ holds for all indices~$k$.
\end{lemma}

\begin{proof}
The base case is as in Waldmann et al.

For \textsc{Process} and \textsc{Infer}, the proof is essentially as in Waldmann
et al., except that we also need to show that
$\LRedIWk(\mathcalx{N}{}_k) \subseteq \LRedIWk(\mathcalx{N}{}_{k+1})$.
This is a consequence of
$\mathcalx{N}{}_k \rhd_{\LRedWksub} \mathcalx{N}{}_{k+1}$ and of
properties (R2) and (R3) of redundancy criteria.

A new case to consider is that of a \textsc{PredElim} transition
$\mathcalx{N}{}_{k} \Longrightarrow_\mathsf{GCP} \mathcalx{N}{}_{k+1}$.
Let $\mathcalx{N}{}_{k} = \mathcalx{N} \cup \mathcalx{M}
\Longrightarrow_\mathsf{GCP} \mathcalx{N} \cup \mathcalx{M}' = \mathcalx{N}{}_{k+1}$,
where $\mathcalx{N} \cup \mathcalx{M} \rhd_\mathbf{P}
\mathcalx{N} \cap \mathcalx{M}'$ and
$\mathcalx{M}'{\downarrow}_\mathsf{active} = \emptyset$.
We assume without loss of generality that $\mathcalx{M} \cap \mathcalx{M}' =
\emptyset$.
Let $\cst{p}$ be the eliminated predicate. Note that $\cst{p}$ occurs in every
clause in $\mathcalx{M}$ but in none of the clauses in $\mathcalx{N}$ or
$\mathcalx{M}'$.
We must show $\FLInf(\mathcalx{N}{}_{k+1} {\downarrow}_\mathsf{active})
\subseteq \LRedIWk(\mathcalx{N}{}_{k+1})$.
As for \textsc{Process}, we have the inclusion
$\FLInf(\mathcalx{N}{}_{k+1} {\downarrow}_\mathsf{active})
\subseteq \FLInf(\mathcalx{N}{}_k {\downarrow}_\mathsf{active})$,
by the side condition that
$\mathcalx{M}'{\downarrow}_\mathsf{active} = \emptyset$.
Moreover, by the induction hypothesis,
$\FLInf(\mathcalx{N}{}_k {\downarrow}_\mathsf{active})
\subseteq \LRedIWk(\mathcalx{N}{}_k)$. Thus,
$\FLInf(\mathcalx{N}{}_k {\downarrow}_\mathsf{active})
\subseteq \LRedIWk(\mathcalx{N} \cup \mathcalx{M})$.

Let $\iota \in \FLInf(\mathcalx{N} {\downarrow}_\mathsf{active})$.
By the argument above, we have
$\iota \in \LRedIWk(\mathcalx{N} \cup \mathcalx{M})$.
We must
show $\iota \in \LRedIWk(\mathcalx{N} \cup \mathcalx{M}')$.
%
By definition of $\LRedIWk$, it suffices to show that for
every ground instance $(C_n,\ldots,C_1,C_0)$ of $\iota$, there exists a finite
clause set $\mathcalx{D} \subseteq \gnd(\mathcalx{N}) \cup \gnd(\mathcalx{M}')$
that is $\prec$-smaller than $C_1$ and such that
$\{C_n,\ldots,C_2\} \cup \mathcalx{D} \modelsWk \{C_0\}$.
Without loss of generality, we assume that $\mathcalx{D}$ is the smallest such
set.

By definition of $\mathcalx{N}$, $\cst{p}$ cannot occur in $C_0$.
By Lemma~\ref{lem:p-Wk-entail}, if $\cst{p}$ occurred in $D \in \mathcalx{D}$,
we could remove $D$, but this would mean $\mathcalx{D}$ is not minimal.
As a result, $\mathcalx{D}$ cannot contain clauses from $\gnd(\mathcalx{M})$ and
hence $\mathcalx{D} \subseteq \gnd(\mathcalx{N})$. Thus,
$\iota \in \LRedIWk(\mathcalx{N})$. By property (R2) of
redundancy criteria, we have the desired result: $\iota \in
\LRedIWk(\mathcalx{N} \cup \mathcalx{M}')$.\qed
\end{proof}

\begin{lemma}
\label{lem:gcp-invar-infinity}
Let $(\mathcalx{N}{}_i)_i$ be a
$\rhd_{\LRedWksub}$-derivation.
If $\Inv\Wksup_\mathcalx{N}(i)$ holds for all indices~$i$, then
$\FLInf(\mathcalx{N}{}_\infty{\downarrow}_\mathsf{active}) \subseteq
\bigcup_i \LRedIWk(\mathcalx{N}{}_i)$
holds.
\end{lemma}

\begin{proof}
We assume $\iota \in \FLInf(\mathcalx{N}{}_\infty {\downarrow}_\mathsf{active})$
and show $\iota \in \bigcup_i \LRedIWk(\mathcalx{N}{}_i)$
for some arbitrary $\iota$.
%
For $\iota$ to belong to $\FLInf(\mathcalx{N}{}_\infty {\downarrow}_\mathsf{active})$, all of
its finitely many premises must be in $\mathcalx{N}{}_\infty {\downarrow}_\mathsf{active}$.
Therefore, there must exist an index $k$ such that
$\mathcalx{N}{}_k {\downarrow}_\mathsf{active}$ contains all of them, and therefore
$\iota \in \FLInf(\mathcalx{N}{}_k {\downarrow}_\mathsf{active})$. Since $\Inv_\mathcalx{N}(k)$
holds, $\iota \in \LRedIWk(\mathcalx{N}{}_k)$.
Hence, $\iota \in \bigcup_i \LRedIWk(\mathcalx{N}{}_i)$.\qed
\end{proof}

\begin{lemma}
\label{lem:fair-gcp-derivations}
Let $(\mathcalx{N}{}_i)_i$ be a $\Longrightarrow_\mathsf{GCP}$-derivation.
If
$\mathcalx{N}{}_0{\downarrow}_\mathsf{active} = \emptyset$
and
$\mathcalx{N}{}_\infty{\downarrow}_l = \emptyset$ for every label
$l \not= \ensuremath{\mathsf{active}}$,
then there exists an index $k$ such that
$(\mathcalx{N}{}_{k+i})_i$ is a fair $\rhd_{\LRedWksub}$-derivation.
\end{lemma}

\begin{sloppypar}
\begin{proof}
By Lemma~\ref{lem:gcp-derivations-are-red-black-derivations}, there must exist
an index $k$ such that the sequence $(\mathcalx{N}{}_{k+i})_i$ is a pure
$\rhd_{\LRedWksub}$-derivation.
By Lemma~\ref{lem:gcp-invar-gcp},
$\Inv\Wksup_\mathcalx{N}(k+i)$ holds for all indices~$i$.
Hence, by Lemma~\ref{lem:gcp-invar-infinity},
$\FLInf(\mathcalx{N}{}_\infty{\downarrow}_\mathsf{active}) \subseteq
\bigcup_{i} \LRedIWk(\mathcalx{N}{}_{k+i})$.
By the second hypothesis, this inclusion simplifies to
$\FLInf(\mathcalx{N}{}_\infty) \subseteq
\bigcup_{i} \LRedIWk(\mathcalx{N}{}_{k+i})$.\qed
\end{proof}
\end{sloppypar}

\begin{theorem}
\label{thm:gcp-complete}
Let $(\mathcalx{N}{}_i)_i$ be a $\Longrightarrow_\mathsf{GCP}$-derivation
with
$\mathcalx{N}{}_0{\downarrow}_\mathsf{active} = \emptyset$
and
$\mathcalx{N}{}_\infty{\downarrow}_l = \emptyset$ for every label
$l \not= \ensuremath{\mathsf{active}}$.
If $\lfloor \mathcalx{N}{}_0\rfloor$ is unsatisfiable,
then
some $\mathcalx{N}{}_i$
contains the empty clause with some arbitrary label.
\end{theorem}

\begin{proof}
By Lemma~\ref{lem:fair-gcp-derivations},
we know that there exists an index $k$ such that
$(\mathcalx{N}{}_{k+i})_i$ is a fair $\rhd_{\LRedWksub}$-derivation.
Moreover, since
$\rhd_{\LRedWksub}$ and
$\rhd_\mathbf{P}$ preserve unsatisfiability
(by (R1) of redundancy criteria, Khasidashvili and Korovin's Theorem~1, and our
Theorem~\ref{thm:pes-sat-equiv}), we have that
$\lfloor \mathcalx{N}{}_k\rfloor$ is unsatisfiable.
Since the base calculus $(\FLInf,\allowbreak\LRed)$ is assumed to be statically
refutationally complete with respect to $\models$, the calculus
$(\FLInf,\allowbreak\LRedWk)$ with a weaker redundancy criterion is also
statically complete with respect to $\models$, and by the saturation framework,
$(\FLInf,\allowbreak\LRedWksub)$ preserves this. Exploiting the equivalence of
static and dynamic completeness, we conclude that some $\mathcalx{N}{}_{k+i}$ must
contain a labeled empty clause.\qed
\end{proof}
\end{rep}

\begin{qle}
\section{Quasipure Literal Elimination}
\label{sec:satfol:quasipure-literal-elimination}

\emph{Pure literal elimination} (PLE) is one of the simplest optimizations
implemented in SAT solvers. It is a special case of variable elimination: If a
given variable always occurs with the same polarity in a problem, the solver can
assign it that polarity without loss of generality, making all the clauses that
contain it tautologies. PLE consists of recursively deleting all such clauses.
%
PLE's generalization to first-order logic considers predicate literals
$\arbpredlit{\cst{p}(\tuple{s}{})}$, where the arguments $\tuple{s}{}$ are
ignored by the analysis; only the polarity matters.

\begin{exa}
   \label{example:ple}
   Consider the clause set $N = \{ \cst{p}(x) \cor \cst{q}(\cst{a}, x){,}\allowbreak\;
   \cst{p}(\cst{f}(x)){,}\allowbreak\; \neglit{\cst{q}(\cst{a}, \cst{a})} \}$.
   Since $\cst{p}$ occurs only positively in $N$, PLE deletes the two first
   clauses. At that point, $\cst{q}$ occurs only negatively in the remaining
   singleton clause set and can be deleted as well. The result is the empty set,
   which is obviously satisfiable, indicating that $N$ is satisfiable.
   As model of $N$, we can take $\ourmodel$ such that $\ourmodel$ makes all
   atoms $\cst{p}(\ldots)$ true and all atoms $\cst{q}(\ldots)$ false.
\end{exa}

\begin{exa}
   \label{example:qle-i}
   Consider the clause set $N' = \{
   \cst{p}(\cst{a}){,}\allowbreak\;
   \neglit{\cst{p}(x)} \lor \cst{p}(\cst{f}(x))
    \}$. This time, PLE does not apply because $\cst{p}$ occurs with both
    polarities. Yet we notice that $\cst{p}$ occurs positively in
    both clauses, and hence that the same reasoning as in the previous
    example applies. In particular, $\ourmodel \models N'$.
\end{exa}

\begin{exa}
   \label{example:qle-ii}
   Consider the clause set $N'' = \{
   \cst{p}(\cst{a}){,}\allowbreak\;
   \cst{q}(x) \lor \cst{p}(\cst{f}(x)){,}\allowbreak\;
   \neglit{\cst{q}(\cst{f}(\cst{a}))}{,}\allowbreak\;
   \neglit{\cst{p}(x)} \lor \neglit{\cst{q}(\cst{f}(x))}
   \}$. This clause set is clearly beyond PLE. Yet each clause contains either
   a positive $\cst{p}$ literal or a negative $\cst{q}$ literal---any
   extraneous, wrong-polarity literals are harmless. Thus $\ourmodel \models
   N''$.
\end{exa}

Examples \ref{example:qle-i} and \ref{example:qle-ii} suggest that PLE is
needlessly restrained, presumably due to its origins in propositional logic. We
propose a generalization to ``quasipure'' predicates.

\begin{defi}
\label{def:quasipure-set}
A set $P$ of predicate symbols is \emph{quasipure} with a polarity map $s$ in a
clause set $N$ if for every symbol $\cst{p} \in P$, every clause in $N$ that
contains a member of $P$ contains at least one occurrence of some $\cst{q} \in
P$ with polarity $s_\cst{q} \in \{+, -\}$.
%
The set $P$ is \emph{quasipure} in $N$ if there exists a polarity map $s$
such that $P$ is quasipure with $s$ in $N$.
\end{defi}

In Example~\ref{example:qle-i}, $\{\cst{p}\}$ is quasipure with $s_\cst{p} =
{+}$. In Example~\ref{example:qle-ii}, $\{\cst{p}, \cst{q}\}$ is quasipure with
$s_\cst{p} = {+}$ and $s_\cst{q} = {-}$. For this example, it is crucial to
consider $\cst{p}$ and $\cst{q}$ together; neither of the singletons
$\{\cst{p}\}$ and $\{\cst{q}\}$ is quasipure.

\begin{defi}
\label{def:quasipure-sym}
A predicate symbol $\cst{p}$ is \emph{quasipure} with polarity $s_\cst{p} \in
\{+, -\}$ in a clause set $N$ if there exists a set $P \ni \cst{p}$ of predicate
symbols and a polarity map $s$ such that $P$ is quasipure with $s$ in $N$.
%
The symbol $\cst{p}$ is \emph{quasipure} in $N$ if there exists a polarity
$s_\cst{p} \in \{+, -\}$ such that $\cst{p}$ is quasipure with $s_\cst{p}$ in
$N$.
%
A literal $L = \arbpredlit{\cst{p}(\ldots)}$ is \emph{quasipure} in $N$ if
$\cst{p}$ is quasipure with $L$'s polarity in $N$.
\end{defi}

Deleting a clause containing a quasipure literal is monotonic in the sense that
it might create new opportunities for quasipure literal elimination but never
ruin existing ones. Therefore, the following nondeterministic procedure yields a
well-defined result:

\begin{defi}
\label{def:qle}
Given a clause set, \emph{quasipure literal elimination} (QLE) iteratively
deletes clauses containing quasipure predicate symbols until no such clauses
remain.
\end{defi}

The key property of QLE is that it preserves unsatisfiability:

\begin{lemma}
Let $C \in N$ be a clause containing a quasipure literal. If $N \setminus \{C\}$
is satisfiable, then $N$ is satisfiable.
\end{lemma}

\begin{proof}
Let $\ourmodel$ be a model of $N \setminus \{C\}$.
Let $P$ be the set of predicates and $s$ the polarity map whose existence is
guaranteed by definition of quasipurity. Let $N_0 \subseteq N \setminus \{C\}$
be the result of applying QLE on $N$. It is easy to see that $N_0$ contains no
occurrences of the symbols in $P$. Extend $\ourmodel$ to $\ourmodel'$ by
making each symbol $\cst{p}$ such that $s_\cst{p} = {+}$ true and
each symbol $\cst{p}$ such that $s_\cst{p} = {-}$ false. Clearly $\ourmodel'$ is
a model not only of $N_0$, which contains no $P$ symbols, but also of $N$,
because each clause in $N \setminus N_0$ contains a quasipure literal, which is
satisfied by $\ourmodel'$.
\qed
\end{proof}

Definition~\ref{def:qle} suggests a naive, nondeterminisitc procedure for
discovering and eliminating quasipure predicate symbols: Choose a predicate
symbol $\cst{p}$ and a polarity $s_\cst{p}$, and take $P = \{\cst{p}\}$. If the
predicate occurs with the wrong polarity in a clause
$\arbpredlit{\cst{p}(\ldots)} \lor \arbpredlit{\cst{q}_1(\ldots)} \lor \cdots \lor
\arbpredlit{\cst{q}_n(\ldots)} \lor C$, try to extend the polarity map $s$ and
the predicate set $P$ with one of the $\arbpredlit{\cst{q}_i}$'s and continue
recursively with $\cst{q}_i$. In Sect.~\ref{sec:satfol:implementation}, we will see a
more efficient approach based on a SAT encoding.
\end{qle}

\section{Satisfiability by Clause Elimination}
\label{sec:satfol:satisfiability-by-clause-elimination}

\looseness=-1
The main approaches to show satisfiability of a first-order problem are to
produce either a finite Herbrand model or a saturated clause set. Saturations
rarely occur except for very small problems or within decidable fragments. In
this section, we explore an alternative approach that establishes satisfiability
by iteratively removing clauses while preserving
%satisfiability and
unsatisfiability,
until the clause set has been transformed into the empty set. So
far, this technique has been studied only for QBF~\cite{hsb-14-QRAT}. We show that
\emph{blocked clause elimination} (BCE)
% for first-order logic
can be used for this purpose. It can efficiently solve some problems for which
the saturated set would be infinite. However, it can break the refutational
completeness of a saturation prover. We conclude with a procedure that
transforms a finite Herbrand model into a sequence of clause elimination steps
ending in the empty clause set, thereby demonstrating the theoretical power of
clause elimination.

Kiesl et al.\ \cite{ksstb-2017-blockedfol} generalized blocked clause
elimination to first-order logic. Their generalization uses flat
$L$-resolvents, an extension of flat resolvents that resolves a single
literal~$L$ against $m$~literals of the other clause.

\begin{defi}
   Let $C = L \cor C'$ and $D = L_1 \cor \cdots \cor L_m \cor D'$, where
   (1)~$m \ge\nobreak 1$,
   (2) the literals $L_i$ are of opposite polarity to $L$,
   (3) $L$'s atom is $\cst{p}(\tuplen{s})$,
   (4) $L_i$'s atom is $\cst{p}(\tuple{t_i}{})$ for each~$i$, and
   (5) $C$ and $D$ have no variables in common.
   The clause
   $\bigl(\bigvee_{i=1}^m \bigvee_{j=1}^n
     \neqlit{s_j}{t_{ij}}\bigr) \cor C' \cor D'$
   is a \emph{flat $L$-resolvent} of $C$~and~$D$.
\end{defi}

\begin{defi}
\label{def:equality-blocked}
\begin{conf}%
   A clause $C = L \cor C'$ is (\emph{equality-})\emph{blocked} by $L$ in a clause
   set $N$ if all flat $L$-resolvents between $C$ and clauses in $N \setminus \{C\}$ are
   tautologies.
\end{conf}%
\begin{rep}%
   Let $C = L \cor C'$ be a clause and $N$ be a clause set.
   Let $N'$ consist of all clauses from $N \setminus \{C\}$ with their variables
   renamed so that they share no variables with $C$.
   The clause $C$ is (\emph{equality-})\emph{blocked} by $L$ in the set $N$ if
   all flat $L$-resolvents between $C$ and clauses in $N'$ are tautologies.
\end{rep}
\end{defi}

Removing a blocked clause from a set preserves unsatisfiability
\cite{ksstb-2017-blockedfol}. Kiesl et al.\ evaluated the effect of removing all
blocked clauses as a preprocessing step and found that it increases
\confrep{}{the }prover's success rate.

In fact, there exist satisfiable problems that cannot be saturated in finitely
many steps regardless of the calculus's parameters but that can be reduced to
an empty, vacuously satisfiable problem through blocked clause elimination.

\begin{exa}
   \label{example:infinite-blocked-set}
   \looseness=-1
   Consider the clause set $N$ consisting of $C = \pospredlit{\cst{p}(x,x)}$ and
   $D = \negpredlit{\cst{p}(y_1,y_3)} \cor \pospredlit{\cst{p}(y_1,y_2)} \cor
   \pospredlit{\cst{p}(y_2,y_3)}$. Note that if no literal is
   selected, all literals are eligible for superposition. In
   particular, the superposition of
   $\pospredlit{\cst{p}(x,x)}$ into $D$'s negative literal
   eventually needs to be performed regardless of the chosen
   selection function or term order, with the conclusion $E_1 =
   \pospredlit{\cst{p}(z_1,z_2)} \cor \pospredlit{\cst{p}(z_2,z_1)}$. Then,
   superposition of $E_1$ into $D$ yields $E_2 = \pospredlit{\cst{p}(z_1,z_2)}
   \cor \pospredlit{\cst{p}(z_2,z_3)} \cor \pospredlit{\cst{p}(z_3,z_1)}$.
   Repeating this process yields infinitely many clauses $E_i =
   \pospredlit{\cst{p}(z_1,z_2)} \cor \cdots \cor
   \pospredlit{\cst{p}(z_i,z_{i+1})} \cor \pospredlit{\cst{p}(z_{i+1},z_1)}$
   that cannot be eliminated using standard redu\-ndancy-based techniques.
\end{exa}

In the example above, the clause $D$ is blocked by its second or third
literal. If we delete $D$, $C$ becomes blocked in turn. Deleting
$C$ leaves us with the empty set, which is vacuously satisfiable. The example
suggests that using BCE during saturation might help focus the proof search.
Indeed, Kiesl et al.\ ended their investigations by asking whether BCE can be
used as an inprocessing technique in a saturation prover. Unfortunately,
in general the answer is no\confrep{.}{:}

\begin{exa}
   \label{example:bce-completeness}
   Consider the unsatisfiable set $N=\{ C_1, \ldots,\allowbreak C_6 \}$, where
%   \begin{linenomath*}
   \begin{align*}
      C_1 &= \negpredlit{\cst{c}} \cor \pospredlit{\cst{e}} \cor \MAX{\negpredlit{\cst{a}}} 
      & C_2 &= \negpredlit{\cst{c}} \cor \MAX{\negpredlit{\cst{e}}}
      & C_3 &= \pospredlit{\cst{b}} \cor \MAX{\pospredlit{\cst{c}}}
      \\
      C_4 &= \negpredlit{\cst{b}} \cor \MAX{\negpredlit{\cst{c}}}
      & C_5 &= \pospredlit{\cst{a}} \cor \MAX{\pospredlit{\cst{b}}} 
      & C_6 &= \pospredlit{\cst{c}} \cor \MAX{\negpredlit{\cst{b}}} 
         &
   \end{align*}
%   \end{linenomath*}
   Assume the simplification ordering $\cst{a} \prec \cst{b} \prec \cst{c} \prec
   \cst{d} \prec \cst{e}$ and the selection function that chooses the last negative literal of a clause as
   presented. Gray boxes indicate literals that can take part in superposition
   inferences. Only two superposition inferences are
   possible:\ from $C_3$ into $C_4$, yielding the tautology $C_{7} = \pospredlit{\cst{b}} \cor
   \MAX{\negpredlit{\cst{b}}}$, and from $C_5$ into $C_6$, yielding $C_{8} = \pospredlit{\cst{a}} \cor
   \MAX{\pospredlit{\cst{c}}}$. Clause $C_{7}$ is
   clearly redundant, whereas $C_{8}$ is blocked by its first literal. If we
   allow removing blocked clauses, the prover enters a loop: $C_{8}$ is repeatedly generated and 
   deleted. Thus, the prover will never generate the
   empty clause for this unsatisfiable set.
\end{exa}

   \newcommand{\aset}{\mathcalx{A}}
   \newcommand{\pset}{\mathcalx{P}}
   As with hidden tautologies, removing blocked clauses breaks the invariant of
   the given clause procedure that all inferences between clauses in $\aset$ are
   redundant. To see this, assume the setting of Example
   \ref{example:bce-completeness}, and let $\pset = N$ and $\aset = \emptyset$.
   Assume $C_1, C_2, C_3$ are moved to the active set. As there are
   no possible inferences between them, the proof state becomes $\aset = \{C_1,
   C_2, C_3\}$ and $\pset = \{C_4,C_5,C_6\}$. After $C_4$ is moved to
   $\aset$, the conclusion $C_7$ is computed, but it is not added to $\pset$ as it
   is redundant. Moving $C_5$ to $\aset$ produces no new conclusions, but after
   $C_6$ is moved, $C_{8}$ is produced. However, if we allow eliminating blocked
   clauses, it will not be added to $\pset$ as it is blocked. The prover then
   terminates with $\aset=N$ and $\pset=\emptyset$, even though the original set
   $N$ is unsatisfiable.

Although using BCE as inprocessing breaks the completeness of superposition in
general, it is conceivable that a well-behaved fragment of BCE might exist.
This could be investigated further.

% @MARIJN : Can you please motivate this part a bit better and
%           add the necessary references? --PV
% In particular, why do we care about short proofs (in FOL)? --JB
% Short proofs are often easier to generate, process, and understand than long
% proofs. SAT solvers can often generate short refutational proofs by adding and
% removing well-chosen clauses using satisfiability-preserving techniques.
% Similarly, by adding some clauses, we can transform a first-order problem so
% that all clauses can be removed by BCE. Since the empty clause set is trivially
% satisfiable, this approach produces easily checkable proofs of satisfiability.

Not only can BCE prevent infinite saturation (Example \ref{example:infinite-blocked-set}),
but it can also be used to convert a finite Herbrand model
into a certificate of clause set satisfiability. The certificate uses only
blocked clause elimination and addition, in conjunction with a transformation
to reduce the clause set to an empty set. This theoretical result
explores the relationship between Herbrand models and satisfiability certificates
based on clause elimination and addition. It is conceivable that it can form
the basis of an efficient way to certify Herbrand models.

\looseness=-1 
In propositional logic, \emph{asymmetric literals} can be added to
or removed from clauses, retaining the equivalence of the resulting clause set with
the original one. Kiesl and Suda \cite{ks-2017-unif-principle} described an extension of
this technique to first-order logic.
Their definition of asymmetric literals can be relaxed to
allow the addition of more literals, but the resulting set is then only
equisatisfiable to the original one, not equivalent.
This in turn allows us to show that a problem is satisfiable by
reducing it to an empty problem, as is done in some SAT solvers.

For the rest of this section, we work with clausal first-order logic
\relax{without equality}. We use \relax{Herbrand models} as canonical
representatives of first-order models, recalling that every
satisfiable set has a Herbrand model \cite[Sect.~5.4]{mf-1996-fol}.

\begin{defi}
   \looseness=-1
   A literal $L$ is a \emph{global asymmetric literal} (GAL) for a clause $C$
   and a clause set $N$ if for every ground instance $C\sigma$ of $C$, there
   exists a ground instance $D\varrho \cor L'\varrho$ of $D \cor L' \in N \setminus \{C\}$ such that $D\varrho
   \subseteq C\sigma$ and $\neglit L'\varrho = L\sigma$.
 \end{defi}
\confrep{}{}

Every asymmetric literal is GAL, but the converse does not hold:

\begin{exa}
   \looseness=-1
   Consider a clause $C = \pospredlit{\cst{p}(x,y)}$ and a clause set $N = \{\pospredlit{\cst{q}} \cor
  \pospredlit{\cst{p}(\cst{a},\cst{a})}\}$. Then, $\negpredlit{\cst{q}}$ is not an asymmetric literal
  for $C$ and $N$, but it is a GAL for $C$ and $N$.
\end{exa}

Adding and removing GALs preserves and reflects satisfiability:

\begin{theorem}
   \label{thm:gal}
   If $L$ is a GAL for the clause $C$ and the clause set $N$, then the set $(N \setminus
   \{C\}) \cup\allowbreak \{ C \cor\nobreak L \}$ is satisfiable if and only if $N$ is satisfiable.
\end{theorem}
\begin{rep}%
\begin{proof}
   \looseness=-1
   Let $N' = N \setminus \{C\} \cup \{ C \cor L \}$.
   Non-trivial direction is proving that if $N'$ has a model, so does
   $N.$ If $N'$ has a model, it has an Herbrand model $\ourmodel$. Clearly,
   $\ourmodel$ satisfies every clause in $N$, with the possible exception of
   $C$. Assume there exists a ground instance $C\sigma$ falsified by
   $\ourmodel$. Since $L$ is a GAL for $C$ and $N$, then there exists a ground
   instance $D\tau \cor L'\tau$ of $D \cor L' \in N'$ and $\neg L'\tau =
   L\sigma$. If $\ourmodel \models
   L'\tau$, for $C\sigma \cor L\sigma$ to be satisfied by $\ourmodel$, some
   literal in $C\sigma$ must be satisfied by $\ourmodel$, contradicting that
   $C\sigma$ is falsified by $\ourmodel$. If $\ourmodel \notmodels L'\tau$
   then some literal in $D\tau$ must be satisfied. As $D\tau \subseteq C\sigma$,
   we again get the same contradiction. Therefore, $\ourmodel$ satisfies $N$,
   as needed.\qed
 \end{proof}

\end{rep}
\looseness=-1
For first-order logic without equality, a clause $L \cor C$ is blocked if all its
$L$-resolvents are tautologies \cite{ksstb-2017-blockedfol}. The $L$-resolvent
between $L \cor C$ and $\neglit{L_1} \cor \cdots \cor \neglit{L_n} \cor
D$ is $(C \cor D)\sigma$, where $\sigma$ is the most general unifier
of the literals $L, L_1, \ldots, L_n$ \cite{bg-01-resolution}.
Given a Herbrand model $\ourmodel$ of a problem, the following procedure
removes all clauses while preserving satisfiability:

\begin{enumerate}
   \item \label{item:step1}
   \looseness=-1
   Let $\cst{q}$ be a fresh predicate symbol. For each atom
   $\cst{p}(\tupleempty{s})$ in the Herbrand universe: If
   $\ourmodel \models \cst{p}(\tupleempty{s})$, add the clause $\cst{q} \cor
   \cst{p}(\tupleempty{s})$; otherwise, add %the clause
   $\cst{q} \cor \neglit{\cst{p}(\tupleempty{s})}$. Adding either clause
   preserves satisfiability as both are blocked by~$\cst{q}$.
% \pagebreak[2]
   \smallskip
   \item \label{item:step2}
   Since $\ourmodel$ is a model, for each ground instance $C\sigma$, there exists a clause
   $\cst{q} \cor L$ with $L \in C\sigma$. We can transform $C \in N$ into
   $C \cor \neglit{\cst{q}}$, since $\neglit{\cst{q}}$ is a GAL for $C$ and $N$.

   \smallskip
   \item \label{item:step3}
   \looseness=-1
   Consider the clause $\cst{q} \cor L$ added by step \ref{item:step1}. Since
   $L$ is ground and no clause $\cst{q} \cor \neglit{L}$ was added (since $\ourmodel$ is a
   model), the only $L$-resolvents are against clauses added by step~\ref{item:step2}.
   Since all of those clauses contain $\neglit{\cst{q}}$, the resolvents are tautologies. 
   Thus, each $\cst{q} \cor L$ is blocked and can be removed in turn.

   \smallskip
   \item The remaining clauses all contain the literal $\neglit{\cst{q}}$. They can be removed
   by BCE as well.
\end{enumerate}


The procedure is limited to the first-order logic without equality, since step
\ref{item:step3} is justified only if $L$ is a predicate literal. (Otherwise,
$L$ cannot block clause $\cst{q} \cor L$ \cite{ksstb-2017-blockedfol}.) The
procedure also terminates only for finite Herbrand models. 

% \begin{rep}

\begin{exa}
   Consider the satisfiable clause set $N = \{\pospredlit{\cst{r}(x)} \cor
   \pospredlit{\cst{s}(x)}{,}\; \negpredlit{\cst{r}(\cst{a})}{,}\allowbreak\;
   \negpredlit{\cst{s}(\cst{b})}\}$ and a Herbrand model $\ourmodel$ over
   $\{\cst{a}, \cst{b}, \cst{r}, \cst{s}\}$ such that
   $\cst{r}(\cst{b})$ and $\cst{s}(\cst{a})$ are the only true atoms in
   $\ourmodel$. We show how to remove all clauses in $N$ using $\ourmodel$ 
   by following the procedure above.

   Let $N_{\ourmodel} = \{ \cst{q} \cor \negpredlit{\cst{r}(\cst{a})}{,}\; \cst{q}
   \cor \pospredlit{\cst{r}(\cst{b})}{,}\; \cst{q} \cor
   \pospredlit{\cst{s}(\cst{a})}{,}\; \cst{q} \cor \negpredlit{\cst{s}(\cst{b})}
   \}$. We set $N \gets N \cup N_{\ourmodel}$. This preserves satisfiability
   since all clauses in $N_{\ourmodel}$ are blocked. It is easy to check that
   $\negpredlit{\cst{q}}$ is GAL for every clause in $N \setminus N_{\ourmodel}$. The
   only substitutions that need to be considered are $\{x \mapsto \cst{a} \}$ and $\{x \mapsto
   \cst{b} \}$ for $\pospredlit{\cst{r}(x)} \cor \pospredlit{\cst{s}(x)}$.
   So we set $N \gets \{\negpredlit{\cst{q}} \cor \pospredlit{\cst{r}(x)} \cor \pospredlit{\cst{s}(x)}{,}\;
   \negpredlit{\cst{q}} \cor \negpredlit{\cst{r}(\cst{a})}{,}\;
   \negpredlit{\cst{q}} \cor \negpredlit{\cst{s}(\cst{b})}\} \cup N_{\ourmodel}$.
   Clearly, all clauses in $N_{\ourmodel}$ are blocked, so we set $N \gets N \setminus
   N_{\ourmodel}$. All clauses remaining in $N$ have a literal $\negpredlit{\cst{q}}$
   and can be removed, leaving $N$ empty as desired.
\end{exa}

% The procedure works only for first-order logic without equality, because it
% relies on adding and removing blocked clauses. In step~\ref{item:step3}, binary
% clauses $\cst{q} \cor L$ are blocked by $L$. If $L$ were a
% functional literal, by definition $\cst{q} \cor L$ would not be blocked.
% \end{rep}

% \looseness=-1
% For many problems model can be described only using infinite Herbrand universe and the described technique for
% certifying satisfiability is not applicable. However, in cases of finite
% Herbrand universes, provers that support GAL addition and blocked clause
% elimination and addition can certify that a given interpretation is
% indeed a Herbrand model.
 
 
\section{Implementation}
\label{sec:satfol:implementation}
% {\large\bf {TODO: Shorten and introduce SPPE here in a sentence}}

\newcommand{\impmap}{\textit{Imp}}

Hidden-literal-based, predicate, \begin{qle}quasipure, \end{qle}and blocked clause
elimination all admit efficient implementations in a superposition
prover. In this section, we describe how to implement the first
\begin{noqle}two\end{noqle}\begin{qle}three\end{qle} sets of
techniques. For BCE, we refer to Kiesl et al.\ \cite{ksstb-2017-blockedfol}.
All techniques are implemented in the Zipperposition prover
\cite{sc-supind-17}. Zipperposition is designed for fast prototyping
of improvements to superposition, but it implements many of the most successful
heuristics from the E prover \cite{scv-19-e23} and has recently become
quite competitive \cite{gs-19-casc27}.

\ourpara{Hidden-Literal-Based Elimination}
% 
% Heule et al.\ \cite{hjb-2010-cl-elim} show how hidden tautologies can be removed
% without creating the binary implication graph. For each literal~$L$ occurring in
% the clause set $N$, they compute the set $\PHL(L,N)$. Then for each $\neglit{L}'
% \in \PHL(L,N)$, they label $L'$ with $L$. A clause $C$ is a hidden tautology if
% there are two literals $L,L' \in C$ such that $L'$ is labeled with $L$. The
% described approach can be generalized to first-order logic and used to implement
% all rules from Sect.~\ref{sec:satfol:hidden-literal-based-elimination}.
% 
For HLBE, an efficient representation of $\HL(L,N)$ is crucial. Because
this set may be infinite, we underapproximate it by
restricting the length of the
transitive chains via a parameter $K_\mathrm{len}$. Given the current
clause set $N$, the finite map $\impmap[L']$ associates with each literal $L'$
a set of pairs $(L,M)$ such that $L' \bigfimpl{N}^k L$, where $k \leq
K_\mathrm{len}$ and $M$ is the multiset of clauses used to derive $L' \bigfimpl{N}^k L$.
Moreover, we consider only transitions of type (1) (as per
Definition~\ref{def:hl-fo}).
%Intuitively,
%$\impmap[L']$ contains all literals which can be reached from $L'$ following $k$
%edges of the generalized binary implication graph.
The following algorithm
maintains $\impmap$ dynamically, updating it as the prover derives and deletes
clauses.
It depends on the global variable $\impmap$ and the parameters
$K_\mathrm{len}$ and $K_{\mathrm{imp}}$.

% By choosing a limit $K_\mathrm{len}$ we can create a map $\impmap[L]$, which maps a literal $L$
% to a finite a set of pairs $(L', M)$ such that
% $\impk{L}{L'}{i}{M}$ where $i \leq K_\mathrm{len}$. It is important to record clauses used
% in deriving the implication, as this information is necessary to construct the
% proof. Mapping $\impmap$ is constructed so that whenever $(L', M) \in
% \impmap[L]$, then $L \in \HL(L',M)$.
% Proof by very simple induction

\looseness=-1

\newcommand{\vn}[1]{\ensuremath{\mathit{#1}}} %variable name
\algrenewcommand\alglinenumber[1]{{\color{gray} \footnotesize #1}}
\algrenewcommand\algorithmicindent{0.75em}

%%% TYPESETTING: hack
\vskip\abovedisplayskip
\kern-.5\jot

%% \begin{nolinenumbers}%\begin{figure}[h!]
\begin{algorithmic}[5]

%   \Procedure{DecomposeNegLit}{$c$, $C$}
%   \State $\vn{res} \gets \{(c, C)\}$
%   \ForAll{non-empty contexts $u$ such that $c = \neqlit{u[s]}{u[t]}$}
%       \State $\vn{res} \gets \{ (\neqlit{s}{t}, C) \} \cup \vn{res}$
%   \EndFor
%   \State \Return $\vn{res}$
%   \EndProcedure

%  \kern1\jot

  \Procedure{AddImplication}{$L_\mathrm{a}{,}\> L_\mathrm{c}{,}\> C$}
    \If{$\impmap[L_\mathrm{a}\sigma] \not = \emptyset$ for some renaming $\sigma$} \label{statement:alpha}
      \State $(L_\mathrm{a}, L_\mathrm{c}) \gets (L_\mathrm{a}\sigma, L_\mathrm{c}\sigma)$
    \EndIf

   %  \vspace{0.1em}
    \If{there are no $L,L',M,\sigma$ such that $(L',M) \in \impmap[L]$,\label{statement:more-general}%
        \\\noindent\kern3.5ex $L\sigma = L_\mathrm{a}$, and $L'\sigma=L_\mathrm{c}$} 
      % \State $\vn{Inst} \gets \{(\sigma, M) \mid (L_\mathrm{c}\sigma, M) \in \impmap[L_\mathrm{a}\sigma]\}$
      \ForAll{$(\sigma, M)$ such that $(L_\mathrm{c}\sigma, M) \in \impmap[L_\mathrm{a}\sigma]$} \label{statement:instances}
        \State {erase all $(L',M')$ such that $M \subseteq M'$ from $\impmap[L_{\mathrm{a}}\sigma]$}
      \EndFor
      
      \ForAll{$L$ such that $(L', M) \in \impmap[L]$\\ \noindent\kern10ex and $L_\mathrm{a}\sigma = L'$ for some $\sigma$} \label{statement:transitivity}
        \If{$|M| < K_\mathrm{len}$}
          \State $\impmap[L] \gets \impmap[L] \cup \{(L_\mathrm{c}\sigma, M \uplus \{C\})\} $
        \EndIf
      \EndFor

      \ForAll{$L$ such that $\impmap[L] \not= \emptyset$\\\noindent\kern10ex and $L\sigma = L_\mathrm{c}$ for some $\sigma$} \label{statement:triggered}
         % \If{$|M| < K_\mathrm{len}-1$}
         \State $\vn{Concl} \gets \{(L'\sigma, M \uplus \{\mathit{C}\}) \mid$%
         \\\noindent\kern14.7ex $(L',M) \in \impmap[L], |M| < K_\mathrm{len}  \}$
         \State $\impmap[L_\mathrm{a}] \gets \impmap[L_\mathrm{a}] \cup  \vn{Concl}$
         % (L'\sigma, M \cup \{\mathit{C}\}) $
         % \EndIf
      \EndFor
      
      \State $\vn{Congr} \gets \{
         (\neqlit{s}{t}, \{C\}) \mid \exists u.\, L_\mathrm{c} = \neqlit{u[s]}{u[t]}
         \}$
      \State $\impmap[L_\mathrm{a}] \gets \impmap[L_\mathrm{a}] \cup \{ (L_\mathrm{c},\{C\}) \} \cup \vn{Congr}$ \label{statement:decomposition}

    \EndIf

  \EndProcedure

  \kern1\jot
 
  \Procedure{TrackClause}{$C$}
    \If{$C$ = $L_1 \cor L_2$}
      \State \textsc{AddImplication}($\neglit{L_1}$, $L_2$, $C$)
      \State \textsc{AddImplication}($\neglit{L_2}$, $L_1$, $C$)
      \If{$L_2 = \neglit L_1\sigma$ for some nonidempotent $\sigma$}  \label{statement:self-impl}
      \ForAll{$i \gets 1$  to $K_\mathrm{imp}$}
         \State $L_2 \gets L_2\sigma$
         \State \textsc{AddImplication}($\neglit L_1$, $L_2$, $C$)
      \EndFor
      \EndIf
    \EndIf
  \EndProcedure

  \kern1\jot

  \Procedure{UntrackClause}{$C$}
   %  \ForAll {$L_\mathrm{a}, L_\mathrm{c}, M$ such that $ (L_\mathrm{c}, M) \in \impmap[L_\mathrm{a}]$}
   \ForAll{$L_\mathrm{a}, L_\mathrm{c}, M$ such that $(L_\mathrm{c},M) \in \impmap[L_\mathrm{a}]$}
      \If{$C \in M$}
         \State erase $(L_\mathrm{c},M)$ from $\impmap[L_\mathrm{a}]$
      \EndIf
      % \EndFor
    \EndFor
  \EndProcedure
\end{algorithmic}
%\end{nolinenumbers}
%\end{figure}

%% TYPESETTING: hack
\vskip-1.5\jot
\vskip\belowdisplayskip

The algorithm views a clause $L \cor L'$ as two implications
$\neglit{L} \impl L'$ and $\neglit{L'} \impl L$.
%
It stores only one entry for all literals equal up to variable renaming (line
\ref{statement:alpha}). Each implication $L_\mathrm{a} \impl L_\mathrm{c}$
represented by the clause is stored only if its generalization is not present in
$\impmap$ (line~\ref{statement:more-general}). Conversely, all instances of the
implication are removed (line~\ref{statement:instances}).

Next, the algorithm
finds each implication stored in $\impmap$ that can be linked to $L_\mathrm{a} \impl L_\mathrm{c}$: Either
$L_\mathrm{c}$ becomes the new consequent (line~\ref{statement:transitivity}) or
$L_\mathrm{a}$ becomes the new antecedent (line~\ref{statement:triggered}). If
$L_\mathrm{c}$ can be decomposed into $\neqlit{u[s]}{u[t]}$, rule~(3) of
Definition \ref{def:hl-fo} allows us to store $\neqlit{s}{t}$ in
$\impmap[L_\mathrm{a}]$ (line~\ref{statement:decomposition}). This is an
exception to the idea that transitive chains should use only rule~(1). The
application of rule~(3) does not count toward the bound $K_\mathrm{len}$.
If $L_\mathrm{a}$ is of the form $\eqlit{u[s]}{u[t]}$, then
$\impmap$ could be extended so that $\impmap[\eqlit{s}{t}] =
\impmap[L_\mathrm{a}]$, but this would substantially increase $\impmap$'s
memory footprint.

In first-order logic, different instances of the same clause can
be used along a transitive chain. For example, the clause $C =
\negpredlit{\cst{p}(x)} \cor \pospredlit{\cst{p}(\cst{f}(x))}$ induces $\cst{p}(x) \bigfimpl{M}^i \cst{p}(\cst{f}^i(x))$ for all $i$.
The algorithm discovers such self-implications (line~\ref{statement:self-impl}): For
each clause~$C$ of the form $\neglit{L} \cor L\sigma$, where $\sigma$ is
some nonidempotent substitution, the entires $(L\sigma^2, \{C\}),\allowbreak
\ldots,\allowbreak (L\sigma^{K_{\mathrm{imp}} + 1}, \{C\})$ are added to
$\impmap[L]$, where $K_{\mathrm{imp}}$ is a parameter. 

\newcommand{\concl}{\ensuremath{\impmap^{-1}}}

\looseness=-1
To track and untrack clauses efficiently, we implement the mapping $\impmap$ as a
nonperfect discrimination tree \cite{rsv-2001-term-indexing}. Given a query
literal $L$, this indexing data structure efficiently finds all literals $L'$ such that
for some $\sigma$, $L'\sigma = L$ and $\impmap[L'] \not= \emptyset$. We can use
it to optimize all lookups except the one on line~\ref{statement:transitivity}.
For this remaining lookup, we add an
index \concl{} that inverts $\impmap$, i.e., $\concl[L] = \{ L' \mid
\impmap[L'] = (L,M) \text{ for some } M \}$. To avoid sequentially going through
all entries in $\impmap$ when the prover deletes them, for each clause $C$
we keep track of each literal $L$ such that $C$ appears in $\impmap[L]$.
Finally, we limit the number of entries stored in $\impmap[L]$ -- by default,
up to 48 pairs in each $\impmap[L]$ are stored.

\newcommand{\unitset}{\ensuremath{\textit{Unit}}}
\begin{conf} 
\looseness=-1   
Rules \infname{HLE} and \infname{HTR} have a simple
implementation based on $\impmap$ lookups. To implement \infname{UnitHLE} and
\infname{UnitHTR}, we maintain the index \unitset, containing literals
$L_\mathrm{c}\sigma$, such that $(L_\mathrm{c}, M) \in \impmap[L_\mathrm{a}]$ for
some $M$ and $L_\mathrm{a}$ and $\sigma$ is the most general unifier of $L'$ and
$L_\mathrm{a}$, for some unit clause $\{ L' \}$. The implementation
of \infname{FLE} and \infname{FLR} also uses \unitset:
When $(L',M)$ is added to $\impmap[L]$, we
check if $(\neglit{L'},M') \in \impmap[L]$ for some $M'$. If so, $\neglit{L}$
is added to $\unitset$.
\end{conf}
\begin{rep}
   
   To implement the \infname{HLE} rule, we use $\impmap[L]$ as
   follows: Given a clause $C = L \cor L' \cor C'$, if there are two literals
   $L_1,L_2$ and a substitution $\sigma$ such that $(L_2, M) \in \impmap[L_1]$,
   $C \not\in M$,  $L_1\sigma = L$, and $L_2\sigma = L'$, we remove $L$ from $C$.
   Literal $L$ can also be removed if $L_1\sigma = \neglit{L'}$ and $L_2\sigma = \neglit{L}$. 
   Rule \infname{HTR} is implemented analogously.

% @PETAR: Used to say "UnitHTR rules rely", but there's only one. Double-check. --JB
   The \infname{UnitHTR} rule relies on maintaining the index $\unitset$, which is
   built as follows. Whenever the prover derives a unit clause $C = \{ L \}$, we
   find all entries $L_\mathrm{a}$ in $\impmap$ such that $L_\mathrm{a}$ and $L$
   are unifiable with the most general unifier $\sigma$. Then, we set $\unitset
   \gets \unitset \cup \{ (L_\mathrm{c}\sigma, M \cup \{ C \} ) \mid
   (L_\mathrm{c}, M) \in \impmap[L_\mathrm{a}] \}$. Given a clause $L \cor C'$
   we apply \infname{UnitHLE} by looking for $(L', M) \in \unitset$ such that
   $L'\sigma = \neglit{L}$, for some substitution $\sigma$; we apply
   \infname{UnitHTR} by looking for $L'$ such that $L'\sigma = L$.  The sets
   stored together with literals in $\unitset$ are used for building the proof
   object and to remove literals from $\unitset$ once a clause from the given
   set becomes redundant.

   The same data structure is used
   for supporting \infname{FLE} and \infname{FLR}. When $(L',M)$ is added to $\impmap[L]$, we
   check whether $(\neglit{L'},M') \in \impmap[L]$ for some $M'$. If so, $(\neglit{L}, M \cup M')$
   is added to $\unitset$.
\end{rep}


% % Using this algorithm, hidden-literal-based elimination can be implemented
% % efficiently. We implement them as simplifications, used during saturation.
% \begin{rep}
% We implement \infname{ShallowHLE} by checking if there are two literals $L,L'
% \in C$ such that for some substitution $\sigma$, and literals $L_1$ and $L_2$,
% $(L_2, M) \in \impmap[L_1]$ and $L_1\sigma = L$, $L_2\sigma = L'$, $L$ can be
% removed from $C$. When $(L',M)$ is added to $\impmap[L]$, Zipperposition checks
% if $(\neglit{L'},M) \in \impmap[L]$ for some $M$. If so, it notes that there
% are complementary literals in $\HL(\neglit{L},M)$. This information is used to
% implement \infname{FLD}. For \infname{HTR}, a check whether there are $L,L' \in
% C$ such that $(L_1, M) \in \impmap[L_0]$, where $\neglit{L_0\sigma} = L$ and
% $L_1\sigma = L'$ is performed. Rule \infname{DeepHLE} is not implemented.

% Rules \infname{UnitHLE} and \infname{UnitHTR} are implemented as well. For
% \infname{UnitHLE}, given a clause $L \cor C$ we do as follows. First we try to
% find a literal $L_0$ such that $\neglit{L_0\sigma} = L$ for some $\sigma$ and
% $(L_1,M) \in \impmap[L_0]$ such that $L_1\sigma$ is subsumed by a unit clause
% indexed by Zipperposition. If this does not succeed, we try finding a literal
% $L_0$ such that $L_0\sigma = L$ for some $\sigma$ and $(L_1,M) \in \impmap[L_0]$
% such that $\neglit{L_1}$ is subsumed by a unit clause. Rule \infname{UnitHTR} is
% implemented analogously.
% \end{rep}

In propositional logic, the conventional approach constructs the
\emph{binary implication graph} for the clause set~$N$
\cite{hjb-2011-big-simplification}, with edges $(\neglit{L}, L')$ and
$(\neglit{L'}, L)$ whenever $L \cor L' \in N.$ To avoid traversing the graph
repeatedly, solvers rely on timestamps to discover connections between
literals. This relies on syntactic literal comparisons, which is very
fast in propositional logic but not in first-order logic, because of
substitutions and congruence.

\ourpara{Predicate Elimination}
% 
To implement portfolio predicate elimination, we maintain a record for each
predicate symbol $\cst{p}$ occurring in the problem with the
following fields:\
set of definition clauses for $\cst{p}$, set of nondefinition clauses in which
$\cst{p}$ occurs once, and set of clauses in which $\cst{p}$ occurs more than
once. These records are kept in a priority queue, prioritized by properties such
as presence of definition sets and number of estimated resolutions. If
$\cst{p}$ is the highest-priority symbol that is eligible for SPE or DPE, we
eliminate it by removing all the clauses stored in $\cst{p}$'s record from the
proof state and by adding flat resolvents to the passive set.
Eliminating a symbol might make another symbol eligible.
%In this case, the updated record is put on the processing queue.

As an optimization, predicate elimination keeps track only of symbols that appear
at most $K_\mathrm{occ}$~times in the clause set. For inprocessing, we use
signals that the prover emits whenever a clause is added to or removed from
the proof state and update the records. At the beginning of the
1st, $(K_\mathrm{iter} + 1)$st, $(2 K_\mathrm{iter} + 1)$st, $\dots{}$ iteration of the given clause procedure's
loop body, predicate elimination is systematically applied to the entire proof
state. The first application of inprocessing amounts to preprocessing. By
default, $K_\mathrm{occ} = 512$ and $K_\mathrm{iter}=10$. The same ideas and
limits apply for blocked clause elimination.

The most important novel aspect of our predicate elimination implementation is
recognizing the definition clauses for symbol $\cst{p}$ in a clause set $N$, which is performed
as follows:
\begin{enumerate}
   \item Let $G = \{ C \mid C = \arbpredlit{\cst{p}(\tupleempty{x})} \cor C', C \in
   N$, no variable repeats in $\tupleempty{x}$, and variables of $C'$ are among
   $\tupleempty{x}\}$. If $G$ is empty, report failure; otherwise continue.

   \smallskip
   \item Rename all clauses in $G$ so that their only variables are~$\tupleempty{x}$.

   \smallskip
   \item\label{step:encoding} Let $\lfloor a \rfloor$ be a function that assigns
   a propositional variable to each atom $a$. This function is lifted to
   literals by assigning $\lfloor \neg a \rfloor  = \neg x$, if $\lfloor a
   \rfloor = x$, and to clauses pointwise. Furthermore,  
   let $E = \{ \lfloor C' \rfloor \mid
   \arbpredlit{\cst{p}(\tupleempty{x})} \cor C' \in G \}$. If $E$ is satisfiable,
   report failure. Else, let $E'$ be the unsatisfiable core of $E$
   and $G'$ the set of corresponding first-order clauses and continue.

   \smallskip
   \item If all resolvents in $G'_{\pospredlit{\cst{p}}} \flatres_{\!\cst{p}}
   G'_{\negpredlit{\cst{p}}}$ are tautologies, then $G'$ is the definition set
   for symbol $\cst{p}$. Else, report failure.
\end{enumerate}

The invalidity of set $E$ from step \ref{step:encoding} is checked using a SAT
solver, which is already integrated in Zipperposition. As modern theorem provers
(such as E or Vampire) also use SAT solvers, the method can easily be
implemented.

During experimentation, we noticed that recognizing definitions of symbols that
occur in the conjecture often harms performance. Thus, Zipperposition
recognizes definitions only for the remaining symbols.

\begin{qle}
\ourpara{Quasipure Literal Elimination}

A simple and efficient implementation of QLE uses a SAT solver. Let $N$ be a
finite $\mathrm{\Sigma}$-clause set. Without loss of generality, we can assume
that $\mathrm{\Sigma}$ is finite. The signature of the SAT problem consists of
the variables $\cst{p}^+, \cst{p}^-$ for each predicate symbol $\cst{p}$ in
$\mathrm{\Sigma}$, where $\cst{p}^s$ means ``$\cst{p}$ is quasipure with
polarity~$s$.'' The SAT problem consists of the following clauses:
%
\begin{enumerate}
\itemsep1\jot
\item For each clause $\arbpredlit{\cst{q}_1(\ldots)} \lor \cdots \lor
  \arbpredlit{\cst{q}_n(\ldots)} \allowbreak\lor C$ contained in $N$, where
  the first $n$~literals have polarities $s_1,\dots,s_n$, respectively,
  and $C$ consists of functional literals,
  generate $n$~clauses of the form
  \[\cst{q}_1^{s_1}\lor\cdots\lor\cst{q}_{i-1}^{s_{i-1}}
    \lor
    \neglit{\cst{q}_i^{-s_i}}
    \lor
    \cst{q}_{i+1}^{s_{i+1}}\lor\cdots\lor\cst{q}_n^{s_n}
   \]
   where $-s$ flips the polarity $s$. Such a clause ensures that whenever a
   literal $\arbpredlit{\cst{q}_i(\ldots)}$ has the wrong polarity according to
   the current variable assignment, one of the other literals must be quasipure.

\item For each $\cst{p}$ in $\Sigma$, generate the clause $\lnot \cst{p}^+ \lor
  \lnot \cst{p}^-$. It ensures that a single polarity is assigned to a quasipure
  predicate.

\item Generate the clause $\cst{p}_1^+ \lor \cst{p}_1^- \lor \cdots \lor
  \cst{p}_n^+ \lor \cst{p}_n^-$, where $\{\cst{p}_1,\dots,\cst{p}_n\}$ are the
  predicate symbols in $\mathrm{\Sigma}$. It tells the SAT solver to look for a
  nontrivial solution, in which at least one predicate is quasipure.
\end{enumerate}

From a satisfying assignment, we can easily read off a predicate set $P$ and a
polarity map $s$. The process can be iterated until we reach a maximal solution,
at which point the SAT solver returns a verdict of ``unsatisfiable.''

So far, we have been unable to prove that quasipurity is NP-complete. Given the
nondeterministic nature of the naive procedure, we suspect that it is, but we
have not found a reduction from 3SAT to quasipurity. Thus, it is unclear whether
our use of a SAT solver is fully satisfactory from a theoretical point of view,
even though it works perfectly well in practice.
\end{qle}

\section{Evaluation}
\label{sec:satfol:evaluation}

% {\bf\large TODO: Evaluate \textit{real} PPE in Figure \ref{fig:preprocess} and discuss SPPE and PPE.
% References to PPE are broken now. }

\newcommand{\baseres}[0]{7897}
% \newcommand{\baseresplusfive}[0]{1563}

\newcommand{\resnum}[1]{
   \FPeval{\result}{clip(#1-\baseres)}%
   \ifnumcomp{\result}{>}{0}{$+\result$}{$\result$}%
}
\newcommand{\resnumphantom}[1]{%
   \FPeval{\result}{clip(#1-\baseres)}%
   \FPeval{\negresult}{clip(-\result)}%
   \ifnumcomp{\result}{>}{0}{\phantom{$0$}{${+}\result$}}{\phantom{$0$}{${-}\negresult$}}%
}
\newcommand{\resnumbold}[1]{
   \FPeval{\result}{clip(#1-\baseres)}%
   \FPeval{\negresult}{clip(-\result)}%
   \ifnumcomp{\result}{>}{0}{\phantom{${+}\result$}\llap{$\mathbf{\boldsymbol{+}\result}$}}{\phantom{${-}\negresult$}$\llap{\mathbf{\boldsymbol{-}\negresult}}$}%
}
\newcommand{\resnumboldphantom}[1]{%
   \FPeval{\result}{clip(#1-\baseres)}%
   \FPeval{\negresult}{clip(-\result)}%
   \ifnumcomp{\result}{>}{0}{\phantom{$0{+}\result$}\llap{$\mathbf{\phantom{0}{\boldsymbol{+}}\result}$}}{\phantom{$0{-}\negresult$}$\llap{\mathbf{\phantom{0}{\boldsymbol{-}}\negresult}}$}%
}
\newcommand{\evaluating}[0]{\textbf{?}}

\looseness=-1
We measure the impact of our elimination techniques for various values of their
parameters. As a baseline, we use Zipperposition's first-order portfolio mode,
which runs the prover in 13~configurations of
heuristic parameters in consecutive time slices. None of these configurations
use our new techniques. To evaluate a given parameter value, we fix
it across all 13~configurations and compare the results with the baseline.

\looseness=-1
The benchmark set consists of all 13\,495 CNF and FOF TPTP 7.3.0 theorems
\cite{gs-17-tptp}.
%
The experiments were carried out on StarExec servers \cite{sst-14-starexec}
equipped with Intel Xeon E5-2609 CPUs clocked at 2.40 GHz. The portfolio mode
uses a single CPU core with a CPU time limit of 180~s. The base
configuration solves \NumberOK{\baseres} problems. The values in the tables
indicate the number of problems solved minus \NumberOK{\baseres}. Thus,
positive numbers indicate gains over the baseline. The best result is shown in
bold.

\ourpara{Hidden-Literal-Based Elimination}
% 
The first experiments use all implemented HLBE rules.
To avoid overburdening Zipperposition, we can enable an option to limit the
number of tracked clauses for hidden literals. Once the limit has been reached, any
request for tracking a clause will be rejected until a tracked
clause is deleted. We can choose which kind of clauses are
tracked:\ only clauses from the active set $\mathcalx{A}$,
only clauses from the passive set $\mathcalx{P}$, or both.
We also vary the maximal implication chain length $K_\mathrm{len}$ and the
number of computed self-implications $K_\mathrm{imp}$.

In Zipperposition, every lookup for instances or generalizations of
$\eqlit{s}{t}$ must be done once for each orientation of the equation.
To avoid this inefficiency, and also because the implementation of hidden literals does not fully
exploit congruence, we can disable\pagebreak[2] tracking clauses with at least one
functional literal. Clauses containing functional literals can then still be
simplified.

Figures~\ref{fig:hle-clauses-neq} and \ref{fig:hle-clauses-eq}
%~\ref{fig:hle}
show the results, without and with functional literal tracking enabled,
for $K_\mathrm{len} = 2$ and $K_\mathrm{imp} = 0$.%
The columns specify different limits on the number of tracked clauses, with $\infty$
denoting that no limit is imposed. The rows represent different kinds of tracked clauses.
The results suggest that tracking functional literals is \NumberOK{not} worth the effort
but that tracking predicate literals \NumberOK{is}. The best improvement is observed when
both active and passive clauses are tracked. Normally DISCOUNT-loop
provers \cite{adf-1995-discount} such as Zipperposition do not simplify
active clauses using passive clauses, but here we see that this can be effective.
%
%--Raw data shows that when up to 250 clauses in either set are
%--tracked, tracking functional literals helps solve \NumberNOK{5} problems that
%--cannot be solved by a configuration that tracks only predicate literals or the
%--base configuration. This suggests that a more optimized indexing implementation
%--would help our technique reach its full potential.
Figure~\ref{fig:km-kimp} shows the impact of varying $K_\mathrm{len}$ and
$K_\mathrm{imp}$, when 500 clauses from the entire proof state are tracked.
These results suggest that computing long implication chains \NumberOK{is counterproductive.}
% Furthermore, raw data reveals that there is
% no gain in this case: not a single problem that configuration with $K_\mathrm{len}=2$ and
% $K_\mathrm{imp}=0$ cannot solve is solved when $K_\mathrm{imp}K_\mathrm{len}$ is set to 8 and
% $K_\mathrm{imp}$ to 0.

\begin{figure}[t]%
\small
\centering
\begin{tabular}[t]{l@{\hskip 1em}c@{\hskip 1em}c@{\hskip1em}c@{\hskip 1em}c}
                     & \multicolumn{4}{c}{Tracked clauses} \\
                     & $250$        & $500$         & $1000$       &$\infty$ \\ \midrule 
\strut Active       & \resnum{7883}  & \resnum{7881}   & \resnum{7889}  & \resnum{7885}    \\
\strut Passive      & \resnumphantom{7904}  & \resnum{7907}   & \resnum{7902}  & \resnum{7862}    \\
\strut Both         & \resnumbold{7909}  & \resnum{7907}   & \resnum{7904}  & \resnum{7852} \\     
\end{tabular}
\caption{Impact of the number and kinds of tracked clauses on HLBE performance, when only predicate literals are tracked}
\label{fig:hle-clauses-neq}
\end{figure}
\begin{figure}[t]%
\small
\centering
\begin{tabular}[t]{l@{\hskip 1em}c@{\hskip 1em}c@{\hskip1em}c@{\hskip 1em}c}
                     & \multicolumn{4}{c}{Tracked clauses} \\
                     & $250$                   & $500$                 & $1000$                    & $\infty$ \\ \midrule 
\strut Active       & \resnum{7887}            & \resnum{7883}         & \resnumphantom{7889}      & \resnum{7879}    \\
\strut Passive      & \resnumphantom{7892}     & \resnumphantom{7892}  & \resnum{7883}             & \resnum{7826}    \\
\strut Both         & \resnumboldphantom{7899} & \resnumphantom{7896}  & \resnumphantom{7889}      & \resnum{7818} \\ 
\end{tabular}
\caption{Impact of the number and kinds of tracked clauses on HLBE performance, when all literals are tracked}
\label{fig:hle-clauses-eq}
\end{figure}

\begin{figure}[t]
\small
\centering
\def\arraystretch{1.1}%
\begin{tabular}[t]{l@{\hskip 2em}c@{\hskip 1.5em}c@{\hskip1.5em}c@{\hskip 1.5em}c} 
                             & \multicolumn{4}{c}{Chain length $K_\mathrm{len}$} \\
                             & $1$                   & $2$                 & $4$                   & $8$ \\ \midrule 
   $K_\mathrm{imp}=0$        & \resnum{7906}         & \resnum{7907}       & \resnum{7904}         & \resnum{7902}    \\
   $K_\mathrm{imp}=1$        & \resnum{7902}         & \resnumbold{7908}       & \resnum{7904}         & \resnum{7901}    \\
   $K_\mathrm{imp}=2$        & \resnum{7903}         & \resnumbold{7908}       & \resnum{7905}         & \resnum{7905}    \\
\end{tabular}
\caption{Impact of the parameters $K_\mathrm{len}$ and $K_\mathrm{imp}$ on HLBE performance}
\label{fig:km-kimp}
\end{figure}

\begin{figure}[t!]
   \small
   \centering
   \def\arraystretch{1.1}%
   \begin{tabular}[t]{l@{\hskip 1.5em}c@{\hskip 1.5em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c}
                           &               & \multicolumn{5}{c}{Relaxed with $K_\mathrm{tol}$} \\
                           & K\&K          & $0$             & $25$           & $50$           & $100$    & $200$    \\ \midrule 
   \infname{SPE} preproc.  & \resnum{7967} & \resnum{8014}   & \resnum{8051}  & \resnum{8057}  & \resnum{8051}      & \resnum{8055} \\
   \infname{PPE} preproc.  & \resnum{7968} & \resnum{8021}   & \resnum{8057}  & \resnum{8061}  & \resnumbold{8062}      & \resnum{8059} \\
   \end{tabular}
   \caption{Impact of the choice of criterion on predicate elimination performance}
   \label{fig:criterion}
\end{figure}

\begin{figure}[t]
   \small
   \centering
   \def\arraystretch{1.1}%
   \begin{tabular}[t]{l@{\hskip 1em}c@{\hskip 1.3em}c@{\hskip 1em}c@{\hskip 1.3em}c@{\hskip 1em}c@{\hskip 1.3em}c}
                         &               &               &                &               &                    & \smash{HLBE} \\[-1pt] %% TYPESETTING
                         &               &               & \smash{SPE}    &               & \smash{PPE}        & \smash{+PPE} \\[-1pt] %% TYPESETTING
                         & BCE           & SPE           & +BCE           & PPE           & +BCE               & +BCE           \\ \midrule 
   \strut Preprocessing  & \resnum{7927} & \resnum{8051} & \resnum{8056}  & \resnum{8057} & \resnumbold{8063}  & \resnum{8059} \\
   \strut Inprocessing   & \resnum{7849} & \resnum{8037} & \resnum{8024}  & \resnum{8043} & \resnum{8028}      & \resnum{8024}     \\
   \end{tabular}
   \caption{Performance of predicate and blocked clause elimination}
   \label{fig:preprocess}
\end{figure}

\begin{qle}
\begingroup
\begin{figure}[t]
\color{red}
   \small
   \centering
   \def\arraystretch{1.1}%
   \begin{tabular}[t]{l@{\hskip 1em}c@{\hskip 1.3em}c@{\hskip 1em}c@{\hskip 1.3em}c@{\hskip 1em}c@{\hskip 1.3em}c}
                         &               &               & \smash{QLE}    \\[-1pt] %% TYPESETTING
                         & PLE           & QLE           & +PPE           \\ \midrule 
   \strut Preprocessing  & \resnum{9999} & \resnum{9999} & \resnum{9999} \\
   \strut Inprocessing   & \resnum{9999} & \resnum{9999} & \resnum{9999} \\
   \end{tabular}
   \caption{Performance of pure and quasipure literal elimination
    (TODO: Compute numbers)}
   \label{fig:ple-qle}
\end{figure}
\end{qle}

\renewcommand{\baseres}{856}

\begin{figure}[t]
   \small
   \centering
   \def\arraystretch{1.1}%
   \begin{tabular}[t]{l@{\hskip 1em}c@{\hskip 1.3em}c@{\hskip 1em}c@{\hskip 1.3em}c@{\hskip 1em}c@{\hskip 1em}c} \\
                         &              &              &                   &              &              & \smash{HLBE}   \\[-1pt] 
                         &              &              & \smash{SPE}       &              & \smash{PPE}  & \smash{+PPE}   \\[-1pt] 
                         & BCE          & SPE          & +BCE              & PPE          & +BCE          & \smash{+BCE}    \\ \midrule 
   \strut Preprocessing  & \resnum{885} & \resnum{902} & \resnumbold{916}  & \resnum{903} & \resnum{915} & \resnum{911}  \\
   \end{tabular}
   \caption{Performance of predicate and blocked clause elimination for establishing satisfiability}
   \label{fig:bce-sat}
\end{figure}

\ourpara{Predicate and Blocked Clause Elimination}
%
% \textit{Evaluation results discussion paragraph---here effects of definitions can be discussed}
% \textit{Do not forget to mention SPE was already implemented and }
For defined predicate elimination, the number of resolvents grows
exponentially with the number of occurrences of $\cst{p}$. To avoid this expensive
computation, we limit the applicability of PPE to proof states for which $\cst{p}$
is singular. According to our informal experiments, full PPE,
without this restriction, generally performs less well.

\begin{qle}\color{red} (TODO: Add QLE)\color{black}\end{qle}

Predicate elimination can be done using Khasidashvili and Korovin's
criterion (K\&K) or using our relaxed criterion with different values of
$K_\mathrm{tol}$. Figure~\ref{fig:criterion} shows the results
for SPE and PPE used as preprocessors.
Our numbers corroborate Khasidashvili and Korovin's findings:
SPE with K\&K proves \NumberOK{70} more problems than the base, a
\NumberOK{0.9\%} increase, comparable to the 1.8\% they observe when
they combine SPE with additional preprocessing.
Remarkably, the number of additional proved problems more than
\NumberOK{doubles} when we use our criterion with $K_\mathrm{tol} > 0$, for both SPE and PPE.

\looseness=-1
Although this is not evident in Figure~\ref{fig:criterion}, varying
$K_\mathrm{tol}$ substantially changes the set of problems solved. For example,
when $K_\mathrm{tol}=0$, SPE proves \NumberOK{60} theorems not proved using
$K_\mathrm{tol}=50$. The effect weakens as $K_\mathrm{tol}$ grows. When
$K_\mathrm{tol}=100$, SPE proves only \NumberOK{13} problems not found when
$K_\mathrm{tol}=200$. Similarly, the set of problems proved by SPE and PPE
differs: When $K_\mathrm{tol}=25$, \NumberOK{14} problems are proved by PPE but
missed by SPE. Recognizing definition sets is useful:
PPE \NumberOK{outperforms} SPE regardless of the criterion.

% \looseness=-1 Having implemented those techniques, we are in position to answer
% two interesting questions: {How helpful are definition clauses in defined
% predicate elimination (DPE)?} and {What are the effects of performing blocked
% clause elimination (BCE) and DPE until fixpoint (BD)?}. The second question was
% also of interest to Kiesl et al.\ \cite{ksstb-2017-blockedfol}, but was left
% unanswered as they did not implement predicate elimination.

\looseness=-1
Performing BCE and variable elimination until fixpoint increases the performance
of SAT solvers~\cite{jbh-10-BCE}. We can check whether the same holds for superposition provers.
In this experiment, we use the relaxed criterion with
$K_\mathrm{tol}=25$ and HLBE which tracks up
to 500 clauses from any clause set, $K_\mathrm{len}=2$, and
$K_{\mathrm{imp}}=0$. We use each technique as preprocessing and inprocessing.

The results are summarized in Figure~\ref{fig:preprocess}, where
the +~sign denotes the combination of techniques.
We confirm the results obtained by Kiesl et al.\ about the performance of BCE
as preprocessing:
It helps prove \NumberOK{30} more problems from our benchmark set, increasing the
success rate by roughly \NumberOK{0.4\%}. The same percentage increase was obtained Kiesl et al.
%
Using BCE as inprocessing, however, hurts performance, presumably because of
its incompatibility with the redundancy criterion.

\looseness=-1%
For preprocessing, the combinations SPE+BCE and PPE+BCE performed roughly \NumberOK{on a
par} with SPE and PPE, respectively. This stands in contrast to the situation
with SAT solvers, where such a combination usually helps. It is also worth noting
that the inprocessing techniques never outperform their preprocessing
counterparts.
%
The last column shows that combining HLBE with other elimination techniques
\NumberOK{overburdens} the prover.

\begin{qle}
\ourpara{Pure and Quasipure Literal Elimination}
\color{red}
Pure literal elimination is a known technique, but quasipure literals are new,
so it is quite interesting to find out whether they bring benefits on
benchmarks. Figure~\ref{ig:ple-qle} considers PLE, QLE, and a combination
of QLE and portfolio predicate elimination, applied in that order. All three
modes are tried both as preprocessing and as inprocessing.

We see TODO.
\endgroup
\end{qle}

\ourpara{Satisfiability by Blocked Clause Elimination}
%
\looseness=-1%
Kiesl et al.\ found that blocked clause elimination is especially effective on
satisfiable problems. To corroborate their results and ascertain
whether a combination of predicate elimination and blocked clause elimination
increases the success rate, we evaluate BCE on all 2273
satisfiable or TPTP FOF and CNF problems.
The hardware and CPU time limits are the same as in the experiments above.
Figure~\ref{fig:bce-sat} presents the results.

\looseness=-1
The baseline establishes the satisfiability of \baseres~problems. We consider
only preprocessing techniques, since BCE compromises refutational
completeness---a saturation does not guarantee that the original problem was
satisfiable. We note that recognizing definition sets makes almost no difference on
satisfiable problems. The sets of problems solved by BCE and PPE
differ---\NumberOK{30} problems are solved by BCE and not by PPE.
% Kiesl sees around 2.5%, we see 3.5% with BCE

\begin{rep}
\section{Discussion and Related Work}
\label{sec:satfol:discussion-and-related-work}

%\textbf{TODO: This section probably needs to be reworked.}

We briefly surveyed related work in Sect.~\ref{sec:satfol:introduction}. In this
section, we give a more detail overview and further discuss connections with
the related work.

The research presented in this \paper{} is two-pronged. For SAT elimination
techniques already generalized to preprocess first-order problems, we looked for
ways to interleave them with the given clause procedure of a superposition
prover, as inprocessing. For techniques that had not yet been ported to
first-order logic, we looked for generalizations that allow both preprocessing and
inprocessing.

Hidden tautology elimination was first described
by Heule et al.\ \cite{hjb-2010-cl-elim}. Better implementation that also
supports hidden literal elimination was later described by the same group of
authors \cite{hjb-2011-big-simplification}. We generalized the underlying
theoretical concepts to first-order logic, and provided an efficient way to deal
with the infinite number of hidden literals that arise  with this
generalization. More efficient graph-based techniques are yet to be explored.

Variable elimination, based on Davis--Putnam resolution \cite{dp-60-dp}, has been studied in
the context of both propositional logic \cite{sp-04-niver,cs-00-zres} and QBF
\cite{ab-2004-re}. It was generalized to first-order logic (as a
preprocessor) by Khasidashvili and Korovin \cite{kk-2016-pe-fol} yielding a technique called
predicate elimination. Improvement of variable elimination,
which uses formula definition information, has been popularized as a preprocessing and
inprocessing technique for CDCL solvers by E{\'{e}}n and Biere
\cite{eb-2005-satpreprocess}. We generalized this improvement to first-order
logic and combined it with existing extension. With tolerable restrictions,
this extension can be used as an inprocessing technique. In SAT and QBF, it was observed that
allowing variable elimination to slightly increase in the clause set size
improves performance \cite{bls-11-bloqqer}. We implemented a similar approach,
achieving the double the number of additional proofs found compared to more
restrictive approaches.

Blocked clause elimination is used in both SAT~\cite{jbh-10-BCE} and QBF
solvers \cite{bls-11-bloqqer}. Its generalization to first-order logic
\cite{ksstb-2017-blockedfol} has showed positive effects when used as a
preprocessor. We showed that blocked clauses cannot be removed during
saturation, but that they can be effectively used to show satisfiability of the
clause set. A combination of blocked clause elimination and
variable elimination performs well in propositional logic \cite{jbh-10-BCE}, but we observed no substantial
improvement when their generalizations are combined.

Our approach is one of many ways to combine ideas from SAT solving and
first-order proving. Other noteworthy architectures that either incorporate a
SAT solver or that generalize the CDCL calculus include DPLL($T$) with
quantifier instantiation
\cite{de-moura-bjoerner-2007,barbosa-et-al-2017,reynolds-et-al-2018},
DPLL($\mathrm{\Gamma}+T$) \cite{bonacina-et-al-2009},
labeled splitting \cite{fietzke-weidenbach-2009},
AVATAR \cite{av-2014-avatar},
MCSAT \cite{de-moura-jovanovic-2013},
CDSAT \cite{bonacina-et-al-2017}, and
SGGS~\cite{bonacina-plaisted-2014}.

% See also \url{https://academic.oup.com/logcom/advance-article-abstract/doi/10.1093/logcom/exaa065/6038918}

% OLD TEXT, might be useful: {\sl A similar issue arises with
% the related SAT technique of \emph{variable addition} \cite{xxx}, which can be
% used to replace the six-clause set $\{\cst{b} \cor \cst{e}{,}\; \cst{c} \cor
% \cst{e}{,}\; \cst{d} \cor \cst{e}{,}\; \cst{b} \cor \cst{f}{,}\; \cst{c} \cor
% \cst{f}{,}\; \cst{d} \cor \cst{f}\}$ by the five-clause set $\{\cst{b} \cor
% \cst{a}{,}\; \cst{c} \cor \cst{a}{,}\; \cst{d} \cor \cst{a}{,}\; \cst{e} \cor
% \neglit{\cst{a}}{,}\; \cst{f} \cor \neglit{\cst{a}}\}$, at the cost of one
% extra symbol ($\cst{a}$).}

% In QBF predicate elimination happens more often than in SAT. Contrast to what we do in this paper.
%%% I'm not sure what to do with this comment. For me, QBF isn't some
%%% intermediate logic between SAT and FOL; it's a freak. We can't
%%% quantify over Booleans in FOL. I also weakened the conjecture about
%%% "the richer the logic, the more useful PE becomes" in the intro. Anyway,
%%% I don't think it belongs in a summary/future-work Conclusion, but feel
%%% free to speculate in the Discussion. ;) --JB
\end{rep}

\section{Conclusion}
\label{sec:satfol:conclusion}

We adapted several preprocessing and inprocessing elimination techniques implemented in
modern SAT solvers so that they work in a superposition prover. This involved lifting
the techniques to first-order logic with equality but also tailoring them to
work in tandem with superposition and its redundancy criterion.
Although SAT solvers and superposition provers embody radically different
philosophies, we found that the lifted SAT techniques provide valuable optimizations.

We see several avenues for future work. First, the implementation of hidden
literals could be extended to exploit equality congruence. Second, although
inprocessing blocked clause elimination is incomplete in general, we hope to achieve
refutational completeness for a substantial fragment of it. Third, predicate and
blocked clause elimination, which thrives on the absence of clauses from the
proof state, could be enhanced by tagging and ignoring generated clauses that
have not yet been used to subsume or simplify untagged clauses. Fourth,
predicate and blocked clause elimination could be extended to work with
functional literals. Fifth, more SAT techniques could be adapted, including
bounded variable addition~\cite{mhb-12-reencoding} and blocked clause
addition~\cite{ok-99-er}. Sixth, the techniques we covered could be adapted to
work with other first-order calculi, or generalized
further to work with higher-order calculi such as combinatory superposition
\cite{br-20-full-sup-w-combs} and $\lambda$-superposition
\cite{bentkamp-et-al-2021-lamsup-journal}.

\let\Acksize=\relax

\def\ackname{\Acksize Acknowledgment}
\ourpara{\ackname}
{\Acksize
We are grateful to the maintainers of StarExec for letting us use their
service.
Uwe Waldmann participated in the search for a counterexample to completeness of
BCE as inprocessing and confirmed that Example~\ref{example:bce-completeness}
is correct. He also suggested major simplifications and helped us debug the
proofs of the claims about predicate elimination.
Anne Baanen helped us define the nonclassical logic used to disallow
splitting.
Ahmed Bhayat, Armin Biere, Mathias Fleury, Benjamin Kiesl, and the anonymous
reviewers made some useful comments on our manuscript, and Mark Summerfield
suggested many textual improvements. We thank them all.

Vukmirovi\'c and Blanchette's research has received funding from
the European Research Council (ERC) under the European Union's Horizon 2020
research and innovation program (grant agreement No.\ 713999, Matryoshka).
%
Blanchette's research has also
received funding from the Netherlands Organization for Scientific Research (NWO)
under the Vidi program (project No.\ 016.Vidi.189.037, Lean Forward).
%
Heule is supported by the National Science Foundation (NSF) under grant CCF-2015445. 

}

\bibliographystyle{IEEEtran}
% \begin{raggedright}
\bibliography{bib}
% \end{raggedright}

\end{document}


\def\ackname{Acknowledgment} % American English
\begin{acknowledgements}
We are grateful to the maintainers of StarExec for letting us use their service.
%Mathias Fleury helped us setup the ORCID icons.
Ahmed Bhayat and Giles Reger guided us
through details of Vampire 4.5. Ahmed Bhayat, Michael F\"arber,
Mathias Fleury, Predrag Jani\v ci\'c,
Mark Summerfield, and the anonymous reviewers suggested content, textual, and
typesetting improvements. We thank them~all.

Vukmirovi\'c, Bentkamp, and Blanchette's research has received funding from
the European Research Council (ERC) under the European Union's Horizon 2020
research and innovation program (grant agreement No.\ 713999, Matry\-osh\-ka).
%
Blanchette and Nummelin's research has
received funding from the Netherlands Organization for Scientific Research (NWO)
under the Vidi program (project No.\ 016.Vidi.189.037, Lean Forward).
\end{acknowledgements}


% Authors must disclose all relationships or interests that
% could have direct or potential influence or impart bias on
% the work:
%
% \section*{Conflict of interest}
%
% The authors declare that they have no conflict of interest.


% BibTeX users please use one of
\bibliographystyle{spmpsci}

\bibliography{bib}

\end{document}
% end of file template.tex
