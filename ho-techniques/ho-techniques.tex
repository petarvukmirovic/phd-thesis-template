\chapter{Making Higher-Order Superposition Work}
\setheader{Making Higher-Order Superposition Work}
\label{ch:ho-techniques}

\authors{
    Joint work with\\
    Alexander Bentkamp,
    Jasmin Blanchette,
    Simon Cruanes,
    Visa Nummelin, and Sophie Tourret
}

\blfootnote{In this work I was the main designer of all presented techniques,
with the exception of inference streams which were designed by Sophie Tourret
and Alexander Bentkamp. Alexander Bentkamp and Jasmin Blanchette also discussed many
of the techniques with me and suggested important updates. Visa Nummelin worked on
implementation of FOOL preprocessing. Simon Cruanes is the original developer
of Zipperposition and provided us with invaluable knowledge.
}


\begin{abstract}
    Superposition is among the most successful calculi for first-order logic. Its
    extension to higher-order logic introduces new challenges such as infinitely
    branching inference rules, new possibilities such as reasoning about
    Booleans, and the need to curb the explosion of specific higher-order rules. We
    describe techniques that address these issues and extensively evaluate their
    implementation in the Zipperposition theorem prover. Largely thanks to their use,
    Zipperposition won the higher-order division of the CASC competition in 2020.  
  \end{abstract}
\newpage

\section{Introduction}
\label{sec:ho-tech:intro}

% %In recent decades,
% Superposition-based first-order automatic theorem provers
% have emerged as useful reasoning tools. They dominate at the annual CASC
% \cite{gs-2016-casc} theorem prover competition, having always won the
% first-order theorem division. They are also used as backends to proof assistants
% \cite{ck-18-coqhammer,ku-15-holyhammer,pb-12-sh}, automatic
% higher-order theorem provers \cite{sb-21-leo3}, and software verifiers
% \cite{fp-13-why3}.

% The superposition calculus has only recently been extended
% to higher-order logic (more precisely, extensional simple type theory
% \cite{henkin-1950-completeness}), resulting in
% \emph{\lsup} \cite{bbtvw-21-sup-lam}, which we developed
% together with Waldmann, as well as \emph{combinatory superposition}
% \cite{br-20-full-sup-w-combs} by Bhayat and Reger. Although these two
% calculi do not support an interpreted Boolean type,
% they can be extended by ad hoc rules \cite{our-bool-paper} that support
% most of the Boolean reasoning necessary in practice.

% Both higher-order superposition calculi were designed to gracefully
% extend first-order reasoning. As most steps in higher-order
% proofs tend to be essentially first-order, extending the most successful first-order
% calculus to higher-order logic seemed worth trying.
% Our first attempt at testing this idea was in 2019:
% Zipperposition~1.5, based on \lsup, finished third
% in the higher-order theorem division of CASC-27 \cite{gs-19-casc27},
% 12~percentage points behind the winner, the tableau prover Satallax 3.4 \cite{cb-2013-satallax}.

The landscape of higher-order proving techniques based on extension of efficient
first-order ones has tremendously expanded in the late 2010s and early 2020s. As
mentioned is Sect.~\ref{sec:pre:ho-sup-calculi} we have implemented three
higher-order calculi---\lfsup{}, \lsup{}, and \osup{}---which extend first-order superposition in a graceful way.
Bhayat and Reger also gracefully extended superposition to higher-order logic using
\textsf{SKBCI} combinators \cite{br-20-full-sup-w-combs}. Significant progress has been
made on SMT front as well \cite{brotb-19-ho-smt}.

In 2019 we tested if the idea of gracefully extending first-order provers to
higher-order logic really improves the state of the art for the first time. We implemented \lsup{} \cite{bbtvw-21-sup-lam} in
Zipperposition~1.5 with basic heuristics and rudimentary extensions of the
calculus to deal with Booleans. It finished third at that year's higher-order
division of CASC competition \cite{gs-19-casc27}, 12~percentage points behind the
winner, the tableau prover Satallax 3.4 \cite{cb-2013-satallax}.

Studying the competition results, we found that higher-order tableaux have some
advantages over higher-order superposition. To bridge the gap, we developed
techniques and heuristics that simulate tableaux in the context of saturation.
We implemented them in Zipperposition~2, which took part at the higher-order
division of CASC \cite{gs-21-cascj10} in 2020. This time, our prover won the
division, proving 84\% of the problems, a whole 20~percentage points ahead of
the runner-up, Satallax 3.4.

In this chapter, we describe the main techniques that explain this reversal of
fortunes. They cover most parts of a modern higher-order theorem prover, from
preprocessing to additional calculus rules to heuristics to backend integration.
Compared to the previous chapter, in which we discussed rules used to treat
Boolean terms, in this chapter we use a newer version of Zipperposition, based
on a newer calculus. Instead of \lsup{} augmented with ad hoc Boolean rules, we
work with {\osup} \cite{bbtv-21-full-ho-sup}, a principled extension of
superposition to full higher-order logic, including an interpreted Boolean type.

Many higher-order problems extensively use symbol definitions to simplify
their representation. We describe several ways to exploit the definitions,
%of which the most successful is
such as turning them into rewrite rules (Sect.~\ref{sec:ho-tech:preprocessing}).
%Interesting patterns can be observed in various higher-order problem encodings.
%We show how we can exploit these to simplify problems (Sect.~\ref{sec:ho-tech:preprocessing}).
%
By working on formulas rather than clauses, tableau techniques take a more
holistic view of a higher-order problem.
Through its support for delayed clausification and, more generally,
calculus-level formula manipulation, \osup{} enables us to
simulate most successful tableau techniques in a saturating prover
(Sect.~\ref{sec:ho-tech:formulas}). This calculus also supports \emph{Boolean selection
functions}, a mechanism that allows us to choose on which Boolean subterms
to perform inferences first.
We implemented some Boolean selection functions and
evaluated them (Sect.~\ref{sec:ho-tech:bool-select}).

\looseness=-1
The main drawback of both $\lambda$-superposition variants compared with
combinatory superposition is that they rely on rules that enumerate possibly
infinite sets of unifiers. We describe a mechanism that interleaves infinitely
branching inferences with the standard saturation process
(Sect.~\ref{sec:ho-tech:infinite-branching}). The prover retains the same
behavior as before on first-order problems, smoothly scaling with increasing
numbers of higher-order clauses.
%
We also propose heuristics to curb the explosion induced by highly
prolific calculus rules (Sect.~\ref{sec:ho-tech:explosiveness}).

Using first-order backends to finish the proof is common practice in
higher-order reasoning. Since \osup{} coincides with standard
superposition on first-order clauses, invoking backends may
seem redundant; yet Zipperposition is nowhere as efficient as E
\cite{scv-19-e23} or Vampire \cite{lkav-13-vampire}, so invoking a more
efficient backend does make sense. We describe how to achieve a balance
between allowing native higher-order reasoning and
delegating reasoning to a backend (Sect.~\ref{sec:ho-tech:backends}).
%
Finally, we compare Zipperposition~2 with other provers on all monomorphic
higher-order TPTP benchmarks \cite{gs-17-tptp} to perform a more extensive
evaluation than at CASC (Sect.~\ref{sec:ho-tech:comparison}). Our evaluation
corroborates the competition results.

\section{Background and Setting}
\label{sec:ho-tech:background}

We focus on monomorphic higher-order logic, without the axiom of infinity or the
axiom of at least two individuals \cite{bm-14-automation-ho}. However, the
techniques can easily be extended with polymorphism. Indeed, Zipperposition
already supports some of them polymorphically. Further, we use exactly the same
notation for this logic as introduced in Chapter \ref{ch:pre}. Since we are
working with extensions of superposition, we assume the clausal structure
(Sect.~\ref{sec:pre:clauses}). Like in the previous chapter, literals
of clauses can contain arbitrary higher-order terms, including formulas.  


\ourpara{Higher-Order Calculi}
\looseness=-1
We briefly introduced the \osup{} calculus \cite{bbtv-21-full-ho-sup} in
Sect.~\ref{sec:pre:ho-sup-calculi}. It is a refutationally complete inference
system and redundancy criterion for higher-order logic with rank-1 polymorphism,
Hilbert choice, and functional and Boolean extensionality.
% The calculus relies on
% \emph{complete sets of unifiers}
% (\emph{CSUs}). The CSU for $s$ and $t$ with respect to a finite set of variables
% $V$, denoted by $\mathrm{CSU}_V(s,t)$, is a set of unifiers of $s$~and~$t$ such
% that for any unifier $\varrho$ of $s$~and~$t$, there exist substitutions $\sigma
% \in \mathrm{CSU}_V(s,t)$ and $\theta$ such that $\varrho(X) = \sigma(\theta(X))$
% for all variables $X \in V$. The set $V$ is used to distinguish
% important variables from auxiliary variables (which may arise in intermediary
% states of the unification procedure). We usually omit it.
Unlike \lsup{}, this calculus 
does not require axioms defining the logical symbols to cope with formulas.
Instead, it includes Boolean inference rules that mimic
superposition from such axioms into Boolean subterms,
while avoiding the explosion incurred by adding these axioms to the proof state. It
also includes rules that simulate Boolean inferences below applied variables.
Both sets of rules are disabled or replaced with incomplete, ad hoc rules
described in the previus chapter in most configurations
of the CASC portfolio.
A new feature of the calculus that we explore in detail is
the ability to select Boolean subterms
to restrict Boolean and superposition inferences.

In contrast to both $\lambda$-superposition variants, combinatory superposition
does not require enumerating elements of CSU to compute results of inferences.
Instead, it avoids computing CSUs by using a form of first-order unification.
Essentially, it enumerates higher-order terms using rules that instantiate
applied variables with partially applied combinators from the complete
combinator set $\{\cst{S}, \cst{K}, \cst{B}, \cst{C}, \cst{I}\}$. This calculus
is the basis of Vampire~4.5 \cite{br-20-full-sup-w-combs}, which finished
closely behind Satallax 3.4 %~and~3.5
at higher-order division of CASC in 2020.

\looseness=-1
A different, very successful calculus is Satallax's SAT-guided tableaux
\cite{backes-brown-2011}. Satallax was the leading higher-order prover of the
2010s. Its simple and elegant tableaux avoid deep superposition-style rewriting
inferences.
Nevertheless, our working hypothesis for the past years has been
that superposition would likely provide a stronger basis for higher-order
reasoning.
Other competing higher-order calculi include SMT (implemented in CVC4
\cite{brotb-19-ho-smt, cbetal-11-cvc4}) and extensional paramodulation (implemented in Leo-III \cite{sb-21-leo3}).


\ourpara{Zipperposition}
Zipperposition \cite{sc-15-simon-phd,bbtvw-21-sup-lam} is a higher-order
theorem prover that implements both $\lambda$-superposition variants, combinatory
superposition, and other superposition-like calculi.
The prover was conceived as a testbed for rapidly
experimenting with extensions of first-order superposition, but over time it
has assimilated many of E's techniques and heuristics and become quite powerful.
Several of our techniques extend the \emph{given clause procedure}, the standard
saturation procedure described in Sect.~\ref{sec:pre:saturation}.

\ourpara{Experimental Setup}
To assess our techniques, we carried out experiments with Zipperposition~2. We
used two sets of benchmarks:\ all 2851~monomorphic higher-order problems from the
TPTP library \cite{gs-17-tptp} version~7.4.0 (labeled \emph{TPTP})
and 1253 Sledgehammer-generated
monomorphic higher-order problems (labeled \emph{SH}).
Although some techniques support polymorphism, we
uniformly used monomorphic benchmarks.

We fixed a \emph{base} configuration
of Zipperposition parameters as a baseline for all comparisons. This is an
incomplete, pragmatic configuration of \osup{} using heuristics expected to perform
well on a wide range of problems.
%Note that
%we use a different baseline configuration than in our earlier paper \cite{making-ho-work}.
%%% That goes without saying. We use a different calculus! Now clarified in intro. --JB
In each experiment, we varied
the parameters associated with a specific technique to evaluate it. The
experiments were run on StarExec Miami \cite{sst-14-starexec} servers, equipped with
Intel Xeon E5-2620 v4 CPUs clocked at 2.10 GHz. Unless otherwise stated, we used a
CPU time limit of 15~s, roughly the time each configuration is given in the
CASC portfolio mode. The raw evaluation results are available online.%
\footnote{\url{http://doi.org/10.5281/zenodo.5007440}}

\section{Preprocessing Higher-Order Problems}
\label{sec:ho-tech:preprocessing}

The TPTP library contains thousands of higher-order problems. Despite their
diversity, they have a markedly different flavor from the TPTP first-order
problems. Notably, they extensively use the \verb|definition| role to identify
universally quantified equations (and equivalences) that define symbols.
%
Definitions $s \eq t$ (or $(s \iequiv t) \eq \itrue$) can be replaced by rewrite
rules $s \longrightarrow t$,
using the orientation given in the input problem. If there are multiple
definitions for the same symbol, only the first one is replaced by a rewrite rule.
Then, whenever a clause is picked in the given clause procedure, it will be rewritten
using the collected rules.
Alternatively, we can rewrite
the input formulas as a preprocessing step. This ensures that the input
clauses will be fully simplified when the proving process starts and no
defined symbols will occur in clauses, which usually helps the heuristics.

Since the TPTP format enforces no constraints on
definitions, rewriting might diverge. To ensure
termination, we limit the number of applied rewrite steps. In
practice, most TPTP problems are well behaved: Only one
definition is given for each symbol, and the definitions are acyclic.

Turning the defining equations into rewrite rules, unfolding the definitions, and
$\beta$-reduc\-ing the result can eliminate all of a problem's higher-order features, making
it susceptible to first-order methods. However, this can inflate the problem
beyond recognition and compromise the refutational completeness of
superposition.

\begin{exa} 
  Removing higher-order features of a problem can have adverse effects.
  Consider the TPTP problem \texttt{NUM636\^{}3}, which defines the predicate $\cst{m}$
  as $\lambda x.\, \cst{s} \, x \ineq x$ and states its conjecture as $\iforall
  x.\, \cst{m} \, x $, where $\cst{s}$ is the standard Peano-style natural number
  successor constructor. When this definition is kept as is, the
  prover can superpose from either $\cst{m}$ or its definition into the
  (clausified) induction axiom, which is also given in the problem, and quickly prove
  the conjecture, without using any advanced inductive reasoning. In contrast,
  when the definition is
  unfolded and the problem is $\beta$-reduced, both $\cst{m}$ and the
  corresponding $\lambda$-abstraction disappear, forcing the prover to guess the
  correct instantiation for the induction axiom.
\end{exa}

We describe two techniques to mitigate these issues. The first one is based on the observation that in practice,
the explosion associated with definition unfolding mostly
manifests itself on definitions of nonpredicate symbols. In some cases, it is
preferable to rely on superposition's term order and powerful simplification
engine to rewrite the proof state rather than to blindly rewrite definitions. On
the other hand, superposition's reasoning with equivalences is often inadequate
\cite{bbtv-21-full-ho-sup, gs-05-boolsup}. Thus, it makes sense to treat only
predicate definitions as rewrite rules.

The second technique aims at preserving completeness: We can try to force the term order that
parameterizes superposition to orient as many definitions as possible and rely on
demodulation to simplify the proof state. Usually, the Knuth--Bendix order (KBO)
\cite{db-1970-kbo} is used. It compares terms by first comparing their weights,
which is the sum of all the weights assigned to the symbols it contains. Given a
symbol weight assignment $\mathcal{W}$, we can update it so that it orients
acyclic definitions from left to right assuming that they are of the form $
\cst{f} \, \overline{X}_m \eq \lambda \overline{Y}_n. \, t$, where the only free
variables in $t$ are $\overline{X}_m$, no free variable is repeated or appears
applied in $t$, and $\cst{f}$ does not occur in $t$. Then we traverse the
symbols $\cst{f}$ that are defined by such equations following the dependency
relation, starting with a symbol $\cst{f}$ that does not depend on any other
defined symbol. For each $\cst{f}$, we set $\mathcal{W}(\cst{f})$ to $w + 1$,
where $w$ is the maximum weight of the right-hand sides of $\cst{f}$'s
definitions, computed using $\mathcal{W}$. By construction, for each equation
the left-hand side is heavier. Thus, the equations are orientable from left to
right.



\begin{exa} 
  Many of the problems in the TPTP library's \verb|LCL| category encode modal logic
  in higher-order logic. More complex modal operators (such as
  implication and equivalence) are defined in terms of basic connectives (such as negation
  and disjunction). Some of the definitions present in the problems are
  $\cst{mnot} := \lambda p\, x. \, \inot \, p \, x$, $\cst{mor} := \lambda p\, q\, x.
  \, p \, x \ior q \, x$,  and $\cst{mimplies} = \lambda p\, q. \allowbreak\, \cst{mor} \,
  (\cst{mnot} \, p) \, q$. Assuming that the weight of $\lambda$, bound
  variables, and basic connectives is 2, we can orient equations using
  the above described approach as follows. Starting from symbols that do not
  depend on the other ones, we set $\mathcal{W}(\cst{mnot}) = 11$ and
  $\mathcal{W}(\cst{mor}) = 17$. Then, we use these values to set
  $\mathcal{W}(\cst{mimplies}) = 37$. Clearly, these weights
  enable us to orient all definitions from left to right.
\end{exa}


% Many higher-order problems, especially those generated from proof assistants,
% contain hundreds of needless axioms.
% To filter out axioms that are unlikely to be useful in a proof attempt, many
% theorem provers rely on the SInE algorithm \cite{hv-2011-sine}. SInE starts with
% the set of symbols occurring in the conjecture and tries to find axioms that
% define the properties of these symbols. Then it looks for the definitions of
% newly found symbols until it reaches a fixpoint. Axioms annotated with
% \verb|definition| ease this search because they explicitly record the
% dependency between a symbol and its characterization. To exploit
% this information, we modified SInE to optionally include the definitions of
% symbols in the conjecture, regardless of whether they are filtered out or not.
% Similarly, we implemented mode of SInE which selects only conjecture and axioms
% annotated with \verb|definition|.

\ourpara{Evaluation and Discussion}

% The \textit{base} configuration treats all axioms annotated with
% \texttt{definition} as rewrite rules applied as preprocessing.
% In addition, we tested
We designed and evaluated the following strategies for handling
\texttt{definition} axioms:

\begin{description}[labelwidth=\widthof{\rm no-RW$+$KBO~}]
  \item[\rm pre-RW~] rewrite all definitions as a preprocessing step;
  \item[\rm in-RW~] rewrite all definitions during the saturation, as an inprocessing step;
  \item[\rm $o$-RW~] rewrite only predicate definitions, during preprocessing;
  \item[\rm $o$-RW$+$KBO~] like $o$-RW but with adjusted KBO weights for the remaining
    definitions;
  \item[\rm no-RW~] no special treatment of definitions;
  \item[\rm no-RW$+$KBO~] like $no$-RW but adjusting KBO weights for all definitions.
\end{description}

The results are given in Fig.~\ref{fig:rewrite}. In all the figures, each
cell gives the number of proved problems, and cells marked with $\star$
correspond to the base configuration. The highest number in a category is typeset in
\relax{bold}. SH benchmarks are not
included because they do not contain the \texttt{definition} role.

\newcommand{\unknownres}{\ensuremath{{\varnothing}}}
\newcommand{\colalign}{\phantom{0}}

\begin{figure}[t]
  \centering
  \def\arraystretch{1.1}%
  \relax{\begin{tabular}{@{}l@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{\hskip 1em}c@{}} \toprule
                & pre-RW                 & in-RW        & $o$-RW               & $o$-RW$+$KBO    & no-RW        & no-RW$ + $KBO  \\ \midrule
           TPTP & {\bf 1635}$^\star$     & 1619         & 1620                 & 1621            & 1298         & 1296
           \\ \bottomrule
         \end{tabular}}
       \captionof{figure}{Impact of the definition rewriting method}
       \label{fig:rewrite}
     \end{figure}



The four configurations in which definitions are treated as rewrite rules
perform much better than the other two. In contrast, adjusting KBO weights gives
no substantial improvement: Looking at raw data, we found only
\NumberOK{2}~problems proved by $o$-RW$+$KBO but not by $o$-RW in which the
feature was used in the proof. For no-RW and no-RW$+$KBO, the
\NumberOK{2}-problem difference may be just noise. Even though it proves fewer
problems, the configuration $o$-RW has some advantages over pre-RW: It proves
\NumberOK{16} problems that pre-RW does not, \NumberOK{3} of which have a TPTP
difficulty rating (the ratio of eligible %TPTP
provers that cannot prove the problem) of~1.

% \looseness=-1
Rewriting after clausification avoids getting stuck rewriting parts of the
proof state that might not contribute to the proof. In practice, we noticed that
rewriting can be so expensive that the prover can spend all
allotted CPU time in the preprocessing phase. The evaluation results confirm this
observation: There are \NumberOK{64} problems proved by in-RW but not by
pre-RW. Moreover, there are \NumberOK{41} problems that can be proved only
by in-RW but not by any other above described configuration. % from Fig.~\ref{fig:rewrite}.


\section{Reasoning about Formulas}
\label{sec:ho-tech:formulas}

Higher-order logic identifies formulas with terms of Boolean type. To prove a problem, we often
need to instantiate a variable with the right predicate.
Finding this predicate can be easier if the problem is not clausified.
Consider the conjecture $\iexists f. \, f \, \cst{p} \, \cst{q} \iequiv \cst{p}
\iand \cst{q}$. Expressed in this form, the formula is easy to prove by taking
$f := \lambda x \, y. \> x  \iand y$. By contrast, guessing the right
instantiation for the negated, clausified form $ \neglit{F \, \cst{p} \,
\cst{q}} \llor \neglit{\cst{p}} \llor \neglit{\cst{q}},
\poslit{F \, \cst{p} \, \cst{q}} \llor \poslit{\cst{p}}$, $\poslit{F \, \cst{p}
\, \cst{q}} \llor \poslit{\cst{q}}$ is more challenging.
One of the strengths of higher-order tableau provers is that they do not clausify the input
problem. This might partly explain Satallax's dominance in the THF division of CASC
competitions until the 2020 edition of CASC.

The \osup{} calculus supports \emph{delayed clausification rules} that insert
problems into the proof state in their original, nonclausified form, and
clausify them gradually. Delayed clausification allows the prover to analyze the
syntactic structure of formulas during saturation, whereas the more traditional
approach of \emph{immediate clausification} applies a standard clausification
algorithm \cite{nw-01-small-cnf} both as a preprocessing step and whenever
predicate variables are instantiated.

An earlier evaluation of the \osup{} calculus \cite{bbtv-21-full-ho-sup} showed
that the \emph{outer} variant of delayed clausification substantially increases
this calculus's performance. The outer variant clausifies top-level logical
symbols, proceeding from the outside inwards; this method corresponds to the one
described in the previous chapter. For example, a clause $C \llor
\neglit{(\cst{p} \iand \cst{q})}$ is transformed into $C \llor \neglit{\cst p}
\llor \neglit{\cst q}$. The calculus also supports \emph{inner} delayed
clausification, which uses only the core calculus rules to clausify problems.
Even though this is the laziest approach to clausification, the earlier
evaluation by Bentkamp et al. \cite{bbtv-21-full-ho-sup} showed that this
approach is inefficient. Thus, we focus only on the outer rules.

\looseness=-1
Delayed clausification rules can be used as inference rules (which add conclusions
to the passive set) or as simplification rules (which delete premises and add
conclusions to the passive set).
%
Inferences are more flexible, as all
intermediate clausification states will be stored in the proof state, at the
cost of producing many clauses. Simplifications produce fewer clauses,
but risk destroying informative syntactic structure.
Since clausifying equivalences can destroy a lot of syntactic structure
\cite{gs-05-boolsup}, we never apply simplifying rules
on them.

Delayed clausification can interfere with clause splitting techniques.
Zipperposition supports a lightweight variant of AVATAR \cite{av-2014-avatar},
an architecture that partitions the search space by
splitting clauses into variable-disjoint subclauses. This lightweight AVATAR is described
by Ebner et al.\ \cite[Sect.~7]{2021-ebt-unifying-splitting}. Combining it
with delayed clausification makes it possible to split a %higher-order
clause $(\varphi_1 \ior \cdots \ior \varphi_n) \eq \itrue$, where
the $\varphi_i$'s are arbitrarily complex formulas that share no free
variables with each other, into clauses $\varphi_i \eq \itrue$.
%
To finish the proof, it suffices to derive the empty clause under each assumption
$\varphi_i \eq \itrue$. Since the split is performed at the formula level, this
technique resembles tableaux, but it exploits the strengths of superposition,
such as its powerful redundancy criterion and simplification machinery, to
close the branches.


