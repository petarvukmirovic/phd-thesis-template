\chapter{Preliminaries}
\setheader{Preliminaries}
\label{ch:pre}

\begin{abstract}
    In this chapter we lay out the basic prerequisites for the remaining
    chapters. We begin by describing the three logics that we work with in this
    thesis: propositional logic, monomorphic first-order logic and simply typed
    higher-order logic. Then, we explain the clausal structure which is the
    backbone of many calculi for automated provers. We finish with the
    description of the superposition calculus. As this thesis discusses practical
    aspects of theorem proving, we define only the fundamental notions, while
    more advanced notions are intuitively described with references to rigorous
    definitions. The text of this chapter is
    partially based on the preliminaries sections of the publications listed in
    Chapter \ref{ch:intro}.
    
\end{abstract}
      
\newpage


\section{Propositional Logic}

\emph{Atomic formulas} of propositional logic are propositional variables
$\cst{p}, \cst{q}, \ldots$ and constants $\itrue$ and $\ifalse$. More complex
formulas are built inductively using logical connectives $\inot, \iand, \ior,
\iimplies, \allowbreak \iequiv$: if $\phi$ and $\psi$ are formulas, then $\inot \phi, \phi \iand
\psi, \allowbreak \phi \ior \psi, \phi \iimplies \psi, \phi \iequiv \psi$ are formulas as well.

To interpret the formulas, propositional variables are assigned 0 (false) or 1
(true), constants $\itrue$ and $\ifalse$ are assigned 1 and 0, respectively; the
formula is interpreted using the rules for each connective
\cite[Sect.~1.4]{hr-00-logic-in-cs}. As there are finitely many propositional
variables in a formula, trying all (finitely many) possible variable assignments
describes an algorithm to decide satisfiability (existence of satisfiable
assignment for a formula) or validity (if all assignments are satisfy the formula).

However, this simple approach is prohibitively expensive and modern tools that
decide propositional satisfiability problem (SAT solvers) use more advanced
approaches such as CDCL calculus \cite{mss-96-cdcl}. Modern SAT solvers also
heavily preprocess the problem and continue simplifying it during proving
process.

\section{First-Order Logic}

First-order logic increases the expressivity by allowing quantification over
objects and has more complicated formula structure. There are many flavors of
first-order logic, but in this thesis we consider monomorphic first-order logic
with equality.

We distinguish a set of base types $T$ which is required to have the Boolean
type $o$, and a set of symbols $\Sigma$. To each symbol $\cst{f} \in \Sigma$ a
tuple $(\tau_1, \ldots, \tau_{n}, \tau), n \geq 0$ of types is assigned, written
$\cst{f} : (\tau_1, \ldots, \tau_{n}) \rightarrow \tau$. We say that $(\tau_1,
\ldots, \tau_{n})$ are \emph{argument types}, $\tau$ is the \emph{return type}, and that $n$
is the \emph{arity} of the symbol $\cst{f}$. If return type of $\cst{f}$ is $o$ we call
$\cst{f}$ a \emph{predicate symbol}, otherwise we call it a \emph{function symbol}. Argument types may not be Boolean.

\emph{Terms} are the basic building blocks of first-order logic, built inductively as follows. Variables
$x,y,\ldots$, assigned types $\tau \in T$, are terms. If $t_1,\ldots,t_n$ are
terms of types $\tau_1, \ldots, \tau_n$, respectively, and $\cst{f} : (\tau_1,
\ldots, \tau_{n}) \rightarrow \tau$, then $\cst{f}(t_1, \ldots, t_n)$ is a term
of type $\tau$. If $n=0$, we drop the parentheses and write $\cst{f}$. We also
abbreviate $\cst{f}(t_1, \ldots, t_n)$ to $\cst{f}(\tuplen{t})$. Using a similar
notation, we abbreviate a tuple of terms $(t_1, \ldots, t_n)$ as $(\tuplen{t})$
or simply $\tuplen{t}$.

Terms are used to build \emph{atoms}. Atom is either a term $t$ of type $o$ (\emph{predicate atom}) or an
equation $s \eq t$ where terms $t$ and and $s$ are of the same type (\emph{equational atom}). Atoms are
combined using logical connectives to build formulas just like in propositional
logic. Additionally, first-order formulas are built using quantifiers: if $\phi$
is a formula and $x$ is a variable then $\iforall x.\, \phi$, as well as
$\iexists x.\, \phi$ are formulas. The first formula requires that $\phi$ holds
for any value of $x$, while the second one requires that there is some $x$ for
which $\phi$ holds. As soon as there is a single functional symbol with arity
greater than 0, there are infinitely many terms that can be substituted for a
free variable. Furthermore, in first-order logic, it does not suffice to assign
values to variables to determine the truth value of the formula. It is also
necessary to interpret the symbols. Therefore, it is obvious that the propositional
technique for deciding satisfiability does not work in the first-order case.

Even though we do not formally define semantics of logic, we assume the natural
extensions of domain, valuation, interpretation and model (as defined by Fitting
\cite{mf-1996-fol}) from unsorted to many-sorted logic. Models we consider are
\emph{normal}, i.e., they interpret $\eq$ as an equality relation. Usual notions
of (un)satisfiability and (in)validity are assumed. We write
$\ourmodel~\models_\xi~N$ to denote that a model $\ourmodel$ satisfies a clause
set $N$, for a variable assignment $\xi$. If $\ourmodel$ is a model of $N$
(i.e., it satisfies it under every variable assignment), we simply write
$\ourmodel \models N.$  Abusing notation, we write $M \models N$ to denote that
$M$ \emph{entails} $N$, i.e., that every model of $M$ is a model of $N.$

A {\em position} in a term is a tuple of natural numbers, with $\varepsilon$ denoting
the empty tuple. {\em Subterms} and positions are inductively defined as follows. Term
$t$ is a subterm of itself at position $\varepsilon$. If $s$ is a subterm of
$t_i$ at position $p$, then $s$ is a subterm of $\cst{f}(\tuplen{t})$ at
position $i.p$. A {\em context} is a term with zero or more subterms replaced by a
hole $\square$. We write $C[\overline{u}_n]$ for the term resulting from filling
in the holes of a context $C$ with the terms $\overline{u}_n$, left to right.
We say a term is {\em ground} if it has no variables.

{\em Substitutions} $\sigma, \varrho, \ldots$ are total mappings from variables to
terms of the same type. Substitutions map only finitely many variables to terms
other than the variable itself. This is denoted as $\{ x_1 \mapsto t_1, \ldots,
x_n \mapsto t_n \}$ where $x_i$ are variables that are not mapped to itself.
Applying a substitution $\sigma$ to a term $t$, denoted $\sigma(t)$ results in
replacing all mapped variables by the corresponding terms.
The composition $\varrho\sigma$ of substitutions is defined by
$\left(\varrho\sigma\right)t=\varrho\left(\sigma t\right)$. We say that substitution $\sigma$ is {\em grounding}
if $\sigma(x) \not= x$ implies that $\sigma(x)$ is ground.

\section{Higher-Order Logic}
\label{sec:pre:hol}

In this thesis we use classic simply typed monomorphic higher-order logic with
the choice operator (and some variations of this logic). Assuming a set of
base types $T$ and a set of symbols $\Sigma$, let us
define terms and types. Types are either base types $\tau \in T$, or function
types $\tau \rightarrow \upsilon$ where both $\tau$ and $\upsilon$ are types.
Each symbol $\cst{f} \in \Sigma$ is assigned a type.

Terms are defined as free variables $F, X, \ldots$, bound variables $x, y, z,
\dotsc$, or symbols $\cst{f}, \cst{g},\allowbreak \cst{a},\allowbreak \cst{b}
\dotsc$. Furthermore, if $s$ and $t$ are terms of type $\tau \rightarrow
\upsilon$ and $\tau$, respectively, then $s \, t$ is a term of type $\upsilon$.
Lastly, if $x$ is a bound variable of type $\tau$ and $s$ is a term of type
$\upsilon$, then $\lambda$-abstraction $\lamx{s}$ is a term of type $\tau
\rightarrow \upsilon$. The syntactic distinction between free and bound variables
gives rise to \emph{loose bound variables} (e.g., $y$ in $\lamx{y \, \cst{a}}$)
\cite{tn-93-patterns}. Note that there is no requirement that a symbol is not
applied to terms of Boolean type as in first-order logic. Furthermore, all
logical connectives are symbols present in the set $\Sigma$. This means that
higher-order logic does not distinguish between terms and formulas. For example,
$\cst{p} \iand \cst{q}$ and $\cst{f} \, (\cst{p} \iand \cst{q})$ are well-formed
terms of higher-order logic. For convenience, we still call terms of Boolean
types \emph{formulas}.

We assume the standard notions of $\alpha$, $\beta$ and $\eta$ conversions and
write $s \equi t$ if terms $s$ and $t$ are equal modulo $\alpha\beta\eta$
converison. We let $s \, \overline{t}_n$ stand for $s \, t_1 \, \ldots \, t_n$
and $\lam{\overline{x}_n}{s}$ for $\lambda x_1. \ldots \lambda x_n. \> s$. Every
$\beta$-reduced term can be written as $\lam{\tuple{x}{m}}{a \, \tuplen{t}}$,
where $a$ is not an application; we call $a$ the \emph{head} of the term. By
convention, $a$ and $b$ denote heads. If $a$ is a free variable, we call the term
\emph{flex}; otherwise, the term is \emph{rigid}.

Deviating from the standard notion of higher-order subterm, we define subterms
on $\beta$-reduced terms as follows: a term $t$ is a subterm of $t$ at position
$\varepsilon$. If $s$ is a subterm of $u_i$ at position $p$, then $s$ is a
subterm of $a\;\overline{u}_n$ at position $i.p$. If $s$ is a subterm of $t$ at
position $p$, then $s$ is a subterm of $\lambda x.\, t$ at position $1.p$. Our
definition of subterm gracefully generalizes the corresponding first-order
notion: $\cst{a}$ is a subterm of $\cst{f} \, \cst{a} \, \cst{b}$ at position 1,
but $\cst{f}$ and $\cst{f} \, \cst{a}$ are not subterms of $\cst{f} \, \cst{a}
\, \cst{b}$. We say a term is ground if it has no variables, and closed
if it has no loosely bound variables.

Higher-order substitutions ($\sigma,\varrho,\ldots$) are functions from free and bound
variables to terms. A variable $F$ is mapped by $\sigma$ if $ \sigma(F) \not\equi
F$. Each substitution maps only finitely many variables, and it is denoted as in
the first-order case. Application of  $\sigma$ to term $t$ is denoted $\sigma(t)$;
this application $\alpha$-renames $t$ to avoid variable capture. For example,
$\{X \mapsto x\}(\lamx{X \, \cst{a}})$ results in $\lam{y}{x \, \cst{a}}$.  Given a substitution
$\varrho$, which maps $F$ to $s$, we write $\varrho\setminus\{F \mapsto s\}$ to
denote a substitution that does not map $F$ and otherwise coincides with
$\varrho$. Given substitutions $\varrho$ and $\sigma$, mapping disjoint variable
sets, we write $\varrho \cup \sigma$ to denote $\varrho\sigma$.

Throughout this thesis, we consider completeness of higher-order calculi only
with respect to Henkin semantics \cite{bm-14-automation-ho}. Note that no
complete calculus exists for higher-order calculi with standard (full)
semantics.

\section{Clausal Forms}

The resolution and superposition calculi do not work directly on formulas, but
on their simplified forms, {\em clauses}. Thus, the initial problem, expressed as a
set of formulas, must be transformed in a set of clauses. For all three
discussed logics, there exists such a transformation that does not affect
satisfiability of the problem \cite{nw-01-small-cnf}.

A {\em literal} $l$ is an equation $s \eq t$ or a disequation $s \noteq t$. A clause is a
finite multiset of literals, interpreted and written disjunctively:\ $l_1 \llor
\cdots \llor l_n$. All free variables occurring in the clause are implicitly
universally quantified. Note that $\llor$ used to denote disjunction in clauses
is different than formula-building $\ior$.

In standard, non-clausal first-order logic, an atom is either
predicate or equational. For uniformity, and to
stay close to the implementation, we encode predicate atoms as equations with
$\itrue$. Negative literals are encoded as disequations. For example,
$\cst{even}(x)$ is encoded as $\poslit{\cst{even}(x)}$, and
$\neg\,\cst{even}(x)$ is encoded as $\neglit{\cst{even}(x)}$. 

Applying substitution to a literal is reduced to applying it to both sides of
the (dis)equa\-tion and it is extended pointwise to clauses. It is written in
postfix notation as $\sigmacl{l}$ and $\sigmacl{C}$. We say $\sigmacl{C}$ is a
ground instance of $C$ if $\sigma$ is a grounding substitution.

\section{Superposition}

Superposition is one of the most successful calculi for first-order logic with
equality. It ows its success to careful treatment of equality and built-in
heuristics to prune the search space such as term order and selection functions.
Before we introduce the rules of the calculus, we provide definitions of these and other
background concepts.

\subsection{Term Order and Selection}

Superposition calculus is parametrized by a {\em reduction ordering} $\succ$, an
ordering on terms that has the following properties:

\noindent\begin{tabular}{p{\dimexpr 0.3\linewidth-2\tabcolsep}p{\dimexpr 0.7\linewidth-2\tabcolsep}}
    \textit{Irreflexive} & For all terms $s$, $s \not\succ s$ \\
    \textit{Transitive} & For all terms $s, t, u$,  if $s\succ t$ and $t \succ u$ then $s \succ u$ \\ 
    \textit{Subterm property} & For all terms $s$ and contexts $C$, $C[s] \succ s$ \\
    \textit{Respects substitutions} & For all terms $s, t$ and substitutions $\sigma$, $s \succ t$ implies $\sigmaterm{s} \succ \sigmaterm{t}$ \\
    \textit{Respects contexts} & For all terms $s,t$ and contexts $C$, $s \succ t$ implies $C[s] \succ C[t]$ \\
    \textit{Ground total} & For any two
    ground terms $s$ and $t$ either $s \succ t$ or $t \succ s$. \\
    \textit{Well-founded} & There are no infinite chains of the form $s_1 \succ s_2 \succ \cdots$ 
\end{tabular}

The term order is lifted to literals and clauses using the {\em multiset extension} of
$\succ$. The order is extended to multisets as follows
\cite[Sect.~2.5]{bg-01-resolution}. For two multisets $S_1$ and $S_2$, we write $S_1 \succ S_2$ if $S_1 \not= S_2$
and whenever $S_2(x) > S_1(x)$ then there is some $y \succ x$ such that $S_1(y)
> S_2(y)$. We use notation $S(x)$ to denote the number of occurrences of $x$ in
$S$. To use multiset extension, positive literals are represented as $ \{ \{s\},
\{t\} \}$ and negative ones as $\{ \{ s, t \} \}$. Clauses are then represented as 
multisets of such literals. 

Two orders that are commonly used in superposition theorem proving are
Knuth-Bendix order (KBO) \cite[Sect.~5.4.4]{bn-98-tr-and-all-that} and
lexicographic path order (LPO) \cite[Sect.~5.4.2]{bn-98-tr-and-all-that}.
KBO assigns integer weights to symbols and uses precedence between symbols to
break eventual ties. LPO entirely relies on precedence and inspects term
structure more closely to determine the order. 

\newcommand{\selfun}{\ensuremath{\mathit{Sel}}}
{\em Literal selection function} is a function that selects a (multi)subset of literals.
Superposition requires that at least one of the selected literals is negative. 

\subsection{Unification}

Calculi from the resolution family, including superposition, perform inferences
only on unifiable terms. We say terms $s$ and $t$ are {\em unifiable} if there is a
substitution $\sigma$ such that $\sigmaterm{s} = \sigmaterm{t}$; we further say
$\sigma$ is the unifier. A \emph{unification constraint} $s \unif t$ is an
unordered pair of two terms of the same type. A \emph{most general unifier} is a
unifier $\sigma$, such that for any other unifier $\theta$, there is a
substitution $\varrho$ such that $\theta = \varrho\sigma$. When there is such a
$\varrho$, we say $\sigma$ is more general than $\theta$. In first-order logic,
the most general unifier is unique up to variable renaming. Furthermore,
first-order logic admits efficient algorithms for computing the most general
unifier \cite{hv-09-unifalgs}.

In higher-order logic, unification is performed modulo rules of
$\alpha\beta\eta$ conversion. Under these conditions, the uniqueness of the most
general unifier is not guaranteed. Consider the constraint $X \,
(\cst{f} \, \cst{a}) \unif \cst{f} \, (X \, \cst{a})$. Any substitution of the
form $\{ X \mapsto \lamx{\cst{f}^i \, x} \}$ is a unifier, with $\cst{f}^i \, x$
denoting iterated, $i$-fold application of $\cst{f}$. However, neither of these substitutions
is more general than the other. To generalize most general
unifiers to higher-order logic, the concept of tje {\em complete sets of unifiers} (CSU) was
introduced.

A (higher-order) unifier of a multiset of unification constraints $E$ is
a substitution $\sigma$, such that $\sigma(s) \equi \sigma(t)$, for all $s \unif t
\in E$. A complete set of unifiers of $E$ is defined as a
set $U$ of $E$'s unifiers along with a set $V$ of \emph{auxiliary variables}
such that no $s \unif t \in E$ contains variables from $V$ and for every unifier
$\rho$ of $E$, there exists a $\sigma \in U$ and a substitution $\theta$ such
that for all $X\not\in V,$ $\rho(X) = \theta(\sigma(X))$. The most general unifier
corresponds to a one-element CSU. A unifier of terms $s$ and $t$ is a unifier of
the singleton multiset $\{ s \unif t \}$. There is no algorithm to decide
if two higher-order terms are unifiable, but there exist algorithms
that enumerate all elements of CSU for unifiable terms.

These higher-order concepts gracefully generalize the corresponding first-order
ones. For example, if $s$ at $t$ are first-order there must exist a singleton
CSU, i.e., the most general unifier. Thus, we will not explicitly qualify
unifiers as first-order or higher-order and ensure the context provides enough
information.

\subsection{Inference Rules}
\newcommand{\mgu}{\ensuremath{\mathit{mgu}}}

Before we spell out the inference rules of standard, first-order superposition
let us introduce some helpful notation, following Schulz \cite{ss-02-brainiac}.
With $t|_p$ we denote subterm of $t$ at position $p$ and with $t[p \leftarrow
t']$ replacement of this subterm with $t'$. Let \selfun{} be a selection function,
$\succ$ a reduction ordering, $C = l \llor C'$ a clause, and $\sigma$ a
substitution. We say that \sigmacl{l} is eligible for resolution if: (1) nothing
is selected by \selfun{} and \sigmacl{l} is $\succ$-maximal within the literals in
\sigmacl{C} or (2) \selfun{} selected some literals and \sigmacl{l} is the maximal
within either positive or negative selected literals. We say \sigmacl{l} is
eligible for paramodulation if it is positive, nothing is selected and
\sigmacl{l} is the maximal literal within the literals of \sigmacl{C}. With
$\mgu(s,t)$ we denote the most general unifier of $s$ and $t$. Using horizontal
line to separate premises and the conclusion, four inference rules of
superposition are as follows:

\begin{tabular}{m{\dimexpr 0.36\linewidth-2\tabcolsep}m{\dimexpr 0.57\linewidth-2\tabcolsep}}
    \multicolumn{2}{l}{{\bf Equality resolution} (ER)} \\[\jot]
    $\namedinference{ER}{s \noteq t \llor C}{\sigmacl{C}}$ & where $\sigma = \mgu(s,t)$, and $(s \noteq t)\sigma$ is eligible for resolution
\end{tabular}

\medskip

\begin{tabular}{m{\dimexpr 0.36\linewidth-2\tabcolsep}m{\dimexpr 0.57\linewidth-2\tabcolsep}}
    \multicolumn{2}{l}{{\bf Equality factoring} (EF)} \\[\jot]
    $\namedinference{EF}{s \eq t \llor u \eq v \llor C}{\sigmacl{(t \noteq v \llor u \eq v \llor C)}}$ 
        & where $\sigma = \mgu(s,u), \sigma(t) \not\succ \sigma(s)$ and 
        $(s \eq t)\sigma$ is eligible for paramodulation
\end{tabular}

\medskip

\begin{tabular}{m{\dimexpr 0.36\linewidth-2\tabcolsep}m{\dimexpr 0.57\linewidth-2\tabcolsep}}
    \multicolumn{2}{l}{{\bf Superposition into positive literals} (SP)} \\[\jot]
    $\namedinference{SP}{s \eq t \llor C \qquad u \eq v \llor D}{(u[p \leftarrow t] \eq v \llor C \llor D)\sigma}$ 
        & where $\sigma = \mgu(u|_p, s), \sigma(t) \not\succ \sigma(s), \sigma(v) \not\succ \sigma(u)$,
        $(s \eq t)\sigma$ is eligible for paramodulation, just like $(u \eq v)\sigma$ and $u|_p$ is not a variable
\end{tabular}

\medskip

\begin{tabular}{m{\dimexpr 0.36\linewidth-2\tabcolsep}m{\dimexpr 0.57\linewidth-2\tabcolsep}}
    \multicolumn{2}{l}{{\bf Superposition into negative literals} (SN)} \\[\jot]
    $\namedinference{SN}{s \eq t \llor C \qquad u \noteq v \llor D}{(u[p \leftarrow t] \noteq v \llor C \llor D)\sigma}$ 
        & where $\sigma = \mgu(u|_p, s), \sigma(t) \not\succ \sigma(s), \sigma(v) \not\succ \sigma(u)$,
         $(s \eq t)\sigma$ is eligible for paramodulation, $(u \noteq v)\sigma$ is eligible for resolution, and $u|_p$ is not a variable
\end{tabular}

% \[
% \namedinference{ER}{s \noteq t \llor C}{\sigmacl{C}}
% \]
% where $\sigma = \mgu(s,t)$, and $(s \noteq t)\sigma$ is eligible
% for resolution; \infname{ER} stands for equality resolution

% \[
% \namedinference{ER}{s \noteq t \llor C}{\sigmacl{C}}
% \]
% where $\sigma = \mgu(s,t)$, and $(s \noteq t)\sigma$ is eligible
% for resolution; \infname{ER} stands for equality resolution

% \[
% \namedinference{EF}{s \eq t \llor u \eq v \llor C}{\sigmacl{(t \noteq v \llor u \eq v \llor C)}}
% \]
% where $\sigma = \mgu(s,u), \sigma(t) \not\succ \sigma(s)$ and 
% $(s \eq t)\sigma$ is eligible for paramodulation; \infname{EF} stands for equality factoring

% \[
% \namedinference{SP}{s \eq t \llor C \qquad u \eq v \llor D}{(u[p \leftarrow t] \eq v \llor C \llor D)\sigma}
% \]
% where $\sigma = \mgu(u|_p, s), \sigma(t) \not\succ \sigma(s), \sigma(v) \not\succ \sigma(u)$,
% $(s \eq t)\sigma$ is eligible for paramodulation, just like $(u \eq v)\sigma$ and $u|_p$ is not a variable;
% \infname{SP} stands for positive superposition 

% \[
% \namedinference{SN}{s \eq t \llor C \qquad u \noteq v \llor D}{(u[p \leftarrow t] \noteq v \llor C \llor D)\sigma}
% \]
% where $\sigma = \mgu(u|_p, s), \sigma(t) \not\succ \sigma(s), \sigma(v) \not\succ \sigma(u)$,
% $(s \eq t)\sigma$ is eligible for paramodulation, $(u \eq v)\sigma$ is eligible for resolution, and $u|_p$ is not a variable;
% \infname{SN} stands for negative superposition
\medskip

\subsection{The Redundancy Criterion and Simplification Rules}

Above described rules are used to generate new clauses from already derived
ones. Even though orderings and selection are used to reduce the search space,
it still grows rather fast. To identify unnecessary clauses in the search
space, superposition features \emph{redundancy criterion}
\cite[Sect.~4.2.2]{bg-01-resolution}. We say that a ground clause $C$ is
{\em redundant} in a ground set of clauses $N$ if it is entailed by a subset of
clauses in $N$ such that each clause in this subset is smaller than $C$. More
generally, a clause (ground or not) is redundant in set $N$ if every ground
instance of $C$ is redundant for some grounding of $N$. Removing redundant
clauses does not affect completeness of superposition.

Other than the four generating rules, superposition provers also use
simplification rules, which are justified using redundancy criterion. These
rules replace the premises with conclusions, and we denote them using double
bars. As an example of such rule, let us introduce rewriting (demodulation) of
negative literals (\infname{RN}):

\[
\namedsimp{RN}{s \eq t \qquad u \noteq v \llor D}
              {s \eq t \qquad (u[p \leftarrow \sigma(t)] \eq v \llor C \llor D)\sigma}
\]
where $\sigma = \mgu(u|_p, s)$, and $\sigma(s) \succ \sigma(t)$.


Simple rules such as removing duplicate literals, literals of the form $s \noteq
s$ or tautological clauses are clearly justified by the redundancy criterion.
Schulz gives an extensive list of such rules \cite{ss-02-brainiac}, including rewriting of positive
literals which has more side conditions.

We say that a clause $C$ subsumes clause $D$ is there is a substitution $\sigma$
such that $\sigmacl{C} \subseteq D$. Subsumption is one of the most important
operations within a theorem prover. This rule does not adhere to redundancy
criterion, but can be justified using other mechanisms \cite{wtrb-20-sat-framework}.

\subsection{The Saturation Procedure}

Superposition provers saturate the input problem with respect to the calculus's
inference rules using the \emph{given clause procedure}
\cite{mcw-1997-otter,adf-1995-discount}. It partitions the proof state into a
passive set $\mathcal{P}$ and an active set $\mathcal{A}$. All clauses start in
$\mathcal{P}$. At each iteration of the procedure's main loop, the prover
chooses a clause $C$ from $\mathcal{P}$, simplifies it, and moves it to
$\mathcal{A}$. Then all inferences between $C$ and active clauses are performed.
The resulting clauses are again simplified and put in $\mathcal{P}$.
The provers differ in which clauses are used for simplification: Otter-loop
\cite{mcw-1997-otter} provers use both active and passive clauses whereas
DISCOUNT-loop \cite{adf-1995-discount} provers use only active clauses.
The provers we discuss in this thesis, E and Zipperposition, are both DISCOUNT-loop provers.
