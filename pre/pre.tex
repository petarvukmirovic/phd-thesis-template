\chapter{Preliminaries}
\setheader{Preliminaries}
\label{ch:pre}

\begin{abstract}
    In this chapter we lay out the basic prerequisites for the remaining
    chapters. We begin by describing the three logics that we work with in this
    thesis: propositional logic, monomorphic first-order logic and simply typed
    higher-order logic. Then, we explain the clausal structure which is the
    backbone of many calculi for automated provers. We finish with the description of the
    superposition calculus.
\end{abstract}
      
\newpage

Our work mostly concerns retrofitting first-order provers to support
higher-order concepts. But part of our work also concerns integrating
successful approaches for propositional logic in first-order
provers. We begin by defining all three logics, in the increasing
order of expressivity.

\section{Propositional Logic}

Atomic formulas of propositional logic are propositional variables
$\cst{p}, \cst{q}, \ldots$ and constants $\top$ and $\bot$. More complex
formulas are built inductively using logical connectives $\inot, \iand, \ior,
\iimplies, \allowbreak \iequiv$: if $\phi$ and $\psi$ are formulas, then $\inot \phi, \phi \iand
\psi, \allowbreak \phi \ior \psi, \phi \iimplies \psi, \phi \iequiv \psi$ are formulas as well.

To interpret the formulas, propositional variables are assigned 0 (false) or 1
(true), constants $\top$ and $\bot$ are assigned 1 and 0 respectively and the
formula is interpreted using the rules for each connective
\cite[Sect.~1.4]{hr-00-logic-in-cs}. As there are finitely many propositional
variables in a formula, trying all (finitely many) possible variable assignments
describes an algorithm to decide satisfiability (existence of satisfiable
assignment for a formula) or validity (if all assignments are satisfy the formula).

However, this simple approach is prohibitively expensive and modern tools that
decide propositional satisfiability problem (SAT solvers) use more advanced
approaches such as CDCL calculus \cite{mss-96-cdcl}. Modern SAT solvers also
heavily preprocess the problem and continue simplifying it during proving
process.

\section{First-Order Logic}

First-order logic increases the expressivity by allowing quantification over
objects and has more complicated formula structure. There are many flavors of
first-order logic, but in this thesis we consider monomorphic first-order logic
with equality.

We distinguish a set of base types $T$ which is required to have the Boolean
type $o$, and a set of symbols $\Sigma$. To each symbol $\cst{f} \in \Sigma$ a
tuple $(\tau_1, \ldots, \tau_{n}, \tau), n \geq 0$ of types is assigned, written
$\cst{f} : (\tau_1, \ldots, \tau_{n}) \rightarrow \tau$. We say that $(\tau_1,
\ldots, \tau_{n})$ are argument types, $\tau$ is the return type, and that $n$
is the arity of the symbol $\cst{f}$. If return type of $\cst{f}$ is $o$ we call
$\cst{f}$ a predicate symbol, otherwise we call it a function symbol. Argument types may not be Boolean.

Terms are the basic building blocks of first-order logic. Variables
$x,y,\ldots$, assigned types $\tau \in T$, are terms. If $t_1,\ldots,t_n$ are
terms of types $\tau_1, \ldots, \tau_n$, respectively, and $\cst{f} : (\tau_1,
\ldots, \tau_{n}) \rightarrow \tau$, then $\cst{f}(t_1, \ldots, t_n)$ is a term
of type $\tau$. If $n=0$, we drop parentheses and write $\cst{f}$. We also
abbreviate $\cst{f}(t_1, \ldots, t_n)$ to $\cst{f}(\tuplen{t})$. Using similar
notation, we abbreviate a tuple of terms $(t_1, \ldots, t_n)$ as $(\tuplen{t})$
or simply $\tuplen{t}$.

Terms are used to build atoms. Atom is either a term $t$ of type $o$ or an
equation $s \eq t$ where terms $t$ and and $s$ are of the same type. Atoms are
combined using logical connectives to build formulas just like in propositional
logic. Additionally, first-order formulas are built using quantifiers: if $\phi$
is a formula and $x$ is a variable then $\iforall x.\, \phi$, as well as
$\iexists x.\, \phi$ are formulas. The first formula requires that $\phi$ holds
for any value of $x$, while the second one requires that there is some $x$ for
which $\phi$ holds. As soon as there is a single functional symbol with arity
greater than 0, there are infinitely many terms that can be substituted for a
free variable. Furthermore, in first-order logic to determine the truth value of
the formula, it does not suffice to assign values to variables. It is also
necessary to interpret symbols. Therefore, it is obvious that the propositional
technique for deciding satisfiability does not work in the first-order case.

A position in a term is a tuple of natural numbers, with $\varepsilon$ denoting
the empty tuple. Subterms and positions are inductively defined as follows. Term
$t$ is a subterm of itself at position $\varepsilon$. If $s$ is a subterm of
$t_i$ at position $p$, then $s$ is a subterm of $\cst{f}(\tuplen{t})$ at
position $i.p$. A context is a term with zero or more subterms replaced by a
hole $\square$. We write $C[\overline{u}_n]$ for the term resulting from filling
in the holes of a context $C$ with the terms $\overline{u}_n$, left to right.
We say a term is ground if it has no variables.

Substitutions $\sigma, \varrho, \ldots$ are total mappings from variables to
terms of the same type. Substitutions map only finitely many variables to terms
other than the variable itself. This is denoted as $\{ x_1 \mapsto t_1, \ldots,
x_n \mapsto t_n \}$ where $x_i$ are variables that are not mapped to itself.
Applying a substitution $\sigma$ to a term $t$, denoted $\sigma(t)$ results in
replacing all mapped variables by the corresponding terms.
The composition $\varrho\sigma$ of substitutions is defined by
$\left(\varrho\sigma\right)t=\varrho\left(\sigma t\right)$.

\section{Higher-Order Logic}

In this thesis we use classic simply typed monomorphic higher-order logic with
the choice operator (and some variations of this logic). Assuming a set of
base types $T$ and a set of symbols $\Sigma$, let us
define terms and types. Types are either base types $\tau \in T$, or function
types $\tau_1 \rightarrow \tau_2$ where both $\tau_1$ and $\tau_2$ are types.
Each symbol $\cst{f} \in \Sigma$ is assigned a type.

Terms are defined as free variables $F, X, \ldots$, bound variables $x, y, z,
\dotsc$, or symbols $\cst{f}, \cst{g},\allowbreak \cst{a},\allowbreak \cst{b}
\dotsc$. Furthermore, if $s$ and $t$ are terms of type $\tau_1 \rightarrow
\tau_2$, and $\tau_1$, respectively, then $s \, t$ is a term of type $\tau_2$.
Lastly, if $x$ is a bound variable of type $\tau_1$ and $s$ is a term of type
$\tau_2$, then $\lambda$-abstractions $\lamx{s}$ is a term of type $\tau_1
\rightarrow \tau_2$. The syntactic distinction between free and bound variables
gives rise to \emph{loose bound variables} (e.g., $y$ in $\lamx{y \, \cst{a}}$)
\cite{tn-93-patterns}. Out of terms, atoms and formulas are built in the same
way as in first-order logic.

We assume the standard notions of $\alpha$, $\beta$ and $\eta$ conversions and
write $s \equi t$ if terms $s$ and $t$ are equal modulo $\alpha\beta\eta$
converison. We let $s \, \overline{t}_n$ stand for $s \, t_1 \, \ldots \, t_n$
and $\lam{\overline{x}_n}{s}$ for $\lambda x_1. \ldots \lambda x_n. \> s$. Every
$\beta$-reduced term can be written as $\lam{\tuple{x}{m}}{a \, \tuplen{t}}$,
where $a$ is not an application; we call $a$ the \emph{head} of the term. By
convention, $a$ and $b$ denote heads. If $a$ is a free variable, we call the term
\emph{flex}; otherwise, the term is \emph{rigid}.

Deviating from the standard notion of higher-order subterm, we define subterms
on $\beta$-reduced terms as follows: a term $t$ is a subterm of $t$ at position
$\varepsilon$. If $s$ is a subterm of $u_i$ at position $p$, then $s$ is a
subterm of $a\;\overline{u}_n$ at position $i.p$. If $s$ is a subterm of $t$ at
position $p$, then $s$ is a subterm of $\lambda x.\, t$ at position $1.p$. Our
definition of subterm gracefully generalizes the corresponding first-order
notion: $\cst{a}$ is a subterm of $\cst{f} \, \cst{a} \, \cst{b}$ at position 1,
but $\cst{f}$ and $\cst{f} \, \cst{a}$ are not subterms of $\cst{f} \, \cst{a}
\, \cst{b}$. We say a term is ground if it has no variables, and closed
if it has no loosely bound variables.

Higher-order substitutions ($\sigma,\varrho,\ldots$) are functions from free and bound
variables to terms. A variable $F$ is mapped by $\sigma$ if $ \sigma F \not\equi
F$. Each substitution maps only finitely many variables, and it is denoted as in
first-order case. Application of  $\sigma$ to term $t$ is denoted $\sigma(t)$;
this application $\alpha$-renames $t$ to avoid variable capture. For example,
$\{X \mapsto x\}(\lamx{X \, a})$ results in $\lam{y}{x \, a}$.  Given a substitution
$\varrho$, which maps $F$ to $s$, we write $\varrho\setminus\{F \mapsto s\}$ to
denote a substitution that does not map $F$ and otherwise coincides with
$\varrho$. Given substitutions $\varrho$ and $\sigma$, which map disjoint sets
of variables, we write $\varrho \cup \sigma$ to denote $\varrho\sigma$.

Throughout this thesis, we consider completeness of higher-order calculi only
with respect to Henkin semantics \cite{bm-14-automation-ho}. Note that no
complete calculus exists for higher-order calculi with standard (full)
semantics.

\section{Clausal Forms}

The resolution and superposition calculi do not work directly on formulas, but
on their simplified forms, clauses. Thus, the initial problem, expressed as a
set of formulas, must be transformed in a set of clauses. For first-order and
higher-order logic, there exists such a transformation that does not affect
satisfiability of the problem \cite{nw-01-small-cnf}.

A literal $l$ is an equation $s \eq t$ or a disequation $s \noteq t$. A clause is a
finite multiset of literals, interpreted and written disjunctively:\ $l_1 \llor
\cdots \llor l_n$. All free variables occurring in the clause are implicitly
universally quantified. Note that $\llor$ used to denote disjunction in clauses
is different than formula-building $\ior$.

In standard, non-clausal, first-order and higher-order logic, an atom  is either
a predicate atom (term of Boolean type) or an equation. For uniformity, and to
stay close to the implementation, we encode predicate atoms as equations with
$\itrue$. Negative literals are encoded as disequations. For example,
$\cst{even}(x)$ is encoded as $\poslit{\cst{even}(x)}$, and
$\neg\,\cst{even}(x)$ is encoded as $\neglit{\cst{even}(x)}$. 

Applying substitution to a literal is reduced to applying it to both sides of
the (dis)equation and extended pointwise to clauses. It is written in postfix notation
as $\sigmacl{l}$ and $\sigmacl{C}$.

\section{Superposition}

Superposition is one of the most successful calculi for first-order logic with
equality. It ows its success to careful treatment of equality and built-in
heuristics to prune the search space such as term order and selection functions.
Before we introduce the rules of the calculus, we provide definitions of these and other
background concepts.

\subsection{Term Order and Selection}

Superposition calculus is parametrized by a reduction ordering $\succ$, an
ordering on terms that has the following properties:

\noindent\begin{tabular}{p{\dimexpr 0.3\linewidth-2\tabcolsep}p{\dimexpr 0.7\linewidth-2\tabcolsep}}
    \textit{Irreflexive} & For all terms $s$, $s \not\succ s$ \\
    \textit{Transitive} & For all terms $s, t, u$,  if $s\succ t$ and $t \succ u$ then $s \succ u$ \\ 
    \textit{Subterm property} & For all terms $s$ and contexts $C$, $C[s] \succ s$ \\
    \textit{Respects substitutions} & For all terms $s, t$ and substitutions $\sigma$, $s \succ t$ implies $\sigmaterm{s} \succ \sigmaterm{t}$ \\
    \textit{Respects contexts} & For all terms $s,t$ and contexts $C$, $s \succ t$ implies $C[s] \succ C[t]$ \\
    \textit{Ground total} & For any two
    ground terms $s$ and $t$ either $s \succ t$ or $t \succ s$. \\
    \textit{Well-founded} & There are no infinite chains of the form $s_1 \succ s_2 \succ \cdots$ 
\end{tabular}

The term order is lifted to literals and clauses using the multiset extension of
$\succ$. The order is extended to multisets as follows
\cite[Sect.~2.5]{bg-01-resolution}. We write $S_1 \succ S_2$ if $S_1 \not= S_2$
and whenever $S_2(x) > S_1(x)$ then there is some $y \succ x$ such that $S_1(y)
> S_2(y)$. We use notation $S(x)$ to denote the number of occurrences of $x$ in
$S$. 

Two orders that are commonly used in superposition theorem proving are
Knuth-Bendix order (KBO) \cite[Sect.~5.4.4]{bn-98-tr-and-all-that} and
lexicographic path order (LPO) \cite[Sect.~5.4.2]{bn-98-tr-and-all-that}.
KBO assigns integer weights to symbols and uses precedence between symbols to
break eventual ties. LPO completely relies on precedence and inspects term
structure more closely to deterine the order. 


\subsection{Unification}
\subsection{Inference Rules}
\subsection{The Redundancy Criterion and Simplification Rules}
\subsection{The Saturation Procedure}
