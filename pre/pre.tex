\chapter{Preliminaries}
\setheader{Preliminaries}
\label{ch:pre}

\begin{abstract}
    In this chapter we lay out the basic prerequisites for the remaining
    chapters. We begin by describing the three logics that we work with in this
    thesis: propositional logic, first-order logic, and higher-order logic. Then,
    we explain the clausal structure which is the backbone of many calculi for
    automated provers. We finish with the description of the superposition
    calculus. As this thesis discusses practical aspects of theorem proving, we
    define only the fundamental notions, while more advanced notions are
    intuitively described with references to rigorous definitions. The text of
    this chapter is partly based on the preliminaries sections of the
    publications listed in Chapter \ref{ch:intro}.
\end{abstract}
      
\newpage


\section{Propositional Logic}

\emph{Atomic formulas} (\emph{atoms}) of propositional logic are propositional variables
$\cst{p}, \cst{q}, \ldots$ and constants $\itrue$ and $\ifalse$. More complex
formulas are built inductively using logical connectives $\inot, \iand, \ior,
\iimplies, \allowbreak \iequiv$: if $\phi$ and $\psi$ are formulas, then $\inot \phi, \phi \iand
\psi, \allowbreak \phi \ior \psi, \phi \iimplies \psi, \phi \iequiv \psi$ are formulas as well.

To interpret the formulas, propositional variables are assigned 0 (false) or 1
(true), and constants $\itrue$ and $\ifalse$ are assigned 1 and 0, respectively; formulas are interpreted using the rules for each connective
\cite[Sect.~1.4]{hr-00-logic-in-cs}. As there are finitely many propositional
variables in a formula, trying all (finitely many) possible variable assignments
describes an algorithm to decide satisfiability (existence of satisfiable
assignment for a formula) or validity (if all assignments satisfy the formula).

This simple approach is prohibitively expensive and modern tools that
decide the propositional satisfiability problem (SAT solvers) use more advanced
approaches such as the CDCL calculus \cite{mss-96-cdcl}. A crucial improvement of
this calculus compared to more naive approaches is that it does not backtrack
chronologically when it determines that the partial model constructed for a set
of clauses does not satisfy all clauses. Instead, it generates a \emph{learned
clause} which explains how this conflicting state was reached, which enables
smarter backtracking.  Modern SAT solvers also heavily preprocess the problem
and continue simplifying it during the proving process.

\section{First-Order Logic}
\label{sec:pre:fol}

First-order logic increases the expressivity by allowing quantification over
objects and has a more complicated formula structure. There are many flavors of
first-order logic, but in this thesis we consider monomorphic first-order logic
with equality.

We distinguish a set of base types $T$ which is required to have the Boolean
type $o$, and a set of symbols $\Sigma$. To each symbol $\cst{f} \in \Sigma$ a
tuple $(\tau_1, \ldots, \tau_{n}, \tau), n \geq 0$ of types is assigned, written
$\cst{f} : (\tau_1, \ldots, \tau_{n}) \rightarrow \tau$. We say that $(\tau_1,
\ldots, \tau_{n})$ are \emph{argument types}, $\tau$ is the \emph{return type}, and $n$
is the \emph{arity} of the symbol $\cst{f}$. If the return type of $\cst{f}$ is $o$, we call
$\cst{f}$ a \emph{predicate symbol}, otherwise we call it a \emph{function symbol}. Argument types may not be Boolean.

\emph{Terms} are the basic building blocks of first-order logic, built inductively as follows. Variables
$x,y,\ldots$, assigned types $\tau \in T\setminus \{ o \}$, are terms. If $t_1,\ldots,t_n$ are
terms of types $\tau_1, \ldots, \tau_n$, respectively, and $\cst{f} : (\tau_1,
\ldots, \tau_{n}) \rightarrow \tau$, then $\cst{f}(t_1, \ldots, t_n)$ is a term
of type $\tau$. If $n=0$, we drop the parentheses and write $\cst{f}$. We also
abbreviate $\cst{f}(t_1, \ldots, t_n)$ to $\cst{f}(\tuplen{t})$. Using a similar
notation, we abbreviate a tuple of terms $(t_1, \ldots, t_n)$ as $(\tuplen{t})$
or simply $\tuplen{t}$.

\looseness=-1
Terms are used to build \emph{atoms}. An atom is either a term $t$ of type $o$ (\emph{predicate atom}) or an
equation $s \eq t$ where terms $t$ and $s$ are of the same type (\emph{equational atom}). Atoms are
combined using logical connectives to build formulas just like in propositional
logic. Additionally, first-order formulas are built using quantifiers: if $\phi$
is a formula and $x$ is a variable, then $\iforall x.\, \phi$, as well as
$\iexists x.\, \phi$ are formulas. Intuitively, the first formula requires that $\phi$ holds
for every value of $x$, while the second one requires that there is some $x$ for
which $\phi$ holds. As soon as there is a single functional symbol with arity
greater than 0, there are infinitely many terms that can be substituted for a
free variable. Furthermore, in first-order logic, it does not suffice to assign
values to variables to determine the truth value of the formula. It is also
necessary to interpret the symbols. Therefore, it is obvious that the propositional
technique for deciding satisfiability does not work in the first-order case.

Even though we do not formally define the semantics of logic, we assume the natural
extensions of domain, valuation, interpretation, and model (as defined by Fitting
\cite{mf-1996-fol}) from unsorted to many-sorted logic. The models we consider are
\emph{normal}, i.e., they interpret $\eq$ as an equality relation. Usual notions
of (un)satisfiability and (in)validity are assumed. We write
$\ourmodel~\models_\xi~N$ to denote that a model $\ourmodel$ satisfies a clause
set $N$, for a variable assignment $\xi$. If $\ourmodel$ is a model of $N$
(i.e., $\ourmodel$ satisfies it under every variable assignment), we simply write
$\ourmodel \models N.$  Overloading notation, we write $M \models N$ to denote that
$M$ \emph{entails} $N$, i.e., that every model of $M$ is a model of $N.$

\looseness=-1
A {\em position} in a term is a tuple of natural numbers, with $\varepsilon$
denoting the empty tuple. {\em Subterms} and positions are inductively defined
as follows. The term $t$ is a subterm of itself at position $\varepsilon$. If
$s$ is a subterm of $t_i$ at position $p$, then $s$ is a subterm of
$\cst{f}(\tuplen{t})$ at position $i.p$. A {\em context} is a term with zero or
more subterms replaced by a hole $\square$. We write $C[\overline{u}_n]$ for the
term resulting from filling in the holes of a context $C$ with the terms
$\overline{u}_n$, from left to right. We say a term is {\em ground} if it has no
variables. By $u[s]$ we denote that $s$ is a subterm of $u$. 

\looseness=-1
{\em Substitutions} $\sigma, \varrho, \ldots$ are total mappings from variables to
terms of the same type. Substitutions map only finitely many variables to terms
other than the variable itself. This is denoted as $\{ x_1 \mapsto t_1, \ldots,
x_n \mapsto t_n \}$ where $x_i$ are variables that are not mapped to themselves.
Applying a substitution $\sigma$ to a term $t$, denoted $\sigma(t)$, results in
replacing all mapped variables by the corresponding terms.
The composition $\varrho\sigma$ of substitutions is defined by
$\left(\varrho\sigma\right)t=\varrho\left(\sigma (t)\right)$. We call a substitution $\sigma$ {\em grounding}
if $\sigma(x) \not= x$ implies that $\sigma(x)$ is ground.

\section{Higher-Order Logic}
\label{sec:pre:hol}

In this thesis we use classic extensional simply typed monomorphic higher-order
logic with the choice operator, but without the description operator, the axiom of infinity,
and the axiom of at least two individuals \cite[Sect.~3.5]{bm-14-automation-ho}.
This logic corresponds to Henkin's extensional type theory. We additionally use
some variations of this logic, clearly stating the differences. Assuming a set of
base types $T$ and a set of symbols $\Sigma$, let us define terms and types.
Types are either base types $\tau \in T$, or function types $\tau \rightarrow
\upsilon$ where both $\tau$ and $\upsilon$ are types. Each symbol $\cst{f} \in
\Sigma$ is assigned a type.

Terms are defined as free variables $F, X, \ldots$, bound variables $x, y, z,
\dotsc$, or symbols $\cst{f}, \cst{g},\allowbreak \cst{a},\allowbreak \cst{b}
\dotsc$. Furthermore, if $s$ and $t$ are terms of type $\tau \rightarrow
\upsilon$ and $\tau$, respectively, then $s \, t$ is a term of type $\upsilon$.
Lastly, if $x$ is a bound variable of type $\tau$ and $s$ is a term of type
$\upsilon$, then $\lambda$-abstraction $\lamx{s}$ is a term of type $\tau
\rightarrow \upsilon$. The syntactic distinction between free and bound
variables gives rise to \emph{loose bound variables} \cite{tn-93-patterns},
bound variables that appear without an enclosing $\lambda$ binder  (e.g., $y$ in
$\lamx{y \, \cst{a}}$). Note that there is no requirement that a
symbol is not applied to terms of Boolean type as in first-order logic.
Furthermore, all logical connectives are symbols present in the set $\Sigma$.
This means that higher-order logic does not distinguish between terms and
formulas. For example, $\cst{p} \iand \cst{q}$ and $\cst{f} \, (\cst{p} \iand
\cst{q})$ are well-formed terms of higher-order logic. For convenience, we still
call terms of Boolean type \emph{formulas}. Our logic also supports the Hilbert
choice operator $\varepsilon$. Intuitively, this operator chooses an arbitrary value that
satisfies a predicate and it is axiomatized as follows: $\iforall p. \, (\iexists
x. \, p \, x) \iimplies p \, (\varepsilon \, p)$.


\looseness=-1
With $\alpha$-conversion we assume the rule $(\lambda x. \, s) \longrightarrow
\lambda y. \, \{x \mapsto y\}(s)$ where $y$ does not occur as a loose bound variable in $s$
and $y$ is not bound in $s$. Application of a substitution to a term
implicitly $\alpha$-renames bound variables to avoid capture.  For example, $\{X
\mapsto x\}(\lamx{X \, \cst{a}})$ results in $\lam{y}{x \, \cst{a}}$. The
$\beta$-reduction rule, roughly speaking, corresponds to passing arguments in a
function call: $(\lambda x. s) \, t \longrightarrow \{x \mapsto t\} (s)$, where
the bound variables in $s$ are renamed to avoid capture. Lastly,
$\eta$-reduction is defined as $\lambda x.\, s \, x \longrightarrow s$ under the
condition that $x$ is not loose bound in $x$. We write $s \equi t$ if terms
$s$ and $t$ are equal modulo $\alpha\beta\eta$-conversion. Terms that are
convertible with respect to any of the rules form equivalence classes. Thus, we
also say that $\alpha$($\beta\eta$)-convertible terms are
$\alpha$($\beta\eta$)-equivalent. 

In practice, many implementations of $\lambda$-terms avoid a named representation
of bound variables described above by using a \emph{locally nameless
representation} \cite{ac-12-locally-nameless}. In this representation, free
variables retain their names, while abstractions are simply represented by a
$\lambda$ symbol, without any names. Further, bound variables can be represented
using \emph{De Bruijn indices}. A bound variable with De Bruijn index
$\dbvar{i}$ is bound by the $i+1$-th enclosing $\lambda$ binder. For example,
the term $\lambda x. \, \cst{f} \, (\lambda y z. \, \cst{g} \, z \, x)$ is represented
as $\lambda\, \cst{f} \, (\lambda \lambda\, \cst{g} \, \db{0} \, \db{2})$.
The locally nameless representation reduces the $\alpha$-equivalence test to 
the syntactic equality test. 

\looseness=-1
As hinted in previous definitions, higher-order substitutions
($\sigma,\varrho,\ldots$) are functions from both free and bound variables to
terms. A variable $F$ is mapped by $\sigma$ if $ \sigma(F)\,\not\equi\,F$.
Substitutions map only finitely many variables. Given a substitution $\varrho$,
which maps $F$ to $s$, with $\varrho\setminus\{F \mapsto s\}$ we denote the
substitution that does not map $F$ and otherwise coincides with $\varrho$. Given
substitutions $\varrho$ and $\sigma$, mapping disjoint variable sets, we write
$\varrho \cup \sigma$ to denote $\varrho\sigma$.

We let $s \, \overline{t}_n$ stand for $s \, t_1 \, \ldots \, t_n$
and $\lam{\overline{x}_n}{s}$ for $\lambda x_1. \ldots \lambda x_n. \> s$,
omitting the length $n \geq 0$ when it is not important or it can be inferred. Every
$\beta$-reduced term can be written as $\lam{\tuple{x}{m}}{a \, \tuplen{t}}$,
where $a$ is not an application; we call $a$ the \emph{head} of the term. By
convention, $a, b$, and $\zeta$ denote heads. If $a$ is a free variable, we call the term
\emph{flex}; otherwise, the term is \emph{rigid}.

Deviating from the standard notion of higher-order subterm, we define subterms
on $\beta$-reduced terms as follows: a term $t$ is a subterm of $t$ at position
$\varepsilon$. If $s$ is a subterm of $u_i$ at position $p$, then $s$ is a
subterm of $a\;\overline{u}_n$ at position $i.p$. If $s$ is a subterm of $t$ at
position $p$, then $s$ is a subterm of $\lambda x.\, t$ at position $1.p$. Our
definition of subterm gracefully generalizes the corresponding first-order
notion: $\cst{a}$ is a subterm of $\cst{f} \, \cst{a} \, \cst{b}$ at position 1,
but $\cst{f}$ and $\cst{f} \, \cst{a}$ are not subterms of $\cst{f} \, \cst{a}
\, \cst{b}$. We say a term is ground if it has no free variables, and closed
if it has no loose bound variables.


Throughout this thesis, we consider completeness of higher-order calculi only
with respect to Henkin semantics \cite{bm-14-automation-ho}. Note that no
complete, consistent calculus can exist for higher-order calculi with standard (full)
semantics.

Some of the approaches described in this thesis are based on {\em rank-1
polymorphism} \cite{bp-13-tff1,ksr-16-th1}. This is an extension of simply typed
logic which adds type variables to the definition of types and type arguments to
polymorphic constants (Chapter \ref{ch:bools}). It further
requires that type arguments are instantiated with concrete, non-quantified
types. In a clausal structure it allows (implicit)
quantification over types only on top-level and not inside literals.

\section{Clausal Forms}
\label{sec:pre:clauses}

The standard resolution and superposition calculi do not work directly on formulas, but
on normal forms, {\em clauses}. Thus, the initial problem, expressed as a
set of formulas, must be transformed into a set of clauses. For propositional
and first-order logic, there exists such a transformation that does not affect
satisfiability of the problem \cite{nw-01-small-cnf}. In higher-order logic,
formulas are first-class citizens and higher-order calculi can in general work
with the nonnormalized problem, in its original form. However, resolution and
superposition-based higher-order calculi still perform best when the problem is clausified
using a transformation similar to the one used in the first-order case. We have analyzed
different approaches to clausification in a higher-order context \cite{bbtv-21-full-ho-sup}.

\looseness=-1
An {\em equation} $s \eq t$ corresponds to an unordered pair of terms. A {\em
literal} $l$ is an equation $s \eq t$ or its negation (disequation) $s \noteq
t$. A clause $C$ is a finite multiset of literals, interpreted and written
disjunctively:\ $l_1 \llor \cdots \llor l_n$. Free variables of clauses are
implicitly universally quantified. Note that clause-level operators ($\llor, \eq, \noteq$) are not typeset in bold
to distinguish them from term-building symbols ($\ior, \ieq, \ineq, \iand, \iforall, \ldots$) which are typeset in bold. In standard,
non-clausal first-order logic, an atom is either predicate or equational. For
uniformity, and to stay close to the implementation, we encode predicate atoms
as equations with $\itrue$. Negative literals are encoded as disequations. For
example, $\cst{even}(x)$ is encoded as $\poslit{\cst{even}(x)}$, and
$\neg\,\cst{even}(x)$ is encoded as $\cst{even}(x) \not\eq \itrue$. To lighten the
notation, we sometimes write the predicate literals in nonencoded form.
Applying a substitution to a literal is reduced to applying it to both sides of
the (dis)equa\-tion and it is extended pointwise to clauses. We say
$\sigmacl{C}$ is a ground instance of $C$ if $\sigmacl{C}$ has no free variables.
\pagebreak[2]

\section{Superposition}
\label{sec:pre:sup}

Superposition is one of the most successful calculi for first-order logic with
equality. It owes its success to a careful treatment of equality and built-in
heuristics to prune the search space such as term order and selection functions.

This calculus works on problems in clausified form and proves the problem by
refuting its negation. In practice this means that, to apply superposition to a
given problem, we first must negate its conjecture and clausify the whole
problem using some of the clausification algorithms
\cite{nw-01-small-cnf,rsv-16-vcnf}. The calculus then applies its inference
rules to the clauses, adding results of the inferences to the working set of
clauses. As this process is quite prolific, it also uses an order on terms which gives an indication of a
term's ``simplicity'' to determine
if some clauses can be simplified or eliminated from the working set.
Superposition is refutationally complete: This means that, if results of all
inferences are computed in a systematic, fair manner, the empty clause ($\bot$) will
eventually be derived for a provable (i.e., valid) problem. As first-order logic is
undecidable, no guarantees are given for unprovable (i.e., invalid) problems.
Before introducing the rules of the calculus, we provide definitions of the
background concepts.

\subsection{Term Order and Selection}
\label{sec:pre:order}
Superposition calculus is parameterized by a
term order $\succ$ with the following properties:

\noindent\begin{tabular}{p{\dimexpr 0.3\linewidth-2\tabcolsep}p{\dimexpr 0.7\linewidth-2\tabcolsep}}
    \textit{Irreflexive} & For all terms $s$, $s \not\succ s$ \\
    \textit{Transitive} & For all terms $s, t, u$,  if $s\succ t$ and $t \succ u$ then $s \succ u$ \\ 
    \textit{Subterm property} & For all terms $s$ and contexts $C$, $C[s] \succ s$ \\
    \textit{Respects substitutions} & For all terms $s, t$ and substitutions $\sigma$, $s \succ t$ implies $\sigmaterm{s} \succ \sigmaterm{t}$ \\
    \textit{Respects contexts} & For all terms $s,t$ and nonempty contexts $C$, $s \succ t$ implies $C[s] \succ C[t]$ \\
    \textit{Ground total} & For all distinct
    ground terms $s$ and $t$ either $s \succ t$ or $t \succ s$ \\
    \textit{Well-founded} & There are no infinite chains of the form $s_1 \succ s_2 \succ \cdots$ 
\end{tabular}

The term order is lifted to literals and clauses using the {\em multiset extension} of
$\succ$. The order is extended to multisets as follows
\cite[Sect.~2.5]{bg-01-resolution}. For two multisets $S_1$ and $S_2$, we write $S_1 \succ S_2$ if $S_1 \not= S_2$
and whenever $S_2(x) > S_1(x)$ then there is some $y \succ x$ such that $S_1(y)
> S_2(y)$. We use the notation $S(x)$ to denote the number of occurrences of $x$ in
$S$. To use the  multiset extension, positive literals are represented as $ \{ \{s\},
\{t\} \}$ and negative ones as $\{ \{ s, t \} \}$, which makes the negative literals slightly greater than the positive ones.
Clauses are then represented as 
multisets of such literals. 

Two orders that are commonly used in superposition theorem proving are the
Knuth--Bendix order (KBO) \cite[Sect.~5.4.4]{bn-98-tr-and-all-that} and the
lexicographic path order (LPO) \cite[Sect.~5.4.2]{bn-98-tr-and-all-that}.
KBO assigns integer weights to symbols and uses precedence between symbols to
break eventual ties. LPO entirely relies on precedence and inspects term
structures more closely to determine the order. 

\newcommand{\selfun}{\ensuremath{\mathit{Sel}}}
The {\em literal selection function} selects a (multi)subset of literals
from a given (multi)set of literals. Superposition requires that at least one of the selected literals is negative.
\pagebreak[2] 

\subsection{Unification}
\label{sec:pre:unif}

Calculi from the resolution family, including superposition, perform inferences
only on unifiable terms. We say terms $s$ and $t$ are {\em unifiable} if there is a
substitution $\sigma$ such that $\sigmaterm{s} = \sigmaterm{t}$; we further say
$\sigma$ is a unifier. A \emph{unification constraint} $s \unif t$ is a pair of two terms of the same type. 
We always specify if the constraint should be interpreted as ordered or unordered.
We say a substitution $\sigma$ is \emph{more general} than $\varrho$ if there exists a substitution $\theta$
such that $\varrho = \theta\sigma$.
A \emph{most general unifier} ({MGU}) is a
unifier $\sigma$, such that for any other unifier $\varrho$, $\sigma$ is more general than $\varrho$. In first-order logic,
the MGU is unique up to variable renaming. For example, $\cst{f}(X) \unif \cst{f}(Y)$ yields the MGU 
$\{ X \mapsto Y\}$ or  $\{ Y \mapsto X \}$. First-order logic also admits efficient algorithms for computing MGUs \cite{hv-09-unifalgs}.

In higher-order logic, unification is performed modulo
$\alpha\beta\eta$-equivalence. Under these conditions, the uniqueness of the MGU is not guaranteed. Consider the constraint $X \,
(\cst{f} \, \cst{a}) \unif \cst{f} \, (X \, \cst{a})$. Any substitution of the
form $\{ X \mapsto \lamx{\cst{f}^i \, x} \}$ is a unifier, with $\cst{f}^i \, x$
denoting iterated, $i$-fold application of $\cst{f}$. However, neither of these substitutions
is more general than the other. To generalize MGUs to higher-order logic, the concept of the {\em complete sets of unifiers} (CSU) was
introduced.

A (higher-order) unifier of a multiset of unification constraints $E$ is
a substitution $\sigma$, such that $\sigma(s) \equi \sigma(t)$, for all $s \unif t
\in E$. A complete set of unifiers of $E$ is defined as a
set $U$ of $E$'s unifiers along with a set $V$ of \emph{auxiliary variables}
such that no $s \unif t \in E$ contains variables from $V$ and for every unifier
$\rho$ of $E$ there exists a $\sigma \in U$ and a substitution $\theta$ such
that for all $X\not\in V,$ $\rho(X) = \theta(\sigma(X))$. The MGU,
when it exists, corresponds to a one-element CSU. A unifier of terms $s$ and $t$ is a unifier of
the singleton multiset $\{ s \unif t \}$. There is no algorithm to decide
if two higher-order terms are unifiable, but there exist procedures
that enumerate all elements of the CSU for two terms.

These higher-order concepts gracefully generalize the corresponding first-order
ones. For example, if $s$ at $t$ are first-order, then there must exist a singleton
CSU, i.e., the MGU. 

\subsection{Inference Rules}
\label{sec:pre:rules}
\newcommand{\mgu}{\ensuremath{\mathit{mgu}}}

Before spelling out the inference rules of standard, first-order superposition,
let us introduce some helpful notation, following Schulz \cite{ss-02-brainiac}.
With $t|_p$ we denote the subterm of $t$ at position $p$ and with $t[p \leftarrow
t']$  we denote the term obtained by replacing this subterm with $t'$. Let \selfun{} be a selection function,
$\succ$ a term order, $C = l \llor C'$ a clause, and $\sigma$ a
substitution. We say that \sigmacl{l} is eligible for resolution if: (1) nothing
is selected by \selfun{} and \sigmacl{l} is $\succ$-maximal within the literals in
\sigmacl{C} or (2) \selfun{} selected some literals and \sigmacl{l} is the maximal
within either positive or negative selected literals. We say \sigmacl{l} is
eligible for paramodulation if it is positive, nothing is selected, and
\sigmacl{l} is the maximal literal within the literals of \sigmacl{C}.  With
$\mgu(s,t)$ we denote the MGU of $s$ and $t$. Using a horizontal
line to separate premises and the conclusion, the four inference rules of
superposition are as follows:

\medskip
\begin{tabular}{m{\dimexpr 0.36\linewidth-2\tabcolsep}m{\dimexpr 0.57\linewidth-2\tabcolsep}}
    \multicolumn{2}{l}{{\bf Equality resolution} (ER)} \\[\jot]
    $\namedinference{ER}{s \noteq t \llor C}{\sigmacl{C}}$ & where $\sigma = \mgu(s,t)$, and $\sigmacl{s \noteq t}$ is eligible for resolution
\end{tabular}

\medskip

\begin{tabular}{m{\dimexpr 0.36\linewidth-2\tabcolsep}m{\dimexpr 0.57\linewidth-2\tabcolsep}}
    \multicolumn{2}{l}{{\bf Equality factoring} (EF)} \\[\jot]
    $\namedinference{EF}{s \eq t \llor u \eq v \llor C}{\sigmacl{t \noteq v \llor u \eq v \llor C}}$ 
        & where $\sigma = \mgu(s,u), \sigma(t) \not\succ \sigma(s)$ and 
        $\sigmacl{s \eq t}$ is eligible for paramodulation
\end{tabular}

\medskip

\begin{tabular}{m{\dimexpr 0.36\linewidth-2\tabcolsep}m{\dimexpr 0.57\linewidth-2\tabcolsep}}
    \multicolumn{2}{l}{{\bf Superposition into positive literals} (SP)} \\[\jot]
    $\namedinference{SP}{s \eq t \llor C \qquad u \eq v \llor D}{\sigmacl{u[p \leftarrow t] \eq v \llor C \llor D}}$ 
        & where $\sigma = \mgu(u|_p, s), \sigma(t) \not\succ \sigma(s), \sigma(v) \not\succ \sigma(u)$,
        $\sigmacl{s \eq t}$ and $\sigmacl{u \eq v}$ are eligible for paramodulation, and $u|_p$ is not a variable
\end{tabular}

\medskip

\begin{tabular}{m{\dimexpr 0.36\linewidth-2\tabcolsep}m{\dimexpr 0.57\linewidth-2\tabcolsep}}
    \multicolumn{2}{l}{{\bf Superposition into negative literals} (SN)} \\[\jot]
    $\namedinference{SN}{s \eq t \llor C \qquad u \noteq v \llor D}{\sigmacl{u[p \leftarrow t] \noteq v \llor C \llor D}}$ 
        & where $\sigma = \mgu(u|_p, s), \sigma(t) \not\succ \sigma(s), \sigma(v) \not\succ \sigma(u)$,
         $\sigmacl{s \eq t}$ is eligible for paramodulation, $\sigmacl{u \noteq v}$ is eligible for resolution, and $u|_p$ is not a variable
\end{tabular}

\medskip
We call these four rules \emph{generating rules} as their conclusions are added to
the set of clauses. In the rest of this thesis we denote generating rules with a
horizontal bar separating premises and conclusions.

Let us give some intuition behind how these rules are applied in practice. The
\infname{ER} rule is used to establish reflexivity of equality---the property
that each term is equal to itself. Consider the formula $\iforall x.\,  x \eq
\cst{2} \iimplies \cst{even}(x)$. In clausal form it is $X \noteq \cst{2} \llor
\cst{even}(X)$. Applying the \infname{ER} rule effectively applies the
precondition $X \eq \cst{2}$ to the conclusion $\cst{even}(X)$ to obtain
$\cst{even}(\cst{2})$. 

Similarly to the factoring rule in the standard resolution calculus, the
\infname{EF} rule can be used to find instances of the clause that duplicate
some literals.
Additionally, it takes care of some rarely occurring edge cases that are a consequence of restrictive
side-conditions of other rules of the calculus. Consider the clause $\cst{f}(X)
\eq \cst{a} \llor \cst{f}(Y) \eq \cst{a} \llor \cst{p}(X, Y)$. Applying
\infname{EF} to its first two literals yields $\cst{a} \noteq \cst{a} \llor
\cst{f}(X) \eq \cst{a} \llor \cst{p}(X, X)$ (with $\sigma = \{ Y \mapsto X \}$).
This clause then further simplifies to $\cst{f}(X) \eq \cst{a} \llor \cst{p}(X,
X)$ using \infname{ER}.

Lastly, the rules \infname{SP} and \infname{SN} are used to simulate what
mathematicians do when they replace equals by equals. In calculi preceding
superposition, the name paramodulation is often used for rules that replace
equals by equals. Note that with \emph{the superposition rule} we will refer to both
\infname{SN} and \infname{SP}. The left premise intuitively states that
an equation $s \eq t$ holds under condition $C$. Similarly, the right premise
asserts that a (dis)equation holds under a condition. The superposition rule
simply concatenates the conditions for both premises and replaces equals by
equals in two main inference terms. For example, given clauses $\neg
\cst{even}(X) \llor \cst{f}(X) \eq \cst{a}$ and $\neg \cst{even}(\cst{b}) \llor
\cst{g}(\cst{f}(Y)) \eq \cst{b}$, superposition between their last literals
results in $\neg \cst{even}(Y) \llor \neg \cst{even}(\cst{b}) \llor
\cst{g}(\cst{a}) \eq \cst{b}$.

\medskip

\subsection{The Redundancy Criterion and Simplification Rules}

The rules described above are used to generate new clauses from already derived
ones. Even though orders and selection are used to reduce the search space,
it still grows rather fast. To identify unnecessary clauses in the search
space, superposition features a \emph{redundancy criterion}
\cite[Sect.~4.2.2]{bg-01-resolution}. We say that a ground clause $C$ is
{\em redundant} in a ground set of clauses $N$ if it is entailed by a subset of
clauses in $N$ such that each clause in this subset is $\succ$-smaller than $C$. More
generally, a clause (ground or not) is redundant in set $N$ if every ground
instance of $C$ is redundant for the grounding of $N$. Removing redundant
clauses does not affect completeness of superposition.

Next to the four generating rules, superposition provers also use
simplification rules, which are justified by the redundancy criterion. These
rules effectively replace the pre\-mises with conclusions, and we denote them
using double bars. In theory, the rules work by adding conclusions and then
using them to show that premises are redundant; after this, premises can be eliminated. As
an example of such a rule, let us introduce rewriting (demodulation) of negative
literals (\infname{RN}):

\[
\namedsimp{RN}{s \eq t \qquad u \noteq v \llor D}
              {s \eq t \qquad \sigmacl{u[p \leftarrow \sigma(t)] \eq v \llor C \llor D}}
\]
where $\sigma = \mgu(u|_p, s)$ and $\sigma(s) \succ \sigma(t)$.


Simple rules such as removing duplicate literals, literals of the form $s \noteq
s$, or tautological clauses are clearly justified by the redundancy criterion.
Schulz gives an extensive list of such rules \cite{ss-02-brainiac}. This list
includes the rule which allows rewriting positive literals but has more side
conditions than \infname{RN}.

We say that a clause $C$ subsumes clause $D$ if there is a substitution $\sigma$
such that $\sigmacl{C} \subseteq D$. Subsumption is one of the most important
operations of a theorem prover. This rule does not adhere to the redundancy
criterion, but can be justified using other mechanisms \cite{wtrb-20-sat-framework}.

\subsection{The Saturation Procedure}
\label{sec:pre:saturation}

Superposition provers saturate the input problem with respect to the calculus's
inference rules using the \emph{given clause procedure}
\cite{mcw-1997-otter,adf-1995-discount}. It partitions the proof state into a
passive set $\mathcal{P}$ and an active set $\mathcal{A}$. All clauses start in
$\mathcal{P}$. At each iteration of the procedure's main loop, the prover
chooses a clause $C$ from $\mathcal{P}$, simplifies it, and moves it to
$\mathcal{A}$ (i.e., it \emph{activates} it). Then all inferences between $C$ and active clauses are performed.
The resulting clauses are again simplified and put in $\mathcal{P}$.
We call the pair $(\mathcal{P}, \mathcal{A})$ the \emph{proof state}.
Provers differ in which clauses are used for simplification: Otter-loop
\cite{mcw-1997-otter} provers use both active and passive clauses whereas
DISCOUNT-loop \cite{adf-1995-discount} provers use only active clauses.
The provers we discuss in this thesis, E and Zipperposition, are both DISCOUNT-loop provers.

\subsection{Higher-Order Superposition Calculi}
\label{sec:pre:ho-sup-calculi}

\looseness=-1
In Chapter \ref{ch:intro} we described a roadmap (consisting of three steps) to
extend an efficient superposition prover to higher-order logic. Together with
our colleagues we did the necessary theoretical work by extending the
superposition calculus using the same roadmap:

\begin{enumerate}
    \item Bentkamp, Blanchette, Cruanes, and Waldmann designed a complete
    superposition calculus for higher-order logic devoid of
    $\lambda$-abstraction and first-class Booleans \cite{bbcw-21-lfho}. This calculus is called the \emph{Boolean-free $\lambda$-free higher-order superposition calculus} 
    and we shortly refer to it as \lfsup{}.
    \item \begin{sloppypar}
    Bentkamp, Blanchette, Tourret,  VukmiroviÄ‡, and Waldmann
    extended this calculus to support $\lambda$-abstraction
    \cite{bbtvw-21-sup-lam}. They call this new calculus the \emph{Boolean-free $\lambda$-superposition calculus},
    and we shortly refer to it as \lsup{}.
    \end{sloppypar}        
    \item Bentkamp, Blanchette, Tourret, and Vukmirovi\'c extended \lsup{} with
    support for first-class Booleans \cite{bbtv-21-full-ho-sup}. This amounts to
    designing the complete calculus for full higher-order logic. This
    calculus is called Boolean $\lambda$-superposition, or shortly \osup{}.
\end{enumerate}
%
Note that we use the name $\lambda$-superposition for both \lsup{}
and \osup{}.

\subsection{Theorem Provers}
\label{sec:pre:theorem-provers}

All the techniques described in this thesis have been implemented in two superposition
theorem provers: Zipperposition
\cite{sc-15-simon-phd,sc-supind-17} and E \cite{ss-02-brainiac}. The former was used as a testbed for
prototyping ideas, while the latter is the main target of our work as it is more
efficient.

\ourpara{Zipperposition}
Zipperposition is a higher-order theorem prover implementing \lfsup{},
\lsup{}, \osup{}, and other superposition-like calculi (including Bhayat and
Reger's combinatory superposition \cite{br-20-full-sup-w-combs}). The prover was
conceived as a testbed for rapidly experimenting with extensions of first-order
superposition, but over time it has assimilated many of E's techniques and
heuristics and become quite powerful. Still, its first-order performance is a
far cry from E's. Zipperposition is written in OCaml, and features a modular
system for adding new rules and techniques to implemented calculi.
This makes it attractive for fast evaluation of various promising extensions of
superposition.

\ourpara{E} E is a state-of-the-art first-order prover based on superposition.
In the last decade, together with its derivatives, it has revolved around the
second place at the first-order division of the CASC theorem prover
competition \cite{ss-96-casc}. It is open-source, written in C (without using any
external libraries), and designed around efficient algorithms and data structures
rather than a highly optimized codebase. This makes it a good target for implementing
successful higher-order techniques previously evaluated in Zipperposition.


