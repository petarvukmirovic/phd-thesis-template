\chapter{Preliminaries}
\setheader{Preliminaries}
\label{ch:pre}

\begin{abstract}
    In this chapter we lay out the basic prerequisites for the remaining
    chapters. We begin by describing the three logics that we work with in this
    thesis: propositional logic, monomorphic first-order logic and simply typed
    higher-order logic. Then, we explain the clausal structure which is the
    backbone of many calculi for automated provers. We finish with the
    description of the superposition calculus. As this thesis discusses practical
    aspects of theorem proving, we define only the fundamental notions, while
    more advanced notions are intuitively described with references to rigorous
    definitions. The text of this chapter is
    partially based on the preliminaries sections of the publications listed in
    Chapter \ref{ch:intro}.
    
\end{abstract}
      
\newpage


\section{Propositional Logic}

\emph{Atomic formulas} of propositional logic are propositional variables
$\cst{p}, \cst{q}, \ldots$ and constants $\itrue$ and $\ifalse$. More complex
formulas are built inductively using logical connectives $\inot, \iand, \ior,
\iimplies, \allowbreak \iequiv$: if $\phi$ and $\psi$ are formulas, then $\inot \phi, \phi \iand
\psi, \allowbreak \phi \ior \psi, \phi \iimplies \psi, \phi \iequiv \psi$ are formulas as well.

To interpret the formulas, propositional variables are assigned 0 (false) or 1
(true), and constants $\itrue$ and $\ifalse$ are assigned 1 and 0, respectively; formulas are interpreted using the rules for each connective
\cite[Sect.~1.4]{hr-00-logic-in-cs}. As there are finitely many propositional
variables in a formula, trying all (finitely many) possible variable assignments
describes an algorithm to decide satisfiability (existence of satisfiable
assignment for a formula) or validity (if all assignments satisfy the formula).

However, this simple approach is prohibitively expensive and modern tools that
decide propositional satisfiability problem (SAT solvers) use more advanced
approaches such as the CDCL calculus \cite{mss-96-cdcl}. Modern SAT solvers also
heavily preprocess the problem and continue simplifying it during the proving
process.

\section{First-Order Logic}
\label{sec:pre:fol}

First-order logic increases the expressivity by allowing quantification over
objects and has a more complicated formula structure. There are many flavors of
first-order logic, but in this thesis we consider monomorphic first-order logic
with equality.

We distinguish a set of base types $T$ which is required to have the Boolean
type $o$, and a set of symbols $\Sigma$. To each symbol $\cst{f} \in \Sigma$ a
tuple $(\tau_1, \ldots, \tau_{n}, \tau), n \geq 0$ of types is assigned, written
$\cst{f} : (\tau_1, \ldots, \tau_{n}) \rightarrow \tau$. We say that $(\tau_1,
\ldots, \tau_{n})$ are \emph{argument types}, $\tau$ is the \emph{return type}, and $n$
is the \emph{arity} of the symbol $\cst{f}$. If return type of $\cst{f}$ is $o$ we call
$\cst{f}$ a \emph{predicate symbol}, otherwise we call it a \emph{function symbol}. Argument types may not be Boolean.

\emph{Terms} are the basic building blocks of first-order logic, built inductively as follows. Variables
$x,y,\ldots$, assigned types $\tau \in T$, are terms. If $t_1,\ldots,t_n$ are
terms of types $\tau_1, \ldots, \tau_n$, respectively, and $\cst{f} : (\tau_1,
\ldots, \tau_{n}) \rightarrow \tau$, then $\cst{f}(t_1, \ldots, t_n)$ is a term
of type $\tau$. If $n=0$, we drop the parentheses and write $\cst{f}$. We also
abbreviate $\cst{f}(t_1, \ldots, t_n)$ to $\cst{f}(\tuplen{t})$. Using a similar
notation, we abbreviate a tuple of terms $(t_1, \ldots, t_n)$ as $(\tuplen{t})$
or simply $\tuplen{t}$.

Terms are used to build \emph{atoms}. An atom is either a term $t$ of type $o$ (\emph{predicate atom}) or an
equation $s \eq t$ where terms $t$ and and $s$ are of the same type (\emph{equational atom}). Atoms are
combined using logical connectives to build formulas just like in propositional
logic. Additionally, first-order formulas are built using quantifiers: if $\phi$
is a formula and $x$ is a variable, then $\iforall x.\, \phi$, as well as
$\iexists x.\, \phi$ are formulas. The first formula requires that $\phi$ holds
for any value of $x$, while the second one requires that there is some $x$ for
which $\phi$ holds. As soon as there is a single functional symbol with arity
greater than 0, there are infinitely many terms that can be substituted for a
free variable. Furthermore, in first-order logic, it does not suffice to assign
values to variables to determine the truth value of the formula. It is also
necessary to interpret the symbols. Therefore, it is obvious that the propositional
technique for deciding satisfiability does not work in the first-order case.

Even though we do not formally define the semantics of logic, we assume the natural
extensions of domain, valuation, interpretation and model (as defined by Fitting
\cite{mf-1996-fol}) from unsorted to many-sorted logic. The models we consider are
\emph{normal}, i.e., they interpret $\eq$ as an equality relation. Usual notions
of (un)satisfiability and (in)validity are assumed. We write
$\ourmodel~\models_\xi~N$ to denote that a model $\ourmodel$ satisfies a clause
set $N$, for a variable assignment $\xi$. If $\ourmodel$ is a model of $N$
(i.e., $\ourmodel$ satisfies it under every variable assignment), we simply write
$\ourmodel \models N.$  Abusing notation, we write $M \models N$ to denote that
$M$ \emph{entails} $N$, i.e., that every model of $M$ is a model of $N.$

A {\em position} in a term is a tuple of natural numbers, with $\varepsilon$ denoting
the empty tuple. {\em Subterms} and positions are inductively defined as follows. Term
$t$ is a subterm of itself at position $\varepsilon$. If $s$ is a subterm of
$t_i$ at position $p$, then $s$ is a subterm of $\cst{f}(\tuplen{t})$ at
position $i.p$. A {\em context} is a term with zero or more subterms replaced by a
hole $\square$. We write $C[\overline{u}_n]$ for the term resulting from filling
in the holes of a context $C$ with the terms $\overline{u}_n$, left to right.
We say a term is {\em ground} if it has no variables. By $u[s]$
we denote that $s$ is a subterm of $u$. 

{\em Substitutions} $\sigma, \varrho, \ldots$ are total mappings from variables to
terms of the same type. Substitutions map only finitely many variables to terms
other than the variable itself. This is denoted as $\{ x_1 \mapsto t_1, \ldots,
x_n \mapsto t_n \}$ where $x_i$ are variables that are not mapped to themselves.
Applying a substitution $\sigma$ to a term $t$, denoted $\sigma(t)$, results in
replacing all mapped variables by the corresponding terms.
The composition $\varrho\sigma$ of substitutions is defined by
$\left(\varrho\sigma\right)t=\varrho\left(\sigma t\right)$. We say that substitution $\sigma$ is {\em grounding}
if $\sigma(x) \not= x$ implies that $\sigma(x)$ is ground.

\section{Higher-Order Logic}
\label{sec:pre:hol}

In this thesis we use classic simply typed monomorphic higher-order logic with
the choice operator (and some variations of this logic). Assuming a set of
base types $T$ and a set of symbols $\Sigma$, let us
define terms and types. Types are either base types $\tau \in T$, or function
types $\tau \rightarrow \upsilon$ where both $\tau$ and $\upsilon$ are types.
When we write $\tau_1 \rightarrow \cdots \rightarrow \tau_n \rightarrow \upsilon$
we assume $\upsilon$ to be a base type ($\upsilon \in T$), by convention.
Each symbol $\cst{f} \in \Sigma$ is assigned a type.

Terms are defined as free variables $F, X, \ldots$, bound variables $x, y, z,
\dotsc$, or symbols $\cst{f}, \cst{g},\allowbreak \cst{a},\allowbreak \cst{b}
\dotsc$. Furthermore, if $s$ and $t$ are terms of type $\tau \rightarrow
\upsilon$ and $\tau$, respectively, then $s \, t$ is a term of type $\upsilon$.
Lastly, if $x$ is a bound variable of type $\tau$ and $s$ is a term of type
$\upsilon$, then $\lambda$-abstraction $\lamx{s}$ is a term of type $\tau
\rightarrow \upsilon$. The syntactic distinction between free and bound variables
gives rise to \emph{loose bound variables} (e.g., $y$ in $\lamx{y \, \cst{a}}$)
\cite{tn-93-patterns}. Note that there is no requirement that a symbol is not
applied to terms of Boolean type as in first-order logic. Furthermore, all
logical connectives are symbols present in the set $\Sigma$. This means that
higher-order logic does not distinguish between terms and formulas. For example,
$\cst{p} \iand \cst{q}$ and $\cst{f} \, (\cst{p} \iand \cst{q})$ are well-formed
terms of higher-order logic. For convenience, we still call terms of Boolean
type \emph{formulas}.

\looseness=-1
With $\alpha$-renaming we assume the rule $(\lambda x. \, s) \longrightarrow \lambda
y. \, \{x \mapsto y\}(s)$ where $y$ does not occur loosely bound in $s$ and 
$y$ is not bound in $s$. Application of substitution $\sigma$ to term
$t$, implicitly $\alpha$-renames bound variables to avoid capture.  For example,
$\{X \mapsto x\}(\lamx{X \, \cst{a}})$ results in $\lam{y}{x \, \cst{a}}$. The
$\beta$-reduction rule corresponds to applying arguments to function: $(\lambda
x. s) \, t \longrightarrow \{x \mapsto t\} (s)$, where the bound variables in $s$
are renamed to avoid capture. Lastly, $\eta$-reduction is defined as $\lambda
x.\, s \, x \longrightarrow s$ under the condition that $x$ is not loosely
bound in $x$. We write $s \equi t$ if terms $s$ and $t$ are equal modulo $\alpha\beta\eta$
converison. 

\looseness=-1
As hinted in previous definitions, higher-order substitutions
($\sigma,\varrho,\ldots$) are functions from both free and bound variables to
terms. A variable $F$ is mapped by $\sigma$ if $ \sigma(F)\,\not\equi\,F$.
Substitutions map only finitely many variables. Given a substitution $\varrho$,
which maps $F$ to $s$, with $\varrho\setminus\{F \mapsto s\}$ we denote a
substitution that does not map $F$ and otherwise coincides with $\varrho$. Given
substitutions $\varrho$ and $\sigma$, mapping disjoint variable sets, we write
$\varrho \cup \sigma$ to denote $\varrho\sigma$.

We let $s \, \overline{t}_n$ stand for $s \, t_1 \, \ldots \, t_n$
and $\lam{\overline{x}_n}{s}$ for $\lambda x_1. \ldots \lambda x_n. \> s$,
omitting the length $n \geq 0$ when it is not important or it can be inferred. Every
$\beta$-reduced term can be written as $\lam{\tuple{x}{m}}{a \, \tuplen{t}}$,
where $a$ is not an application; we call $a$ the \emph{head} of the term. By
convention, $a, b$ and $\zeta$ denote heads. If $a$ is a free variable, we call the term
\emph{flex}; otherwise, the term is \emph{rigid}.

Deviating from the standard notion of higher-order subterm, we define subterms
on $\beta$-reduced terms as follows: a term $t$ is a subterm of $t$ at position
$\varepsilon$. If $s$ is a subterm of $u_i$ at position $p$, then $s$ is a
subterm of $a\;\overline{u}_n$ at position $i.p$. If $s$ is a subterm of $t$ at
position $p$, then $s$ is a subterm of $\lambda x.\, t$ at position $1.p$. Our
definition of subterm gracefully generalizes the corresponding first-order
notion: $\cst{a}$ is a subterm of $\cst{f} \, \cst{a} \, \cst{b}$ at position 1,
but $\cst{f}$ and $\cst{f} \, \cst{a}$ are not subterms of $\cst{f} \, \cst{a}
\, \cst{b}$. We say a term is ground if it has no free variables, and closed
if it has no loosely bound variables.


Throughout this thesis, we consider completeness of higher-order calculi only
with respect to Henkin semantics \cite{bm-14-automation-ho}. Note that no
complete calculus exists for higher-order calculi with standard (full)
semantics.

\section{Clausal Forms}
\label{sec:pre:clauses}

The resolution and superposition calculi do not work directly on formulas, but
on their simplified forms, {\em clauses}. Thus, the initial problem, expressed as a
set of formulas, must be transformed into a set of clauses. For all three
discussed logics, there exists such a transformation that does not affect
satisfiability of the problem \cite{nw-01-small-cnf}.

\looseness=-1
An {\em equation} $s \eq t$ corresponds to an unordered pair of terms. A {\em literal} $l$ is an equation $s \eq t$ or its negation (disequation) $s \noteq t$. A clause $C$ is a
finite multiset of literals, interpreted and written disjunctively:\ $l_1 \llor
\cdots \llor l_n$. Free variables of clauses are implicitly
universally quantified. Note that $\llor$ denoting disjunction in clauses
is different from the formula-building $\ior$.

In standard, non-clausal first-order logic, an atom is either
predicate or equational. For uniformity, and to
stay close to the implementation, we encode predicate atoms as equations with
$\itrue$. Negative literals are encoded as disequations. For example,
$\cst{even}(x)$ is encoded as $\poslit{\cst{even}(x)}$, and
$\neg\,\cst{even}(x)$ is encoded as $\neglit{\cst{even}(x)}$. To lighten the notation,
we sometimes write the predicate literals in non-encoded form.

Applying substitution to a literal is reduced to applying it to both sides of
the (dis)equa\-tion and it is extended pointwise to clauses. It is sometimes
also written in postfix notation as $l\sigma$ and $C\sigma$. We say
$\sigmacl{C}$ is a ground instance of $C$ if $\sigma$ is a grounding
substitution.

\section{Superposition}

Superposition is one of the most successful calculi for first-order logic with
equality. It owes its success to a careful treatment of equality and built-in
heuristics to prune the search space such as term order and selection functions.
Before introducing the rules of the calculus, we provide definitions of these and other
background concepts.

\subsection{Term Order and Selection}
\label{sec:pre:order}
Superposition calculus is parameterized by a {\em reduction ordering} $\succ$, an
ordering on terms that has the following properties:

\noindent\begin{tabular}{p{\dimexpr 0.3\linewidth-2\tabcolsep}p{\dimexpr 0.7\linewidth-2\tabcolsep}}
    \textit{Irreflexive} & For all terms $s$, $s \not\succ s$ \\
    \textit{Transitive} & For all terms $s, t, u$,  if $s\succ t$ and $t \succ u$ then $s \succ u$ \\ 
    \textit{Subterm property} & For all terms $s$ and contexts $C$, $C[s] \succ s$ \\
    \textit{Respects substitutions} & For all terms $s, t$ and substitutions $\sigma$, $s \succ t$ implies $\sigmaterm{s} \succ \sigmaterm{t}$ \\
    \textit{Respects contexts} & For all terms $s,t$ and contexts $C$, $s \succ t$ implies $C[s] \succ C[t]$ \\
    \textit{Ground total} & For all distinct
    ground terms $s$ and $t$ either $s \succ t$ or $t \succ s$. \\
    \textit{Well-founded} & There are no infinite chains of the form $s_1 \succ s_2 \succ \cdots$ 
\end{tabular}

The term order is lifted to literals and clauses using the {\em multiset extension} of
$\succ$. The order is extended to multisets as follows
\cite[Sect.~2.5]{bg-01-resolution}. For two multisets $S_1$ and $S_2$, we write $S_1 \succ S_2$ if $S_1 \not= S_2$
and whenever $S_2(x) > S_1(x)$ then there is some $y \succ x$ such that $S_1(y)
> S_2(y)$. We use the notation $S(x)$ to denote the number of occurrences of $x$ in
$S$. To use the  multiset extension, positive literals are represented as $ \{ \{s\},
\{t\} \}$ and negative ones as $\{ \{ s, t \} \}$. Clauses are then represented as 
multisets of such literals. 

Two orders that are commonly used in superposition theorem proving are the
Knuth-Bendix order (KBO) \cite[Sect.~5.4.4]{bn-98-tr-and-all-that} and the
lexicographic path order (LPO) \cite[Sect.~5.4.2]{bn-98-tr-and-all-that}.
KBO assigns integer weights to symbols and uses precedence between symbols to
break eventual ties. LPO entirely relies on precedence and inspects term
structures more closely to determine the order. 

\newcommand{\selfun}{\ensuremath{\mathit{Sel}}}
The {\em literal selection function} is a function that selects a (multi)subset of literals
from a given (multi)set of literals. Superposition requires that at least one of the selected literals is negative. 

\subsection{Unification}
\label{sec:pre:unif}

Calculi from the resolution family, including superposition, perform inferences
only on unifiable terms. We say terms $s$ and $t$ are {\em unifiable} if there is a
substitution $\sigma$ such that $\sigmaterm{s} = \sigmaterm{t}$; we further say
$\sigma$ is the unifier. A \emph{unification constraint} $s \unif t$ is a pair of two terms of the same type. 
We always specify if the constraint should be interpreted as ordered or unordered.
We say a substitution $\sigma$ is \emph{more general} than $\varrho$ if there exists a substitution $\theta$
such that $\varrho = \theta\sigma$.
A \emph{most general unifier} is a
unifier $\sigma$, such that for any other unifier $\theta$, there is a
substitution $\varrho$ such that $\theta = \varrho\sigma$. In first-order logic,
the most general unifier is unique up to variable renaming. Furthermore,
first-order logic admits efficient algorithms for computing the most general
unifier \cite{hv-09-unifalgs}.

In higher-order logic, unification is performed modulo rules of
$\alpha\beta\eta$ conversion. Under these conditions, the uniqueness of the most
general unifier is not guaranteed. Consider the constraint $X \,
(\cst{f} \, \cst{a}) \unif \cst{f} \, (X \, \cst{a})$. Any substitution of the
form $\{ X \mapsto \lamx{\cst{f}^i \, x} \}$ is a unifier, with $\cst{f}^i \, x$
denoting iterated, $i$-fold application of $\cst{f}$. However, neither of these substitutions
is more general than the other. To generalize most general
unifiers to higher-order logic, the concept of the {\em complete sets of unifiers} (CSU) was
introduced.

A (higher-order) unifier of a multiset of unification constraints $E$ is
a substitution $\sigma$, such that $\sigma(s) \equi \sigma(t)$, for all $s \unif t
\in E$. A complete set of unifiers of $E$ is defined as a
set $U$ of $E$'s unifiers along with a set $V$ of \emph{auxiliary variables}
such that no $s \unif t \in E$ contains variables from $V$ and for every unifier
$\rho$ of $E$, there exists a $\sigma \in U$ and a substitution $\theta$ such
that for all $X\not\in V,$ $\rho(X) = \theta(\sigma(X))$. The most general unifier
corresponds to a one-element CSU. A unifier of terms $s$ and $t$ is a unifier of
the singleton multiset $\{ s \unif t \}$. There is no algorithm to decide
if two higher-order terms are unifiable, but there exist algorithms
that enumerate all elements of the CSU for unifiable terms.

These higher-order concepts gracefully generalize the corresponding first-order
ones. For example, if $s$ at $t$ are first-order, then there must exist a singleton
CSU, i.e., the most general unifier. Thus, we will not explicitly qualify
unifiers as first-order or higher-order and ensure the context provides enough
information.

\subsection{Inference Rules}
\label{sec:pre:rules}
\newcommand{\mgu}{\ensuremath{\mathit{mgu}}}

Before spelling out the inference rules of standard, first-order superposition
let us introduce some helpful notation, following Schulz \cite{ss-02-brainiac}.
With $t|_p$ we denote the subterm of $t$ at position $p$ and with $t[p \leftarrow
t']$ replacement of this subterm with $t'$. Let \selfun{} be a selection function,
$\succ$ a reduction ordering, $C = l \llor C'$ a clause, and $\sigma$ a
substitution. We say that \sigmacl{l} is eligible for resolution if: (1) nothing
is selected by \selfun{} and \sigmacl{l} is $\succ$-maximal within the literals in
\sigmacl{C} or (2) \selfun{} selected some literals and \sigmacl{l} is the maximal
within either positive or negative selected literals. We say \sigmacl{l} is
eligible for paramodulation if it is positive, nothing is selected and
\sigmacl{l} is the maximal literal within the literals of \sigmacl{C}. With
$\mgu(s,t)$ we denote the most general unifier of $s$ and $t$. Using a horizontal
line to separate premises and the conclusion, four inference rules of
superposition are as follows:

\medskip
\begin{tabular}{m{\dimexpr 0.36\linewidth-2\tabcolsep}m{\dimexpr 0.57\linewidth-2\tabcolsep}}
    \multicolumn{2}{l}{{\bf Equality resolution} (ER)} \\[\jot]
    $\namedinference{ER}{s \noteq t \llor C}{\sigmacl{C}}$ & where $\sigma = \mgu(s,t)$, and $\sigmacl{s \noteq t}$ is eligible for resolution
\end{tabular}

\medskip

\begin{tabular}{m{\dimexpr 0.36\linewidth-2\tabcolsep}m{\dimexpr 0.57\linewidth-2\tabcolsep}}
    \multicolumn{2}{l}{{\bf Equality factoring} (EF)} \\[\jot]
    $\namedinference{EF}{s \eq t \llor u \eq v \llor C}{\sigmacl{t \noteq v \llor u \eq v \llor C}}$ 
        & where $\sigma = \mgu(s,u), \sigma(t) \not\succ \sigma(s)$ and 
        $\sigmacl{s \eq t}$ is eligible for paramodulation
\end{tabular}

\medskip

\begin{tabular}{m{\dimexpr 0.36\linewidth-2\tabcolsep}m{\dimexpr 0.57\linewidth-2\tabcolsep}}
    \multicolumn{2}{l}{{\bf Superposition into positive literals} (SP)} \\[\jot]
    $\namedinference{SP}{s \eq t \llor C \qquad u \eq v \llor D}{\sigmacl{u[p \leftarrow t] \eq v \llor C \llor D}}$ 
        & where $\sigma = \mgu(u|_p, s), \sigma(t) \not\succ \sigma(s), \sigma(v) \not\succ \sigma(u)$,
        $\sigmacl{s \eq t}$ and $\sigmacl{u \eq v}$ are eligible for paramodulation, and $u|_p$ is not a variable
\end{tabular}

\medskip

\begin{tabular}{m{\dimexpr 0.36\linewidth-2\tabcolsep}m{\dimexpr 0.57\linewidth-2\tabcolsep}}
    \multicolumn{2}{l}{{\bf Superposition into negative literals} (SN)} \\[\jot]
    $\namedinference{SN}{s \eq t \llor C \qquad u \noteq v \llor D}{\sigmacl{u[p \leftarrow t] \noteq v \llor C \llor D}}$ 
        & where $\sigma = \mgu(u|_p, s), \sigma(t) \not\succ \sigma(s), \sigma(v) \not\succ \sigma(u)$,
         $\sigmacl{s \eq t}$ is eligible for paramodulation, $\sigmacl{u \noteq v}$ is eligible for resolution, and $u|_p$ is not a variable
\end{tabular}

\medskip
Let us give some intuition behind how these rules are applied in practice. The
\infname{ER} rule is used to replace equality with equality within a clause.
Consider the formula $\iforall x.\,  x \eq \cst{2} \iimplies \cst{even}(x)$. In
clausal form it is $X \noteq \cst{2} \llor \cst{even}(X)$. Applying the
\infname{ER} rule effectively applies the precondition $X \eq \cst{2}$ to the
conclusion $\cst{even}(X)$ to obtain $\cst{even}(\cst{2})$. The \infname{EF}
rule can be used to find instances of the clause that can be further simplified.
Consider the clause $\cst{f}(X) \eq \cst{a} \llor \cst{f}(Y) \eq \cst{a} \llor
\cst{p}(X, Y)$. Applying \infname{EF} to its first two literals yields $\cst{a}
\noteq \cst{a} \llor \cst{f}(X) \eq \cst{a} \llor \cst{p}(X, X)$ (with $\sigma = \{ Y \mapsto X \}$). This clause
then further simplifies to $\cst{f}(X) \eq \cst{a} \llor \cst{p}(X, X)$ using \infname{ER}.

Lastly, the rules \infname{SP} and \infname{SN} are used to simulate what mathematicians do when
they ``replace equals by equals''. The left premise intuitively states that an
equation $s \eq t$ holds under condition $C$ (or more precisely, its negation).
Similarly, the right premise asserts that a (dis)equation holds under a
condition. The superposition rule simply concatenates the conditions for both
premises and replaces equals by equals in two main inference terms. For example
given clauses $\neg \cst{even}(X) \llor \cst{f}(X) \eq \cst{a}$ and $\neg
\cst{even}(\cst{b}) \llor \cst{g}(\cst{f}(Y)) \eq \cst{b}$, superposition between their
last literals results in $\neg
\cst{even}(X) \llor \neg \cst{even}(\cst{b}) \llor \cst{g}(\cst{a}) \eq
\cst{b}$.

% \[
% \namedinference{ER}{s \noteq t \llor C}{\sigmacl{C}}
% \]
% where $\sigma = \mgu(s,t)$, and $(s \noteq t)\sigma$ is eligible
% for resolution; \infname{ER} stands for equality resolution

% \[
% \namedinference{ER}{s \noteq t \llor C}{\sigmacl{C}}
% \]
% where $\sigma = \mgu(s,t)$, and $(s \noteq t)\sigma$ is eligible
% for resolution; \infname{ER} stands for equality resolution

% \[
% \namedinference{EF}{s \eq t \llor u \eq v \llor C}{\sigmacl{(t \noteq v \llor u \eq v \llor C)}}
% \]
% where $\sigma = \mgu(s,u), \sigma(t) \not\succ \sigma(s)$ and 
% $(s \eq t)\sigma$ is eligible for paramodulation; \infname{EF} stands for equality factoring

% \[
% \namedinference{SP}{s \eq t \llor C \qquad u \eq v \llor D}{(u[p \leftarrow t] \eq v \llor C \llor D)\sigma}
% \]
% where $\sigma = \mgu(u|_p, s), \sigma(t) \not\succ \sigma(s), \sigma(v) \not\succ \sigma(u)$,
% $(s \eq t)\sigma$ is eligible for paramodulation, just like $(u \eq v)\sigma$ and $u|_p$ is not a variable;
% \infname{SP} stands for positive superposition 


% \[
% \namedinference{SN}{s \eq t \llor C \qquad u \noteq v \llor D}{(u[p \leftarrow t] \noteq v \llor C \llor D)\sigma}
% \]
% where $\sigma = \mgu(u|_p, s), \sigma(t) \not\succ \sigma(s), \sigma(v) \not\succ \sigma(u)$,
% $(s \eq t)\sigma$ is eligible for paramodulation, $(u \eq v)\sigma$ is eligible for resolution, and $u|_p$ is not a variable;
% \infname{SN} stands for negative superposition
\medskip

\subsection{The Redundancy Criterion and Simplification Rules}

The rules described above are used to generate new clauses from already derived
ones. Even though orderings and selection are used to reduce the search space,
it still grows rather fast. To identify unnecessary clauses in the search
space, superposition features a \emph{redundancy criterion}
\cite[Sect.~4.2.2]{bg-01-resolution}. We say that a ground clause $C$ is
{\em redundant} in a ground set of clauses $N$ if it is entailed by a subset of
clauses in $N$ such that each clause in this subset is smaller than $C$. More
generally, a clause (ground or not) is redundant in set $N$ if every ground
instance of $C$ is redundant for some grounding of $N$. Removing redundant
clauses does not affect completeness of superposition.

Other than the four generating rules, superposition provers also use
simplification rules, which are justified using the redundancy criterion. These
rules replace the premises with conclusions, and we denote them using double
bars. As an example of such a rule, let us introduce rewriting (demodulation) of
negative literals (\infname{RN}):

\[
\namedsimp{RN}{s \eq t \qquad u \noteq v \llor D}
              {s \eq t \qquad \sigmacl{u[p \leftarrow \sigma(t)] \eq v \llor C \llor D}}
\]
where $\sigma = \mgu(u|_p, s)$, and $\sigma(s) \succ \sigma(t)$.


Simple rules such as removing duplicate literals, literals of the form $s \noteq
s$ or tautological clauses are clearly justified by the redundancy criterion.
Schulz gives an extensive list of such rules \cite{ss-02-brainiac}, including rewriting of positive
literals which has more side conditions.

We say that a clause $C$ subsumes clause $D$ if there is a substitution $\sigma$
such that $\sigmacl{C} \subseteq D$. Subsumption is one of the most important
operations within a theorem prover. This rule does not adhere to the redundancy
criterion, but can be justified using other mechanisms \cite{wtrb-20-sat-framework}.

\subsection{The Saturation Procedure}
\label{sec:pre:saturation}

Superposition provers saturate the input problem with respect to the calculus's
inference rules using the \emph{given clause procedure}
\cite{mcw-1997-otter,adf-1995-discount}. It partitions the proof state into a
passive set $\mathcal{P}$ and an active set $\mathcal{A}$. All clauses start in
$\mathcal{P}$. At each iteration of the procedure's main loop, the prover
chooses a clause $C$ from $\mathcal{P}$, simplifies it, and moves it to
$\mathcal{A}$. Then all inferences between $C$ and active clauses are performed.
The resulting clauses are again simplified and put in $\mathcal{P}$.
The provers differ in which clauses are used for simplification: Otter-loop
\cite{mcw-1997-otter} provers use both active and passive clauses whereas
DISCOUNT-loop \cite{adf-1995-discount} provers use only active clauses.
The provers we discuss in this thesis, E and Zipperposition, are both DISCOUNT-loop provers.

\subsection{Higher-Order Superposition Calculi}
\label{sec:pre:ho-sup-calculi}

\looseness=-1
In Chapter \ref{ch:intro} we mention that we identified three steps on the road
to extending efficient superposition provers to higher-order logic. Together with
our colleagues we did the necessary theoretical work by extending the superposition
calculus using the same roadmap:

\begin{enumerate}
    \item Bentkamp, Blanchette, Cruanes, and Waldmann designed a complete
    superposition calculus for higher-order logic devoid of
    $\lambda$-abstraction and first-class Booleans \cite{bbcw-21-lfho}. This calculus is called \emph{Boolean-free $\lambda$-free higher-order superposition calculus} 
    and we shortly refer to it as \lfsup{}.
    \item \begin{sloppypar}
    Bentkamp, Blanchette, Tourret,  Vukmirović, and Waldmann
    extended this calculus to support $\lambda$-abstraction
    \cite{bbtvw-21-sup-lam}. They call this new calculus \emph{Boolean-free $\lambda$-superposition calculus},
    and we shortly refer to it as \lsup{}.
    \end{sloppypar}        
    \item Bentkamp, Blanchette, Tourret, and Vukmirovi\'c extended \lsup{} with
    support for first-class Booleans \cite{bbtv-21-full-ho-sup}. This amounts to
    designing the complete calculus for full higher-order logic. This
    calculus is called Boolean $\lambda$-superposition, or shortly \osup{}.
\end{enumerate}
%
Note that we use the name $\lambda$-superposition interchangeably for \lsup{}
and \osup{} where the context gives enough information to disambiguate this name.
