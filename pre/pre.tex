\chapter{Preliminaries}
\setheader{Preliminaries}
\label{ch:pre}

\begin{abstract}
    In this chapter we lay out the basic prerequisites for the remaining
    chapters. We begin by defining three logics that we work with in this
    thesis: propositional logic, monomorphic first-order logic and simply typed
    higher-order logic. Then, we explain the clausal structure which is the
    backbone of many calculi for automated provers. We further describe the
    superposition calculus and finish with description of the process superposition
    provers use to actually perform the inferences.
\end{abstract}
      
\newpage

Our work mostly concerns retrofitting first-order provers to support
higher-order concepts. But part of our work also concerns integrating
successful approaches for propositional logic in first-order
provers. We begin by defining all three logics, in the increasing
order of expressivity.

\section{Propositional Logic}

Basic formulas of propositional logic are propositional variables
$\cst{p}, \cst{q}, \ldots$ and constants $\top$ and $\bot$. More complex
formulas are built inductively using logical connectives $\neg, \land, \lor,
\imp, \lequiv$: if $\phi$ and $\psi$ are formulas, then $\neg \phi, \phi \land
\psi, \phi \lor \psi, \phi \imp \psi, \phi \lequiv \psi$ are formulas as well.

To interpret the formulas, propositional variables are assigned 0 (false) or 1
(true), constants $\top$ and $\bot$ are assigned 1 and 0 respectively and the
formula is interpreted using the rules for each connective
\cite[Sect.~1.4]{hr-00-logic-in-cs}. As there are finitely many propositional
variables in a formula, trying all (finitely many) possible assignments
describes an algorithm to decide satisfiability (existence of satisfiable
assignment) or validity (if all assignments are satisfiable).

However, this simple approach is prohibitively expensive and modern tools that
decide propositional satisfiability problem (SAT solvers) use more advanced
approaches such as CDCL calculus \cite{mss-96-cdcl}. Modern SAT solvers also
heavily preprocess the problem and continue simplifying it during proving
process.

\section{First-Order Logic}

First-order logic increases the expressivity by allowing quantification over
objects and has more complicated formula structure. There are many flavors of
first-order logic, but in this thesis we consider monomorphic first-order logic
with equality.

We distinguish a set of base types $T$ which is required to have the Boolean
type $o$, and a set of symbols $\Sigma$. To each symbol $\cst{f} \in \Sigma$ a
tuple $(\tau_1, \ldots, \tau_{n}, \tau), n \geq 0$ of types is assigned, written
$\cst{f} : (\tau_1, \ldots, \tau_{n}) \rightarrow \tau$. We say that $(\tau_1,
\ldots, \tau_{n})$ are argument types, $\tau$ is the return type, and that $n$
is the arity of the symbol $\cst{f}$. If return type of $\cst{f}$ is $o$ we call
$\cst{f}$ a predicate symbol, otherwise we call it a function symbol. Argument types may not be Boolean.

Terms are the basic building blocks of first-order logic. Variables
$x,y,\ldots$, assigned types $\tau \in T$, are terms. If $t_1,\ldots,t_n$ are
terms of types $\tau_1, \ldots, \tau_n$, respectively, and $\cst{f} : (\tau_1,
\ldots, \tau_{n}) \rightarrow \tau$, then $\cst{f}(t_1, \ldots, t_n)$ is a term
of type $\tau$. If $n=0$, we drop parentheses and write $\cst{f}$. We also
abbreviate $\cst{f}(t_1, \ldots, t_n)$ to $\cst{f}(\tuplen{t})$. Using similar notation,
we abbreviate a tuple of terms $(t_1, \ldots, t_n)$ as $(\tuplen{t})$ or simply $\tuplen{t}$.

Terms are used to build atoms. Atom is either a term $t$ of type $o$ or an
equation $s \eq t$ where terms $t$ and and $s$ are of the same type. Atoms are
combined using logical connectives just like in propositional logic.
Additionally, first-order formulas are built using quantifiers: if $\phi$ is a
formula and $x$ is a variable then $\forall x.\, \phi$, as well as $\exists x.\,
\phi$ are formulas. The first formula requires that $\phi$ holds for any value
of $x$, while second requires that there is some $x$ for which $\phi$ holds. As
soon as there is a single functional symbol with arity greater than 0, there are
infinitely many terms that can be substituted for a free variable. It is
therefore obvious that the propositional technique for deciding satisfiability
does not work in the first-order case.

Substitutions $\sigma, \varrho, \ldots$ are total mappings from variables to
terms of the same type. Substitutions map only finitely many variables to terms
other than the variable itself. This is denoted as $\{ x_1 \mapsto t_1, \ldots,
x_n \mapsto t_n \}$ where $x_i$ are variables that are not mapped to itself.
Applying a substitution $\sigma$ to a term $t$ results in replacing all mapped
variables by the corresponding terms and it is denoted.

\section{Higher-Order Logic}

We assume classic simply typed monomorphic higher-order logic with the choice
operator. Assuming a set of base types $T$ and a set of symbols $\Sigma$ as in
first-order logic, let us define terms and types. Types are either base types
$\tau in T$, or function types $\tau_1 \rightarrow \tau_2$ where both $\tau_1$
and $\tau_2$ are types. Each symbol $\cst{f} \in \Sigma$ is assigned a type.

Terms are defined as free variables $F, X, \ldots$, bound variables $x, y, z,
\dotsc$, or symbols $\cst{f}, \cst{g},\allowbreak \cst{a},\allowbreak \cst{b}
\dotsc$. Furthermore is $s$ and $t$ are terms of type $\tau_1 \rightarrow
\tau_2$, and $\tau_1$ respectively, then $s \, t$ is a term of type $\tau_2$.
Lastly, if $x$ is a bound variable of type $\tau_1$ and $s$ is a term of type
$\tau_2$, then $\lambda$-abstractions $\lamx{s}$ is a term of type $\tau_1
\rightarrow tau_2$. The syntactic distinction between free and bound variables
gives rise to \emph{loose bound variables} (e.g., $y$ in $\lamx{y \, \cst{a}}$)
\cite{tn-93-patterns}. In Chapter \ref{ch:ehoh} we write free variables in
lowercase as the logic described in that chapter disallows
$\lambda$-abstraction, and consequently bound variables.

We assume the standard notions of $\alpha$, $\beta$ and $\eta$ conversions. We
let $s \, \overline{t}_n$ stand for $s \, t_1 \, \ldots \, t_n$ and
$\lam{\overline{x}_n}{s}$ for $\lambda x_1. \ldots \lambda x_n. \> s$. Every
$\beta$-normal term can be written as $\lam{\tuple{x}{m}}{s \, \tuplen{t}}$,
where $s$ is not an application; we call $s$ the \emph{head} of the term. If $s$
is a free variable, we call the term \emph{flex}; otherwise, the term is
\emph{rigid}.

Throughout this thesis, we consider completeness of higher-order calculi only
with respect to Henkin semantics \cite{bm-14-automation-ho}. Note that no
complete calculus exists for higher-order calculi with standard (full) semantics.

\section{Clausal Forms}
\section{Superposition}
\subsection{Term Order and Selection}
\subsection{Unification}
\subsection{Inference Rules}
\subsection{The Redundancy Criterion}
\subsection{The Saturation Procedure}
